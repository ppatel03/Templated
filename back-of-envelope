THESE ARE IMPORTANT NOTES WHICH CAN HELP A LEAD ENGINEER IN DAY TO DAY JOB. I RECOMMEND TO EVEN ENTRY LEVEL ENGINEERS UNDERSTAND AND DESIGN SOFTWARE DESIGN BASED ON TRADE-OFFS.

THIS NOTES ARE NOT MEANT FOR ANY INTERVIEW PREP BUT MORE IN  DAY TO DAY JOB FOR BUILDING REAL-WORLD SYSTEMS. ANY COMMENTS/SUGGESTIONS ARE WELCOME.

Links :

https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation

https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05


https://docs.google.com/document/d/1wUCqhVHydWiDk6FJdFLSMpgigNrGcs4OFZg0Wa7JGEw/edit

Framework - 
  1. List all Functional requirements and assumptions
      requirements : in form of say, ability to xyz
      assumptions : types of users, are users already auth, types of devices (for device synchronization)
  2. Scale 
    2.1 Estimate QPS 
      a. Estimate Read vs Write QPS 

  3. Non-Functional Requirements ( Healthy resilent consistent scalable and secure systems with change safe user operations)
  Usually cover the topline Healthy is enough to start with 
    3.1 - Healthy systems 
      3.1.1. - Availability vs Consistency based on CAP which simply means Recency or Linearizability/Total ordering (since consistency has different context for ACID, Consistent hashing )
          - Availability %
          - Eventual Consistency (most of the times)
            - Data freshness SLA if eventual consistency
            - CAN PUT Strong consistency for few APIs like profile update, offline alerting, billing info. ( For example,  in Cassandra, you can specify query level if quorum consistency is needed - SELECT * FROM users WHERE id = 123 USING CONSISTENCY QUORUM;)
      3.1.2. Latency 
      3.1.3. Error rate ( optional )
    3.2 - Resilent systems ( measured by MTTR, MTTD, Auto-remediation rate)
      3.2.1. Fault tolerant (DR ready - mirroring, statlessness) 
      3.2.2  Durability (No data loss - replication, Use pull) 
      3.2.3. Autoscaling, Auto-heal like in Kubernetes
    3.3. - Consistency 
       - Type and freshness SLA if eventual/temporal : covered during availability above 
       - Data integrity / Accuracy 
          - Idempotency
          - Exactly once processing 
          - Detection using merklee tree 
          - Resoluion using Reconcilliation process
    3.4 - Scalability / Saturation ( measured by CPU throttling, OOM kills, Disk exhaustion, Pod kills) 
        - Autoscaling
        - Stateful service orchestrator ( example, Orion at Pinterest using Command Control)
    3.5 - Security : AAA ( Authentication, Authorization and Accounting) 
        - envoy helping with AAA based on policy 
        - VPC
        - HTTPS (HTTP + TLS with Certificate exchange + public key and later with symmetric key) based Data encryption esp. if personal data

    3.5 - Change safe ( measured change failure rate, rollback rate, lead time to productio, PR throughput)
      -  Automated reviews with AI, clear documentation with rules for AI editors like cursor 
      -  Integration/E2E testing before merge 
      -  Staging and Pre-prod/Canary : Automated change analysis for regression detection 
      -  Prod : Automated change analysis for regression detection  
    3.6 - User perceived reliability ( measure by incoming bugs from user, Apdex score) 
    3.7 - Operations ( measured by # of alerts, # of hours in operations, false positive alerts,  % of auto-remediated alerts )
      - metrics, monitoring and alerting set up, oncall setup 
      - auto-remediation 
      - reduce false positive alerts 

  4. Metrics monitoring & Alerting for oncall using datadog
    a. Business success 
    b. Engagement metrics
    c. Reliability metrics like above in Non-functional requirements 
      

  5. Estimation 
   NOW TELL THAT YOU WILL ESTIMATE AT HIGH LEVEL ONLY 

    2.2 Estimate bandwith quickly
      b. Estimate bandwidth per user for read and write QPS 
      c. Estimate bandwidth in total for read and write QPS
        tell why you are estimating.
        High total bandwidth means you need distributed system with high availability and replication in DBs to spread out the load
        Example, High user bandwidth say 100 MB/s means latency could be very high. So split the bandwidth with more replication 

    2.3 Estimate Total Storage bytes
      tell why you are estimating. 
        Just a high level conclusion if you need DB sharding which depends on size and bandwidth. 

      TRICK
      1. B * K/M/G = K/M/G (always same when multiplied with bytes)
      2. K * K/M/G = M/G/T 
      2. M * K/M/G = G/T/P  

  From above, Conclude at high level from Back of envelope
   Example, High availability, bandwidth, large storage
        - needs multiple instances 
        - needs sharded storage 
        - needs replicated storage  
  
     
Lets take a look into details : 

1. List all Functional requirements and assumptions
    - Ask detailed functional requirements 
        Why ? 
          - first since it helps with API design (example, ability to post tweet, view tweet)
  
    Assumptions
    - if its okay to assume user is authenticated
    - Ask about interaction points - API / UI or both

    If there is time, then 
      - Ask about devices - Mobile or WEB 
          (though need reasoning why you asked this question :) . 
            1. Consistency between devices ( example, chats read) 
            2. mobile could be low bandwidth network ~ 100 Mbps or 10 MBps -   https://www.quora.com/From-what-maximum-speed-can-a-smartphone-download-or-upload-some-data

      - Ask types of users (admin, general) 
          Why ? 
            - since it helps with data access patterns and security checks (admin can edit personal info/block users )
          Why ? 
            - since it helps you think end to end from UX perspective 
     

Time to understand scalability, latency aspects

2. Estimate QPS 
  a. Estimate Read vs Write QPS
    1. Ask about # of users - DAU/MAU
        QPS estimate
          - Read : Write ratio
            DO NOT divided by no. of seconds to get QPS if its explictly mentioned about concurrent users say 10% of DAU. In such cases, QPS = 10% DAU
          - 1 day ~ 10^5 s
          - 1 week ~ 7 * 10 ^ 5s and further ~ 10 ^ 6s
          - 1 month ~ 3 * 10 ^ 6s and further ~ 10 ^ 6s
          - 1 year ~ 365 * 10 ^ 5s and further ~ 10 ^ 8s
          For sake of time constraints, do not hesitate to round off large amount


4. Non-Functional Requirements (https://www.datacore.com/blog/availability-durability-reliability-resilience-fault-tolerance/ )

    a. Reliability Standards
      https://www.pagerduty.com/resources/learn/reliability/ 
          Reliability, on the other hand, is a measure of the probability that the system will meet defined performance standards in performing its intended function during a specified interval.  Example of not meeting reliability stands is Twitter was hacked in 2019.
          means you need to meet certain standards. 

      a.1 - Availability vs Consistency 
        If high availability, then ask availability nos example, 99.999 or 3 9s
          - Availability %
            - https://www.pagerduty.com/resources/learn/reliability/ 
                Availability is a measure of the percentage of time that an IT service or component is in an operable state. Example of not meeting availability is Facebook was down in 2021 for about 3-5 hrs.
                With this you can even decide that, your system/part of the sytem could be offline for 10s in a day for reasons like roll out, stateful servers scaling, upgrading the database or even doing DB sync etc. 
                4 9s -> 10s  - which is 300s a month which is 60-120s of AWS failover time. This is also AWS ec2 instance/ EKS and Azure's AKS availability percentage  
                5 9s -> 1 s
                6 9s -> 100 ms or less than 1s
                8 9s -> 1 ms or very high (no downtime)
                > 6 ps -> in us or super high (no downtime else SLA violated)
            - if high availability, then ask availability nos example, 99.99 or 4 9s
            - With 2 servers of service runnings parallel each with 99.9% availability. The overall availability increase ~ 99.9999% http://tinyurl.com/ypgx8qqc 
              99.9% ~ 0.1% ~ 0.001 ~ 0.001 ^ 2 ~ 0.000001 ~ 1 - 0.000001 ~ 0.999999 ~ 99.9999 % 

          - Consistency 
            - Types of Consistency (https://www.geeksforgeeks.org/sequential-consistency-in-distributive-systems/# )
              - Strong consistency (all provides total ordering)
                - Linearizability/Atomic consistency (atomic + real-time order) 
                  - Recency guarantee
                  - One popular example is password update of someone’s bank account.
                  - Linearizability enforces real-time ordering. If an operation completes before another starts, the first must appear before the second in the global order.
                - Quorum consistency 
              - Serializability (transactional order, no real-time requirement)
                - Example
                    Two processes, shared variable x = 0:
                    
                      Time →
                        P1: write(x=1) ──────────────┐
                                                     │ completes at T2
                        P2:            write(x=2) ────┼───┐
                                                      │   │ completes at T3
                        P3:                          read(x) ────┐
                                                                  │ completes at T4
                    Serializable (but not Linearizable)
                    Valid execution:
                      - P3 reads x = 1 (even though write(x=2) finished before the read started)
                      - Equivalent to serial order: write(x=1) → read(x=1) → write(x=2)
                      - Why valid: The read completes at T4, after both writes. But serializability only requires equivalence to some serial order; it doesn't enforce that the read sees the most recent write.
              - Sequential consistency ( per program order preserved)
                - DOES NOT ensure writes are visible instantaneously or in the same order as according to some global block.
                - One example is that of facebook posts of friends (sequential for a particular friend, but not for overall)
              - Monotonic read consistency Pg 165 DDIA ( no going backwards)
              - Causal consistency (cause-effect preserved, So its partial ordering and detect parallel events)
                - Preserves the order of casually-related(dependent) operations.
                - So, that’s why it does not ensure the ordering of operations that are non-causal.
                - One example is Comment replies.
              - Read-after-write consistency ( session level)
                - examples like profile update, whether you liked a post or not
              - Temporal consistency ( Data is valid within a time window)
                - always associated with data freshness sla
              - Eventual consistency which is weakest (default setting in Cassandra)

            90% of the time your ask of strong consistency should be simple 
             -  Quorum consistency or 
             -  you can extend to Recency guarantee - Linearazibility

            - if Consistency over availability, it means high consitency.
            - Eventual Consistency (most of the times)
              - Data freshness SLA if eventual consistency ( which makes temporal) 
                - With this in mind, it provides as good idea about time to resolve conflicts with concurrent writes  
                    - Example, like counter DB between datacenters. So using CRDT to resolve conflicts
                    - Example, building search engine or yelp. With this you can even decide that, your system could be out of sync from third part servers for 10s in a day
            - Latency + quorum to replica if Quorum consistency 
            - If Strong consistency ( Quorum consistency)  (for NoSQL like Cassandra)
              - check with latency compromise due to replication and building concensus
              - if also needed strong isolation, then can help reason for SQL databases due to isolation level support

            Tips to ensure consistency with non-database services 
            - To maintain data consistency between internal services, ensuring idempotency + exactly-once processing is very important.
            - To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency + exactly once processing and reconciliation.
                  
      a.2. Latency 
        - this would help decide whether you really need low latency optimizations like cache, or not. DBs can be as fast as ms today and cache as us today
        - Remember today mosts have 1 Gbps network and internal datacenter is 25 Gbps ~ 2.5 GBps
        - Latency number for 1G Ethernet 
          - maximum size Ethernet frame (1500 bytes) at 100 Mbps the latency is 120 µs. For comparison, the minimum size frame (64 bytes) at Gigabit speeds has a latency of just 0.5 µs

      a.3. Fault tolerant + Durability + Resilency
        - Fault tolerance : tolerate faults which means high standards than just availability ( DR ready : mirroring ) 
        - Durability : no data loss ( redundancy with replication )
            - can help reason how much to replicate. 
                Example, say 99.999999% (six 9's) durability can be achieved assuming failure rate of one server is  0.01% (say 99.99% guarantee for ec2 on aws). This means you need 2 servers atleast which means 1 replica 
                  0.0001 ^ 2    = 0.0000001    or  10 ^ -8
                  1 - 0.0000001 = 0.99999999 or  8 9s  
                  % * 100       = 99.999999%      or  8 9s 
            - can help to reason using pull architectures with persistence
            - can help to reason out if you need erasure coding for data recovery
            - can help to reason out if you need checksum for data integrity
            - can help to reason out if you need commit log (WAL- append-only sequence of bytes) like Postgresql, Binlog (Row based log) MySQL and LSM tree even Cassandra

        - Resiliency
                https://www.datacore.com/blog/availability-durability-reliability-resilience-fault-tolerance/ 
                Resiliency describes the ability of a storage system to self-heal, recover, and continue operating after encountering failure, outage, security incidents, etc. High resiliency doesn’t mean there is high data availability. It just means that the storage infrastructure is equipped enough to overcome disruptions.
                One indication of resiliency is measuring mean time to repair (MTTR), which captures how long it takes to get the storage infrastructure up and running after a failure. Lower the MTTR, better the resiliency. 
                Example, kubernetes cluster spuns up new pod on failure
    

  5. Metrics monitoring & Alerting for oncall using datadog
    a. Business success 
    b. Engagement metrics
    c. Reliability metrics 


  6. CI/CD Best practices if possible in deployment & scaling 
    a. Stateless services
      - try to add stateless servers if possible
      - stateful server only for few use cases like socket connections. Make sure you know how to deal with scalability for socket connections.

  7. Estimation
    Estimate bandwidth for read and write QPS 
      say 1 KB for 100K QPS ~ 100 MBps
      - can help reason your latency
      - can help reason your replication in DBs or multiple servers
      - can help reason your sharding for database (remember internal data center max bandwidth to 25 Gbps ~ 2.5 GBps ) https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html 

    Estimate Total Storage bytes (a year, 10 years)
      a. tell why you are estimating. Just a high level conclusion if you need DB sharding which depends on size and bandwidth. 
        - a year, 10 years 
      example, to help decide if sharding is need and master-replica model per DB or per parition. Example more than 64 TB of storage definitely needs sharding (most AWS DBs or even cache like Redis has max 64 - 128 TB of storage https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-aurora-increases-maximum-storage-size-128tb/ )

  7. High level design ( super important and help in broad understanding of the system)

  Conclude at high level from Back of envelope
    - High availability, 
      - multiple instances 
    - High bandwidth, large storage (you can even bring this in data storage discussion)
      - sharded storage 
      - replicas 
    
  Key components in high level design 

    a. Separate read and write service (CQRS)
      - different bandwidth requirements
      - easier to scale independently
    b. separate out different components of the system (eg. video recommendation system )
      - avoids single point of failure (SPOF)
      - avoids monolithic 
      - map-reduce if needs to be sequential 
    c. data layer (just add the box data layer) 
    d. cover all API interactions at high level
    e. atleast one domain specific problem

    In-depth 
      a. think about data layer 
          1. Draw Data model first
            - if noSQL, 1:many is separate table of <parent_id, list of child_ids>
            - if SQL, 1:many is commonly represented in child table ( child_id (pk), parent_id (fk), child_properties)
           
          2. Choose the database
            - SQL :                       Read heavy (B tree)  + Fixed schema + Relationship + no Sharding ( sharding is manual even with AWS RDS - http://tinyurl.com/2yapkhgn  )
            - no SQL like DynamoDB :      Read heavy(B tree)  + Schema flexbility
            - no SQL like MongoDB :       Read heavy(B tree)  + Schema flexbility + Document DB
            - no SQL like CassandraDB :   Write heavy(LSM tree + SS Table with sparse index) + Schema flexibility
            - no SQL like CassandraDB with read heavy :   Write heavy, Read heavy (LSM tree + SS Table with sparse index + bloom filter) + Schema flexibility 
            - Timeseries like InfliuxDB, Prometheus:   Write heavy + Burst Read + TSM tree + Schema flexibility 

         
          - sharding support
            - All NoSQL DB like Cassandra, MongoDB, Dynamo DB supports sharding with managed service. This means sharding strategy of hash based (consistent hashing) is already supported and also in-built routing logic for right shard. 

            - With AWS offers you can shard both SQL DB like AWS RDS BUT Sharding is manual and not a managed service http://tinyurl.com/2yapkhgn 

            - Without AWS offers, SQL DB like MySQL, Postgresql are hard to shard - https://www.squash.io/tutorial-on-database-sharding-in-postgresql/
            
          
        Advanced
        - sharding (from back of envelope), 
        - replication (back of envelope), 
        - consistency (quorum), 
        - challenges - hot partitioning  

      b. Queue to avoid synchronous processing and support fault tolerant, durability 

      c. Think about edge cases for product
        1. Rate limiter 
          - Algorithms 
           - Token bucket : requests check if it can get the token from bucket, if bucket has token then request can pass through  
              - Pros : great algo to allow burst traffic. Example if your bucket is 10 and fill rate is 5/min, you could have burst update 10 request/min
              - Cons : 2 parameters to tune ( bucket size and fill rate)
           - Leaky bucket : uses fixed size queue instead of bucket but requests are serviced from queue at fixed rate 
              - Pros : smooth traffic 
              - Cons : does not allow burst, 2 parameters to tune ( queue size and queue service rate)
           - Fixed window 
              - Pros : memory efficienct, one param to tune 
              - Cons : has boundary issue
           - Sliding window is the best 
              - Pros :  one param to tune, no boundary issue 
          - Implementation 
            - distributed counting using redis (single threaded and hence atomic). Race condition can be avoid using Redis sorted set + help of Lua script which executes atomically
              SLIDING_WINDOW_LUA_SCRIPT = """
                  local key = KEYS[1]
                  local limit = tonumber(ARGV[1])
                  local window = tonumber(ARGV[2])
                  local now = tonumber(ARGV[3])

                  -- Remove expired entries
                  redis.call('ZREMRANGEBYSCORE', key, 0, now - window)

                  -- Count current requests
                  local current = redis.call('ZCARD', key)

                  if current < limit then
                      -- Add current request
                      redis.call('ZADD', key, now, now)
                      redis.call('EXPIRE', key, window)
                      return 1  -- Allowed
                  else
                      return 0  -- Rate limited
                  end
            """
          - Rate limiting rules config 
          - Exceeded rate limit HTTP error : 429 too many request 
            - HTTP response headers for additional info when user gets 429 too many requests 
              - X-Ratelimit-Remaining: The remaining number of allowed requests within the window.
              - X-Ratelimit-Limit: It indicates how many calls the client can make per time window.
              - X-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled.
          - Amazon API gateway uses token bucket 
          - Some great links 
            - https://medium.com/geekculture/system-design-design-a-rate-limiter-81d200c9d392
            - https://systemsdesign.cloud/SystemDesign/RateLimiter 

        2. Consistent Hashing 
          - Ring formation, 
          - add/remove server, 
          - To solve hot Node partition, you assign virtual replicas on ring to get even distribution of keys, 
          - To solve hotspot key problem,  you can increase number of replicas but its common acorss all paritions and NOT customized for a key
            - Resolution for hotspot key :
              - detect the hot spot key using sampling method (like in Pig) or dashboard (like in CockroachDB)
              - for that hotspot key, assign the copy number to each key and create combined partition key for Cassandra (https://stackoverflow.com/a/27889289/1540256 )
            - CockraochDB provides dashboard insights for hot spot (https://www.cockroachlabs.com/blog/the-hot-content-problem-metadata-storage-for-media-streaming/ )
          - Replication strategies for Cassandra (https://www.baeldung.com/cassandra-replication-partitioning )
              - Cassandra provides pluggable replication strategies by allowing different implementations of the AbstractReplicationStrategy class. Out of the box, Cassandra provides a couple of implementations, SimpleStrategy and NetworkTopologyStrategy.
              - Once the partitioner calculates the token and places the data in the main node, the SimpleStrategy places the replicas in the consecutive nodes around the ring.
              - On the other hand, the NetworkTopologyStrategy allows us to specify a different replication factor for each data center. Within a data center, it allocates replicas to nodes in different racks in order to maximize availability.
              - The NetworkTopologyStrategy is the recommended strategy for keyspaces in production deployments, regardless of whether it’s a single data center or multiple data center deployment 
        3. Key value store 
          - data partition - consistent hashing 
          - data replication
          - data consistency (quorum) R + W > N 
          - inconsistency resolution using versioning (RiakDB uses version vector, Dynamo / Cockroach DB / MongoDB uses vector clocks or lamport timestamp, Cassandra uses LWW using timestamp  ). 
            - Refer Pg 346 use the sequence number generated from lamport timestamp or Zookeeper based counter which supports total ordering (but cannot tell if two operations are conncurrent or are causally related) which supports linearizability (recency)
            - (Mature approach) Refer Pg 184 for version vector which supports causal order (can tell if two operations are concurrent or casually related) by keeping concurrent versions. You need version vectors for 
              - operational CRDT ( vector of increments for every node - Example node1 count=33 and increments [1,2,1], node2 count =33 and increments [1,0,1]. Here, to increment the count, you do not need to pass entire version vector on the network - https://tinyurl.com/2a3wd6ln) and 
              - State based CRDT(causal relationships and need to pass version vectors over the network)
            - Version vector data structure : [version at node1, version at node2] | [[value at node1], [value at node2]]
                  https://www.youtube.com/watch?v=1BXCxpcsmzc&t=622s&ab_channel=Jordanhasnolife 
          - Handle failures 
            - detection using gossip protocol or using zookeeeper
            - temporary failures from sloppy quorum and hinted handoffs (changes since last failure is synced back to replica)
            - permanent failures - use merklee tree to detect selected discrepancies and replicate rather than sending entire DB data over the network. Example Dynamo uses Merklee Tree during Anti-Entropy process (https://www.youtube.com/watch?v=Jy4Cm2WEZVg&ab_channel=Jordanhasnolife )
          - Flow : 
            - Write path (LSM tree + SS Table)
            - Read path (LSM tree + SS Table + Bloom filter + sparse index)
        4. Design A Unique ID Generator In Distributed Systems
            - Requiremnt : 10K ids per second, so 64-bit ( 2^64 ~ 10^16) is more than enough
            - Single database (bottleneck SPOF)
            - Multi-database set up with increments by K for K DBs (scaling problem, ID synchronization)
            - UUID - UUID is a 128-bit number used to identify information in computer systems. UUID has a very low probability of getting collusion. Quoted from Wikipedia, “after generating 1 billion UUIDs every second for approximately 100 years would the probability of creating a single duplicate reach 50%” [1]. Cons : (128 bits long but our requirements is 64 bits and they not go up with time)
            - Ticket Server like Flickr (SPOF)
            - Twitter snowflake approach (64 bits = 5 datacenter + 5 machine + 41 timestamp +  12 sequence + 1 sign  )
              - timestamp in milliseconds is 13 digits 
              - So how many numbers could be generated in 1 milliseconds ? ~10^4 
                - number generator can just execute in may be 10 - 100 ns consider counter is in L1/L2 cache. So ~10^4 numbers could be easily generated in 1 miliseconds. So 12 bits atleast for sequence number
            Other talking point - time sychronization using NTP, Bit size tuning - timestamp vs sequence bits to support low concurrency 
        5. URL shortener 
            - URL redirect response is status code as 301 and location = original url in response headers. 302 for temporary redirect. 
            - 301 helps in reducing server load vs 302 helps in tracking 
            - Data model - id, short url, long url 
            - Hash function : 62 ^ n = 300 B urls. So n ~ 7 chars 
            - Techniques 
              - hash + collison resolution 
                - SHA-1 (120 bits), SHA-256 (256 bits)
                - bloom filter to improve look up 
              - Base 62 conversion 
                - using unique id generator, generate unique id then perform  unique id % 62 
              - Hash vs Base 62 
                - fixed vs variable lenght
                -       vs  depend on unique id generator 
                - collision vs collision free 
                - cannot figure next url vs easy to figure out next url if unique id increments by 1 for new entry 
            - Others : Rate limiter (avoid overflow), Scaling with stateless, DB scaling with replication & sharding, Analytics 
          6. Web Crawler 
            - High level : 
              URL frontier (Seed urls)-> Queue-> Downloader (DNS resolover) ->Q-> Content Parser( DB) -> Q-> Link Extractor -> Q ->send to frontier
            - BFS over DFS
            - URL Frontier 
              - politeness ( avoid too many requests and getting blocked by rate limiter )
              - priority based on (pagerank) and Freshness (update history)
            - Downloader (Robots)
              - performance : distributed crawl, cache DNS, Locality, short timeout
            - Reliability/Robustness : scaling using consistent hashing, save state using Q, exception handling,
            - Extensibility : Rather than just "text", support new content types say images or video 
            - Detect and avoid malciousness
              - redundant content using deduplication with hashing
              - avoid spider traps by setting max depth (A spider trap is a web page that causes a crawler in an infinite loop either through dynamically generated links or recursive links.)
              -  noise (ads, code snippets)
            - Others : Server side rendering, Scaling with statelessness, DB replication and sharding, Analytics
                - Server side rendering using Dynamic rendering ( Dynamic Rendering: A technique where the server serves pre-rendered content to crawlers while serving client-side rendered content to regular users.)
                  -Dynamic rendering is done with help of user-agent header 
                    - Crawler
                      - GET /page HTTP/1.1
                        Host: example.com
                        User-Agent: Googlebot
                    - Normal user
                      - User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
          7. Design A Notification System
            - Different types of notification
              - App notification
                - iOS : APNs, Android : FCM. For this you need two things - certificate upload(to build trust between APN/FCM provider and device) and device token (for APNs/FCM to track about what device)
              - SMS 
                - Services like Twilio, Nexmo 
              - Email service
                - Mailchimp or Sendgrid 
            - Contact gathering flow  
              - during app install or user sign up, collects user name,  device info ( phone number, certificate, device token), phone number
                  User ---> LB ----> API server ----> DB
            - High level design 
              - sender services -> notification service (DB look up) -> Device + notification type queue -> Consumers -> Service Providers (APNs, FCMs, Twilio, MailChimp)

            - Reliability (exactly once processing, prevent data loss by logging failed messages for retry)
            - Notification template (cached in our DB so that Consumer can use it)
            - Notification setting (opt in / opt out)
            - Rate limiting ( avoid users with too many notifications)
            - Retry (PUT IT BACK TO THE QUEUE OR DLQ after multiple retries)
            - Security (certificate upload, Appkey/appSecret pair between our notification service and providers)
            - Monitor queued notifications
            - Events tracking (state of notification)
          8. Design A News Feed System
            - build timeline On-Demand vs Push
              - on-demand for celebrities posts (> 1 M followes)
              - push for other users post
            - Feed publish flow
              - Client/Browser -> LB -> publish-service (online path) -> Post DB -> Post cache
                                                |
                                            Queue (offline) ---> Consumer ----> Queue ------> Consumer ---------> News Feed Cache 
                                                            - Friends fetcher                     |
                                                            - User prefences data                 |
                                                                                                Notification service                     
                                                                         
                Dataflow with multiple Queues (like Spark, Flink) over Fanout service
                  - Offline
                  - Scale different systems independently 
                  - Durable system with help of Queues ( retry if failied )
            - Feed fetch flow
               - Client/Browser -> LB -> feed-fetcher (online path) -> News Feed cache (set of post ids)
                                                                    -> Post cache (post details) -> Post DB 
                                                                    -> User data (name, profile picture)
            - Need for caching to achieve us latency 
            - Others 
              - Scaling each component
              - DB - sharding, Replication and consistency different levels ( quorum or even read after write consistency for post, per user sequential consitency for news feed)
              - Multiple data centers 
              - Metrics monitoring 
          9. Design A Chat System
            - Polling (nw bandwidth) vs Long Polling (still makes periodic one-direction connections) vs WebSocket ( 2-way communication which starts with TCP handshake + TLS + Websocket upgrade)
            - Data model ? 
              - 1:1 : id, from, to, message, timestamp
              - Group : id, from, message, timestamp, group_id
            - High level design 
              - High level 
                - Client <-> Chat Server (websocket)              -> key value DB
                          -> API servers (help discover servers)  -> key value DB
                - Chat Server discovery with help of Zookeeper
              - 1:1 flow :  User A <-> Chat Server A -> Queue for User B -> Chat Server B <-> User B
                                                            |
                            User C <-> Chat Server A --------
              - Group  chat flow : similar to above where User A can send messsage to multiple Queue's for B and C both
                  Wechat still uses per user per message queue. So Group chats, user duplicates messages to all his/her friend's queue
              - Message flows 1:1 with synchronization on muliple devices by simply having each device storing max_current_message_id. 

        -------------------------------- Interesting open question on websocket ----------------------------

              Client <-> Websocket server vs Client <-> LB <-> Websocker server
                MODERN APPS USE MQTT ON TOP OF WEBSOCKET http://tinyurl.com/22dm8t6x 
                STILL A BIG QUESTION : https://stackoverflow.com/questions/65174175/how-do-websocket-connections-work-through-a-load-balancer 
                Nice implementatiom : https://dzone.com/articles/load-balancing-of-websocket-connections

                  - Websocket through LB is possible through tunneling and is possible with HaProxy LB : http://tinyurl.com/yurdrom2 
                  - NGINX LB also  supports WebSocket by allowing a tunnel to be set up between a client and a backend server. https://www.nginx.com/blog/websocket-nginx/ 
                  - Even companies like Whatsapp goes with this approach ( tried practically)

                Option 1 : Client <-> LB <-> Websocker server
                  Pros
                  - helps in scaling -  creates new socket connections, update existing HTTP connections to socket.  
                  - more security due to LB's reverse proxy capabilities
                  - easy to configure Websocket with available LBs like HaProxy or Ngnix 
                  
                  Cons
                  - connection limits (65K) per network IP interface. So LB becomes a bottleneck

                Option 2: Client <-> Websocket server implmented in https://dzone.com/articles/load-balancing-of-websocket-connections 
                  Pros 
                  - easier to scale
                  - A good way to implement is
                      1. client connects to LB which forwards requests to Service discovery 
                      2. Service discovery selects the best websocket chat server to select 
                      3. Client directly establishes connection to server's public IP
                  Cons
                  - security due to man in middle attack. This can be avoided by establishing Auth first 

                Option 3: connect with L3 LB 
                  https://stackoverflow.com/questions/12526265/loadbalancing-web-sockets?rq=4 

          --------------------------------- Interesting open question on websocket ----------------------------
                
              
            - Online Presence/ Status Fan out 
            - Others - support media files, E2E Encryption, Improve Load time by multi-datacenter, Retry with Queue
          10. Design A Search Autocomplete System
              - High level design (trie)
                  - option 1 : indexing on mysql 
                    - for example, get suggestions for "app"
                    - 
                                              B-tree Structure (3 levels, simplified):

                                                              ┌─────────────────────────────────┐
                                                              │     ROOT NODE                   │
                                                              │  Keys: "appalachian", "banana"  │
                                                              │  (split points)                 │
                                                              └──────────────┬──────────────────┘
                                                                            │
                                                              ┌──────────────┴──────────────┐
                                                              │                             │
                                                  ┌───────────▼──────────┐    ┌────────────▼────────┐
                                                  │   INTERNAL NODE 1    │    │   INTERNAL NODE 2   │
                                                  │ Keys: "apple"        │    │ Keys: "banana"      │
                                                  │ (split point)        │    │ (split point)       │
                                                  └──────────┬───────────┘    └──────────┬─────────┘
                                                            │                           │
                                                  ┌──────────┴──────────┐    ┌──────────┴──────────┐
                                                  │                      │    │                      │
                                          ┌───────▼──────┐    ┌─────────▼─────┐    ┌──────────────▼─────┐
                                          │ LEAF NODE 1  │    │  LEAF NODE 2  │    │   LEAF NODE 3      │
                                          ├──────────────┤    ├───────────────┤    ├────────────────────┤
                                          │ apple   →id1│    │ appalachian   │    │ apply       →id4  │
                                          │ appi    →id2│    │ →id3          │    │ apt         →id5  │
                                          └──────────────┘    └───────────────┘    │ banana      →id6  │
                                                                                    │ orange      →id7  │
                                                                                    └────────────────────┘
                  
                      - Query: "app%"

                          [ROOT] ──disk I/O──> [INTERNAL] ──disk I/O──> [LEAF 1]
                                                                          │
                                                                          ├─ Compare "apple" (string compare)
                                                                          ├─ Compare "appi" (string compare)
                                                                          └─disk I/O──> [LEAF 2]
                                                                                        │
                                                                                        ├─ Compare "appalachian"
                                                                                        └─disk I/O──> [LEAF 3]
                                                                                                      │
                                                                                                      ├─ Compare "apply"
                                                                                                      └─ Compare "apt" (STOP)
                      - Complexity : O(log n + k*m + disk_io_overhead)
                          where:
                            n = total number of words
                            k = number of words matching the prefix
                            String comparison per entry: O(m) where m = average word length
                  - Trie Data structure with frequency stored at every node with EOW = true 
                      Complexity :   O(m + k)
                          where:
                            m = length of prefix (e.g., "app" = 3)
                            k = number of words matching the prefix
                  - But to fetch top 5 suggestions by frequency, complexity O(m)  + O(k) + O(sort by frequency)
                    - if we limit prefix length to search say p=50 and cache top 5 search at every node, then complexity will be constant
                  -  Trie cache will be in memory in form of trie structure 
                  - Trie cache is updated daily via logs collected. So data gathering in the back-end (offline) to populate new values in the trie daily

                  Client/Browser -> API Server -> Trie Cache -> Trie DB 
                                                                    
                  Analytics Logs -> Data parser -> Trie Builder (workers) -> update periodically to Trie DB
 
              - Performance : Browser cache (header cache-control)
              - Smart sharding based on historical queries for equal load partition
              - Filter unwanted words 
              - Others - multi-criteria ranking, online update Vs offline batch update
          11. Design Youtube
              - High level design 
                - Upload flow (Grokking)
                    Browser/Client ->  API Service   ------------->    MetaData DB 
                          |                                                 ^
                          |(presigned url)                                  | update once transcoding is complete 
                          |                                                 |
                          |-------->Orginal Storage    -----------------> Transcode Service -----> Transcoded Storage (S3) -> CDN
                                                         (DAG - audio, video(resolution, fps, bit rate thumbnail watermark), metadata)

                - Why Video transcoding (different resolutions, video formats (HLS, DASH), fps, bit rate)
                - DAG model for video ( Video, Audio, Metadata. Video has further parallel step -> transcoding different resolutions, thumbnail, watermark)
                - Performance - CDN, GOP/multipart upload, download chunks during playing rather than whole video, upload service across datacenters, parallelism everywhere
                - Safety : Presigned URL for authorized users to upload/modify
                - Video Protection - AES encryption, watermarking 
                - Others - Live streaming, scaling the database
            12. Design Google Drive

              - High level design
                  Browser/Client  -> Block server (socket) -> Object storage 
                              |        | on completion 
                              |        V
                              | -->   LB.   -> API server -> Metadata DB 
                                                    | notifies
                                                    V
                                                  Notification service

              
              - Block server (avoid uploading entire file and instead focus only on changed lines by breaking into blocks, compression, encryption. The server supports delta detection)
                - why dropbox breaks into 4MB
                   - max block size is 4MB in block storage - http://tinyurl.com/2ya6opzs
                   - However most computer architectures support 4KB blocks due to page size in main memory, 4KB sector size in SSDs, avoids fragmentation. But modern architectures like ARM or RISC support configurable upto 4MB for large data workflows for i/o efficiency, reduced metadata overhead like HDFS 
                 - why File chunking, hashing and encryption on block server over client side
                  - client side is less secure, needs different implements on iOS, Android, WEB
                
              - High consistency on metadata DB 
                  - User -> workspace (id, user id) -> file (id, workspace id) -> versions (id, file id, version number) -> block (id, version id) 
              - Upload flow 
                  connects to API Server -> Gets Block server -> on completion of block upload -> Notification service
              - Download flow 
                  from notification service -> downloads metadata about block ids -> downloads blocks from block server
              - Save storage space (discard old versions, set a limit on versions, dedup blocks, move to cold storage)
              - How to resolve conflicts https://www.youtube.com/watch?v=0TJtwV29cjM&ab_channel=CrushingTechEducation 
                  - locks by line but its low performant
                  - operative transformation (google docs)
                      - both gets their local copy updated from local change
                      - block server reacts as master server and apply transformation on both of those local changes
                      - it is dependent on master server 
                  - using CRDT
                      - uses decimal indexes and avoids changing existing indices.
                      - clients with their indexes sends their local copy to block server which updates in DB supporting CRDT
                      - Block server notifies about the change to download. So this is eventually consistent
                      - CRDT : use union like in list based CRDT 
                          - use decimal points instead of integers. This gives the opportunity to insert in between without changing the original indices. Example - 0.2, 0.4 and you insert something in between like 0.2, 0.3, 0.4 
                          {(H,0.1), (i, 0.5)} U insert {(i,0.2), (!,0.6)} = {(H,0.1), (i, 0.2), (i, 0.5), (!,0.6)}
            13. Proximity Service
             
              - High level design
                  -  Browser/Client -> LB -> API Service -> DB

                  - MySQL look up is expensive due to latitude and longitude indexes are expensive 
                  - Other techniques
                      - use GeoHash (size of planet ~ 5K * 5K km ~ 5 * 10 ^3 ~ 10 ^ 4.  Each divides by 4. So total 6 level nested geohash = 4 ^ 6 = 4096 ) - Redis, Bing map, Lyft
                        - Pros 
                            - faster look up in computing and fetching business by geo hash 
                            - less operational overhead 
                        - Cons 
                            - popular cities will have many business under same geohash
                            - boundary issues (closer business in different geohash. So go with shared prefix with an exception that equator divison has no shared prefix.)
                      - use Quadtree (keep dividing into 4 quadrants till min number of business in each grid) - Yext
                        It is in-memory structure, so performing the calculations
                          - Each grid can store a maximal of 100 businesses
                          - Number of leaf nodes = ~200 million / 100 = ~2 million
                          - Number of internal nodes = 2 million * 1/3 = ~0.67 million. If you do not know why the number of internal nodes is one-third of the leaf nodes
                          - Total memory requirement = 2 million * 832 bytes + 0.67 million * 64 bytes = ~1.71 GB
                        - Pros 
                            - solves popular cities problem of Geohash
                        - Cons 
                           - in memory challenge and operational start up time to construct / update quad tree
                            - Estimate of time taken on 3GHz cpu ~ 3 * 10 ^ 9 cpu cycles /s 
                              - Balanced tree construction ~ c * n log n where c = # of cpu cycles per operation 
                              - c = 100 ( add/sub/comparison takes 5-10 cycles + avl tree balancing ~40 + L1/L2 cache miss ~40 + branch misprediction ~15 )
                              - so 100 * 2*10^6 * log base 2 (2*10^6) ~ 6 billion cycle ~ 6 s
                      - Google S2 - Google Maps, Tinder
                  Browser/Client -> LB -> API Service -> Geo hash DB (redis)
                                                      -> DB (business info)

              - Data model
                  business info
                    business id -> location info (lat and long), name, etc.
                  search info
                    geohash -> business id 
             
              - Sharding (not needed since entire dataset can fit in memory)
              - Performance - Multi data center set up
              - Country with privacy laws (only route traffic to that country's datacenter)
            14. Nearby Friends  
               - Server based over peer to peer network
                - P2P needs heavy client logic, high power consumption and flaky connections 
              - High level design 

                  Client ---> LB ---> API server         
                    |
                    |
                    |. ---------------> Nearby server   <----> Redis pub sub
                            (socket).        |
                                             |
                                             |
                                             DB (location history table and user_friends table)

                - leverage redis pub sub
              - Periodic location update 
              - Data model
                  - user id -> lat, long 
              - Why cache is needed ? - to store recent data with TTL
              - Scaling 
                - web socket, 
                - user DB, 
                - location cache, 
                - redis pub/sub lightweight 
                  -  How many servers 
                      - total is 10 million connections and 100B each connection and just 1 GB of memory. So its not memory bound. 
                      - Worst case ~10 million updates per second, so its IO bound and modern 1 Gbps network can holds  ~ 100K updates (100 MB/100B ~ 1M updates and /10 due to unexpected bandwidth) so need 100 servers for 10 million connections 
                  - redis pub/sub server will use consistent hashing with zookeeper having its address, 
                  - websocket servers register to the updates from zookeeper which stores the ring structure (example : node1, node2, node3, node 2, node 1, node 4) and hashfunction 
                  - websocket servers stores the all list of channel ids it subscribed to 
                  - using hashfunction and ring from zookeeper, it will know which redis pub sub server to talk too
                  - any change in ring structure will cause websocket servers resubscribing for all of its channel ids. 
                      - basically it computes the server using the hashfunction. 
                      - If the computed node is different, then it resubsribe

              - Operation savings : 
                  - User is by default subscribed to his/her friend's channel whether they are active or not. Saves the cost of new user joining the nearby service and having all his/her friend's subscribe to it.

              - Adding / removing friends is straightforward - just pass an event to subscribe/unsubscribe
              - User with many friends (not a problem since their friends channel would be distributed across)
              - Nearby random persion (use geohash as the channel)
              - Alternative to Redis pub-sub 
                  - Erlang having lightweight process which uses Akka pattern and support 10 million active users
                  
            15. Distributed Message Queue
              - point to point and pub-sub model 
              - topics, partition and brokers 
              - consumer group (multiple consumers in a group can access the message from the topic by subscribing to multiple partitions. To avoid sync issues, one partition could only be accessed by one consumer in the group)
              - high level design 
                  Producer -> Queue (brokers   holding  partition.   ) -----------------------------------------> Consumer group 
                                  |                           
                                Zookeeper                     
                   1. (offset of consumers, paritition info, etc.)      
                   2. Coordination service  
                      a. Service discovery: which brokers are alive.                                                
                      b. Leader election: one of the brokers is selected as the active controller. There is only one active controller in the cluster. The active controller is responsible for assigning partitions.
                      c. Apache Zookeeper [2] or etcd [3] are commonly used to elect a controller.


              - data storage (WAL which will perform sequential IO using disk. Each file could be like a segment. )
              - message structure (key, value, topic, partition, offset, timestamp, size, crc for intergrity)
              - router at producer and co-ordinator at consumer to join the parition
              - consumer - push vs pull, rebalancing using co-ordinator
                - Despite the naming similarity, the consumer group coordinator is different from the coordination service
              - replication - partitions replicas into multiple brokers , in sync replicas 
              - scalability 
                  - producer easy since no cordination needed, 
                  - consumer through coordinator, 
                  - queue broker 
                    - broker removal by creating a new parition plan and shifting replicas to available brokers and 
                    - broker addition by adding some replicas and remove them from existing brokers to split the load
              - At most once, Atleast once and Exactly once
              - Message filtering using tags 
              - Delayed and scheduled messages (temporarily storage)
              - Retry : failed consumed messages could be retried through dedicated Retry topic 
            16. Metrics Monitoring and Alerting System
              - Data model : timestamp, value, set of <key,value> labels 
              - line protocal which is used in Prometheus, OpenTSDB
              - choice of DB : TimeSeries (burst read, high writes)
                  - OpenTSDB is a distributed time-series database, but since it is based on Hadoop and HBase, running a Hadoop/HBase cluster adds complexity. 
                  - Twitter uses MetricsDB [10], and Amazon offers Timestream as a time-series database [11].
                  - Promotheus 
                  - For this design, we use InfluxDB because of scale - 8-core 32G ram, 250K writes/s, stores ~1M timeseries
              - High leve design 
                  Metrics Source -> Metrics Collector -> Timeseries DB -> Query Service -> Alert Manager (uses alert rules) -> Notification service 
                                                                                 -> Visualization 
              - Push vs Pull
                - Prometheus is pull based vs InfluxDB is pull based
              - Queue for Metrics Collector (parition by metrics name, topic by region, further tags for filters, )
              - Storage optimization (RLE and Delta compression, downsampling & retention )
              - Alerting system (how to dedup - check if alert message is already provided)
              - Visualization system - Grafana as open source
            17. Ad Click Event Aggregation
              - Data model
                - star schema (fact table storing denormalized form and dimension table derived from fact for Aggregated data. To avoid indexing on every column on dimension table, Use filter id technique to further have faster retrieval)
              - Keep Raw incoming data 
              - DB : Cassandra 
              - High level design 
                  Logs/collection source -> Producer -> Queue -> Consumer aggregation (map reduce job) -> DB -> Query Service 
              - Streaming vs Batching (use both : Kappa architecture like Flink) 
              - Data recalculation using raw DB or Queue 
              - Time : event time vs processing time. Use event time but know the gap = processing - event time. 
              - Watermarking : use the above gap as extended interval 
              - Tumbling window vs Sliding window 
              - Delivery guarantee (exactly once using offset id and storing it only when Aggregator saves in DB)
              - Scaling : Aggregator using consumer rebalancing, Map reduce using YARN, Cassandra supports consistent hashing
              - Fault tolerant : use checkpoint barriers like in Flink to store the snapshot 
              - Others - Data monitoring (Queue size, latency, CPU/RAM), Reconcialiation 
            18. Hotel Reservation System
              - API design
              - Data model (SQL for consistency and read more, write less)
              - Improved data model since room_id cannot be decided at booking time 
                  - hotel (hotel id, ..)
                  - user (user id, ..)
                  - room_type_inventory (hotel id, room type id, # available)
                  - reservation (reservation id, hotel id, room type id,  user id, start date, end date)
                  - room (room id, room type id, floor) populated during check-in
              - Idempotency through checkout id
              - concurrency issues 
                - Pessimistic locking (like Serializable isolation, low performant and block the reads, depends on DB support for locking)
                - Optimistic locking 
                  - using versioning (like in serializable snapshot isolation but has write contention)
                  - using unique key constraint (depends on DB support and has write contention)
                Optimisitic locking makes more sense due to low writes 
              - DB sharding (do we really need it - No based on requirements)
              - what if DB is sharded (then we need distributed transaction like 2PC or Saga )
            19. Distributed Email Service
              - SMTP for sending emails and at times receiving too. But IMAP/POP3 famous for receiving 
              - How to get address of email servers - using nslookup domain_name
              - Attachment - MIME support 
              - High level design 
                - Email Sending flow 
                    Client -> LB -> Web Server -> Outgoing queue (error queue) -> SMTP Outgoing (check spam, virus and retry) -> Internel
                                      |
                          Storage (metadata, search store, object store, cache)
                - Email receiving flow 
                    Internel -> LB -> SMTP Server -> Incoming queue -> Mail processor (check spam, virus) -> Web socket server -> Client 
                                          |                                                               -> Notification service if offline
                                 Storage (metadata, search store, object store, cache)
              - Metadata DB (relational -> user_folder (user_id, folder_id),  folder_emails(message_id, folder_id, from, to, subject, link_to_s3, is_read, etc.))

              - Queries : 
                1. get all folders of users : parition key (user id) 
                2. get all emails from folders ->  folder_id + sort key (message_id which is time based UUID)
                - Email threads could be identified by using message_id
                3. get all read + unread emails : extract all using userid+folder_id and perform filtering but its expensive to perform filtering at large scale. So use denormalized tables for read and unread emails.
              - Email deliverability 
                - dedicated IPs, 
                - Separate servers for promotion emails, 
                - Email sender reputation (Warm up new email server IP addresses slowly to build a good reputation, so big providers such as Office365, Gmail, Yahoo Mail,    
                    etc. are less likely to put our emails in the spam folder.), 
                - Ban spammers, 
                - Feedback processing, 
                - Good relationship with ISPS
              - Authentication : SPF (sender policy framework, Domain based message authentication). As an example, you can see that in "signed-by" section to every email
              - Search store : elastic search out of box 
              - Others : Compliance using GDPR, Storage optimization by deduping attachments
            20. S3-like Object Storage
              - Block vs File vs Object (cheap, rest api support, high durability)
              - High level design 
                  Client -> LB -> API Service -> Metadata Serice -> Metadata DB 
                                              -> Data store service (data routing, placement service like resource manager) -> Object DB
                  
                  Metadata DB  - (bucket table (bucket_id, details), bucket_objects (object_name, bucket_id, object_id, version_id) table)

                  Upload flow : store basic info in Metadata DB, uploads the file via Data store service, gets the object id, stores in Metadata DB
                  Download flow : Using bucket name and object_name, looks up the Metadata DB for object id and fetch the object from data store service
              - Data store (append only, different files as segments, separate object_mapping table : object id -> file_name, offset, data size, checksum)
              - Durability 
                  - ( more replicas vs parity bits which is optimized for space but low speed because of parity computation)
                  - Example 
                    - with replicas : 4 bit data (d1, d2, d3 and d4) will have its copies. So its  2 * (d1, d2, d3 and d4) ~ 8 
                    - with parity : 4 bit data (d1, d2, d3 and d4) will only have its 2 parity bits. So its  2 + (d1, d2, d3 and d4) ~ 6
              - Integrity using checksum 
              - Metadata sharding 
                  - shard by bucket name, object name since most queries are based on that
                  - but what about query to list all objects -> use separate denormalized table to store bucket name -> list of object ids 
              - Object versioning 
              - Multipart upload (breaks the file into muliple parts, server sends eTag for each part, after finish clients sends all etags in order for server to resemble and perform ordering )
              - Garbage collection : same as compaction process in LSM+SS Table
            21. Real-time Gaming Leaderboard
              - High level design 
                  Client -> Game service post score to -> Leadboard service -> DB 
                          \                               /
                            \___________________________/
                              use for read queries only
                  Data model (user id, game_id, score)
              - Scale using Redis sorted set (hashtable + skip list)
                  add scores using ZINCRBY leaderboard_feb_2021 1 'mary1934'
                  get top 10 using ZREVRANGE leaderboard_feb_2021 0 9 WITHSCORES
                  fetch rank using ZREVRANK leaderboard_feb_2021 'mary1934'
                  if above returns 361, fetch 10 nearby ranks using  ZREVRANGE leaderboard_feb_2021 357 365
              - Paritioning 
                  - fixed partition using scores which is range partitioning (easy for reads esp. top 10, hard for writes since users will keep moving across parition for score updates)
                  - hash slot based partition based on key (hard to get top 10, need to read every shard). One option is you can return scores in percentiles which is rank within shard.
              - Other DB option is Dynamo which uses consistent hashing 
                  - If game_id, date is used as shard, then what about hot shard for recent game. Add random number across the key and distribute it across the shard. Only challenge is it adds reads complexity to get partition #. 
                  - One option is you can return scores in percentiles which is rank within shard.
              - How to implement  report scores if there is a Tie (may be one which got the score first - so timestamp based)
            22. Payment System 
                PLEASE REFER THIS VIDEO : http://tinyurl.com/2978ktbs 
              - High level design 
                  Pay-in flow

                    Client  -> LB -> Payment service -> Payment executor -> PSP (Stripe, Paypal)
                                      |         |               |
                                      |         |               |
                              Ledger Service.  Wallet Service   DB
                                    
                  Data model 
                      (checkout_table -> checkout id, buyer info, credit card info, payment_status, payment_token from PSP )
                      (order_details -> order_id, checkout_id, seller_id, amount, currency, ledger_updated, wallet_updated, payment_status)
              - Double entry ledger 
                  - The double-entry system states that the sum of all the transaction entries must be 0. 
                  - Account	Debit	Credit
                      buyer	  $1	
                      seller		    $1
              - Hosted payment page 
              - PSP integration ( Our payment executor creates nonce using checkout id and sends to PSP and at same time redirects user to PSP hosted page with redirect URL back to our page when payment is complete. Also we provide webhook to PSP to notify success ).
              - Reconcilliation (automated, fixed with code and manual)
                - Every night the PSP or banks send a settlement file to their clients. The settlement file contains the balance of the bank account, together with all the transactions that took place on this bank account during the day. The reconciliation system parses the settlement file and compares the details with the ledger system.
              - Handle processing delays using webhook URL or polling
              - Handling failed payments with Retry + dead letter queue
              - Exactly once delivery using idempotency on payment token OR nonce (checkout id)
              - Security 
                  - use HTTPS, Enforce encrpytion, DB replication across regions, Rate limiting to avoid DDOS, Card theft by tokenization, PCI compliance, Address verification + card verification value (CVV)
            23. Digital Wallet
              - High level design 
                  Client -> LB -> Wallet Service -> DB 

              - But what if DB is sharded 
                1. 2PC (lock + commit) 
                  - depends on DB support, XA standard to implement it for heterogeneous systems. 
                  - Cons 
                    - The biggest problem with 2PC is that it’s not performant, as locks can be held for a very long time while waiting for a message from the other nodes. 
                    - Another issue with 2PC is that the coordinator can be a single point of failure
                2. TCC (Try Confirm/Cancel)  - say transfer $1 from account A in database A to account B in database B
                  - In Try phase, it executes minus transaction in database A and no-op for database B. If both calls are successful, then it goes ahead with Confirm phase where it has no-op transaction in database A and add transaction for database B. If try or confirm phase is NOT successful, then it goes to cancel where it undo all the operations.
                  - Cons : 
                    - leaves DB in inconsistent state but application has full control to handle it. 
                    - Same with 2PC, needs co-ordinator support.
                3. Saga - Best choice for microservice. Can be Decentralized and Orchestration using co-ordinator but unlike TCC, its slow. 
                For 2 and 3 to handle unbalances state, use phase status table (transaction_id, phase_type, name, status, out_of_order flag for network lag)

              - Event sourcing with materialized views 

                So  With Saga or TCC
                      Client -> LB -> Wallet Service (Orchestrator) ---> Commands -->  Queue  --> Consumer/Executor ->  DB partition 1
                                          |                           to Partition 1                      |
                                          |                            which has A                       State DB
                                  queries State DB for status           balance 
                                                                    ---> Commands -->  Queue  --> Consumer/Executor --> DB partition 2
                                                                     to Partition 2                       |
                                                                     which has B                        State DB
                                                                     balance  
                                                                    
            24. Distributed Job scheduler
              - High level design
                                        + Retry
                  Jobs -> Producer ----> Queue -------> Consumer (Driver) ----------(schedule)--------------> Workers 
                              |                            |
                              |                            | (which hosts to schedule)
                              V                            V
                             DB                        Resource manager(YARN)  ---------------------> Workers 
                                                                               (query host resources)
                  Resource manager 
                    - based on resources available 
                  
                  Workers 
                    - cgroup to limit resources per job
                    - promise cache for stream-table join
                    - low frequency checkpointing 
                    - async i/o support for jobs


            25. Skyscanner ( price drop alerting system)
              - https://excalidraw.com/#room=47267ffe5868ab0f2007,UNO6JqgrtVZwYNRVl3G0sw 
              IF YOU HAVE MORE THAN 1 CONTINOUS COMPONENT
                alert-service (fan-out service)
                  1. query DB and getting routes
                  2. query APIs for routs
                  3. form the url and send to price-price to get e price
                  4. ask the eligibility service
                  5. send to notification

              - PIPELINE over FAN OUT DESIGN 
                - When you do not need to send acknowledgement back to user ( not a deal breaker since you can provide status api to user )
                - When your dependent jobs and time-consuming 

              - FAN-OUT DESIGN Disadvantages 
                - single point of failure
                - Failure in one results in restarting the job (lose durability)
                - A dependent time consuming job keeps the resources blocked 


      d. Some well-known challenges 
        1.1 ALWAYS CHOOSE QUEUE WHEN YOU HAVE ASYNC SYSTEM AND MORE THAN ONE CONTINOUS SYSTEM https://excalidraw.com/#room=47267ffe5868ab0f2007,UNO6JqgrtVZwYNRVl3G0sw 
        1.2 Why Queue
            - scaling independently 
            - handles backpressure
            - increase durability (no data loss)
        1.3 Queue challenges
          a. Exactly once if queue is used. 
            - retry on producer end 
            - idempotency on consumer end with additional consideration if you have already processed downstream or not (use rolling bloom filter for checking which does garbage collection based on watermarked time. https://cloud.google.com/dataflow/docs/concepts/exactly-once#latency )

          b. Also Metrics monitoring on queue for time sensistive operations like notification system. 
              - Also event tracking for analytics service which can help visualize the state (start, pending, in progress, complete) of the system.
        2. Consistency always with CQRS(sep read and write systems) or 
        3. Reconciallation for stream + batch DBs or payment system
        3. Idempotency for retry systems
        4. NTP sync if relying on timestamps
        5. Map-Reduce scaling, hot keys, priority pre-emption
        6. Verify Fault tolerence, Resiliency and durability
            For fault tolerance(operable) - enough replicas, data mirroring 
            For resilient (auto-repair) - Retry with circuit breaker, autoscale, auto-heal
            For durability(no data loss):  data mirroring , Commit log, Snapshotting/checkpointing, use Kafka 
        7. For Accuracy 
                
          a. Discuss Isolations if needed : 
              - Read committed by default to guarantee no dirty reads and writes. (default MySQL, Postgresql)

              - Check if you need Snapshot isolation to avoid non-repeatable reads (Oracle, MySQL).  Snapshot isolation solves by reading from consistent snapshot of DB. Great example here - https://techcommunity.microsoft.com/t5/sql-server-blog/serializable-vs-snapshot-isolation-level/ba-p/383281 

              But what if an update in target row could cause side effect in other rows which is also called write skew (eg., check unique username or atleast one oncall or no meetings for that room at time X) ? 
              - When it read from consistent snapshot, username was not unique or 1 oncall was present or no meeting at that time. But when writing, multiple users try to write it at the same time causing incorrect state of DB with 2 duplicate users or 0 oncall or meeting overbooked.

              - Solution is Serializable isolation (CockroachDB for distributed and Postgresql for single node) for phantoms(write skew) where you cannot reserve locks on returned rows and write in one transaction say row X,Y affects the other say row A,B 
                - keep in mind that 
                  - unique username COULD BE SOLVED with unique constraint also. But unique constraint creates another unique index which slows down writes 
                  - meeting rooms COULD BE SOLVED with adding rows based on future time slots and creating lock per row. This is process is called materialization of conflicts but its not always feasible (too many wastage of space esp. for companies with 10K meeting rooms ).  

                FOR MOST SQL DATABASES, WRITES TO B+ tree based FILE OR LOCATION IS LOCKED TO AVOID WRITE CONFLICTS ??  
              - Serializable snapshot isolation : similar to snapshot isolation with conflict detection and rejecting the writes like in postgresql. https://www.postgresql.org/docs/current/transaction-iso.html#XACT-REPEATABLE-READ 
          b. For distributed DBs, Concensus esp. for consistency on distributed leaderless DBs like Cassandra
              Trade-offs with strict isolation / concensus VS latency / availability 

        9. Distributed transactions if it involves DB tables on different nodes or heterogeneous systems
            - 2PC or TCC(Try + confirm/cancel) or Saga (new microsservice design pattern for DBs which do not support 2PC)
            Example balance transfer 
        10. Zookeeper for any of the operations : 
              - Service discovery + 
              - Concensus by providing total order broadcast + (vs Raft - https://stackoverflow.com/questions/47760448/what-is-the-difference-between-zookeeper-and-raft )
              - leader election + 
              - failure detection + 
              - subscribe to config change updates + 
              - consistent, high available state storage
            Zookeeper is best deployed for small pieces of state where linearizability and high availability is critical. (https://aphyr.com/posts/291-call-me-maybe-zookeeper)
        11. Security : 
              - Storing Personal Data : encrtyped form 
              - Rate limiter to avoid DDOS
              - TLS + JWT token based Auth 2.0 
        12. Reduce Storage Size
            - move infrequent data to Amazon S3 glacier
            - avoid short-term checkpoints
            - retention policies after 1+ year
            - compression - RLE, Delta compression for timeseries
        13. Latency 
            - caching 
                - distributed cache, CDN 
            - bloom filter for look ups
            - materialized views for frequent queries (postgresql support "refresh materialized view" command)
        14. Hot shard 
            - spread the load across additional nodes 
              1. more replicas like for cassandra but its for entire table (https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#partitioning-and-replication)
                - Smart strategy for resolving hotspot on per key basis :
                  - detect the hot spot key using sampling method (like in Pig) or dashboard (like in CockroachDB)
                  - for that hotspot key, assign the copy number to each key and create combined partition key for Cassandra (https://stackoverflow.com/a/27889289/1540256 )
              - more reducers for Map-Reduce framework pg 408 DDIA
              Q. How to detect hot shard - pg 407 DDIA
                - skewed join method in Pig runs a sampling method to detect hot keys
                - But Hive requires hot keys to be explicitly specified. It uses Map side joins for hot keys  
        15. output from mapreduce is Write once, Read everywhere strategy.
            - use B tree then (Pg 413 DDIA)
            - but if bulk writes, then use LSM tree 
        16. if a client owns the lock from zookeeper to access the data but goes on GC pause. This will starve other client
            - use fencing (lease only for sometime) - Pg 303 DDIA
        17. How to ensure consistency with Queue + DB pipeline (event passed but the DB read still does not have it due to evenutal consistency)
           1. linearizable using total order broadcast. - Pg 351
              - if you know latest position of log message, then your read can wait for that position. (example Zookeeper sync() operation works similarly) - https://www.youtube.com/watch?v=F06tdYgcz_A&ab_channel=Jordanhasnolife 
              - another technique is post dummy message to the queue, then wait for the message to appear in consumer and then you can start reading the values.(examle quorum reads in etc work similar)
              Q. But what if those events belong to different partitions in the queue ? - Pg 462 DDIA
        18. How will you ensure exactly once processing for stream processing 
          1. using checkpoint barries like in Flink - Pg 477 
            Q. But what about downstream changes in database. if you retry the stream, then the database state will be incorrect - how will you ensure atomicity then ?
              1. This is hard since it needs atomic commit (XA - standard for 2PC). Also further hard to ensure atomic commit on heterogeneous systems (example, between Queue and DB)
              Google Cloud Dataflow, Storm, Kafka is going to support this but only with homogeneous systems - like 2 tables in database
        19. how will ensure idempotency with stream process (same result when applied multiple times) - Pg 478
            - like above, checkpoint markers will solve this for the stream but for downstream like DBs -> you can store the offset along with the DB update. This offset will tell if this message was already stored in DB or not. 
        20. Whats the best way to support CDC ? 
            - DB changes -> log based message passing (kakfa) -> different sources like Search index, Database, materialized views
            Kafka already log compaction to avoid storing outdated data in its partition
            All relational databases (MySQL, Oracle, Postgresql) have CDC to Kafka. Additional Debezium is the tool to support both relational and non-relational DB like Cassandra (Enable CDC on cassandra.yaml - http://tinyurl.com/ytw3ogbu  ). 
        21. How to ensure recency guarantee ? - Linearizeability 
            - Total ordering
              - using sequence number from Lamport timestamp (just the counter to record to indicate the order and help whats more updated. It is more compact ) or Zookeeper monotonically increasing transaction id
              - using Quorum consistency (R+W > N) like Dynamo or strong consistency
              - very strict : serializable isolation with quorum consistency 

            However in total ordering, you cannot tell if 2 events are concurrent. You need causal ordering for that.
            - Causal ordering 
              - Version vectors (Partial ordering) :  can check if 2 operations are concurrent or causally dependent on one other. Riak 2.0 uses version vectors - Pg 184 
              - Vector clocks 
                - Pg 346 - MongoDB, Dynamo, Cockroach DB uses vector clocks. 
                - Refer https://cs.stackexchange.com/questions/101496/difference-between-lamport-timestamps-and-vector-clocks 
                - Nice video : https://youtu.be/1BXCxpcsmzc?t=524 
            
        22. How to ensure good Job placement for a Job manager / resource manager ? 
          https://medium.com/pinterest-engineering/tuning-flink-clusters-for-stability-and-efficiency-50d3d50384ed 
            - place it with resource awareness (CPU aware) , 
            - place it with machine type awareness based on bottleneck resources (cpu-instance hardware vs memory instances) 
            - place soft limits on jobs using Cgroups and some proportion reserved for burst capacity (each job needs during start up). However, also look for ways to avoid internal fragmentation / banding.     
            - place consecutive subtasks belong to same job on same hosts  (it also reduces network bandwidth)
            - Premptive priority based scheduling for resource constraints
              - https://www.geeksforgeeks.org/preemptive-priority-cpu-scheduling-algortithm/# 
              - Other method is SJFS (Shortest job first)
            - For software efficiency, support async I/O to avoid memory consuming threads blocked on I/O and not being able to utilize CPU
            - For demand efficiency, identify bottlenecks for jobs which does overfetching 

        23. How to effectively do multi-leader replication ? (all relational DBs, Kafka, Redis)
            Great video : 
            (Multi-leader) https://www.youtube.com/watch?v=1BXCxpcsmzc&ab_channel=Jordanhasnolife 
            (Leaderless) https://www.youtube.com/watch?v=atDe7qka6q8&ab_channel=Jordanhasnolife 
            
            5 techniques : 
              - Master-Replica model : conflict avoidance (just make writes of an item from one leader). 
                - However, what if a master gets message 1, 2 in order but replica gets 2, 1 ? 
                  - Its highly unlikely since most DBs sends in replication log. Replica will remember its last offset and start reading from 1 and then 2.
              - Leaderless/MultiLeader : LWW
              - Leaderless/MultiLeader :Conflict on read (calls conflict resolution code on read like Dynamo read-repair /Azure CosmosDB/CouchDB read-repair)
              - Leaderless/MultiLeader :Conflict on write (calls conflict resolution code on write like CRDTs)
              - Leaderless/MultiLeader :Version vectors (like in Riak 2.0) 

              Multi-leader replication is implemented with Tungsten Replicator for MySQL, BDR(bi-directional replication) for PostgreSQL or GoldenGate for Oracle. MySQL uses circular topology for its replication.

            - the common use case is multi-datacenter need where there is a leader in each datacenter
            - make sure there is atleast quorum consistency ( strong ) consistency in each datacenter
            - writes/reads of a particular user/session go to same datacenter
            - for conflicts, use different strategies shared above
              - some details about version vectors - (also helps with ordering) - RiakDB
                https://youtu.be/1BXCxpcsmzc?t=524 
                - merging siblings are hard even with version vectors. You need to use something advanced called CRDTs (Riak DB, Redis, Cassandra)
                    - How CRDT is implemented ? - https://www.youtube.com/watch?v=FG5Varj1Ows 
                        - Operational CRDT : just the operation (like inc or dec) is sent instead of vectors. usually used in counters.
                        - State based CRDT : entire vector is sent (like vec[add(1,2), remove(1)] ). It helps with causal relationship. usually used in adding elements to the list
                        - Sequence CRDT : builds an eventually consistent list. Its hard. used in collaborative editors like Google docs. Example, using decimal and placing random number range in between decimals for insertion. [(H, 0.1), (!, 0.3)] U [(H, 0.1), (i, 0.2), (!, 0.3)] -> H!(already in DB) + Hi!(node2) -> Hi!
              - for ordering, you can use sequence number from lamport timestamp - recency guarantee (just incrementing version to get latest versions) 

              ONE FACT WITH RELATIONAL DBS : 
                To order these events correctly, a technique called version vectors can be used, which we will discuss later in this chapter (see “Detecting Concurrent Writes” on page 184). However, conflict detection techniques are poorly implemented in many multi-leader replication systems. For example, at the time of writing, PostgreSQL BDR does not provide causal ordering of writes [27], and Tungsten Replicator for MySQL doesn’t even try to detect conflicts [34].

              Examples of Leaderless replication : Riak, Cassandra, Dynamo.
              So how does multi-datacenter operation works for leaderless replication ? 
                  Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link.

        24. How to maintain data consistency between databasesm internal services, internal service + third party ? 
              - To maintain data consistency between replicas, use strong consistency ( quorum consistency) or eventual consistency.
              - To maintain data consistency between internal services, ensuring exactly-once processing is very important.
                  - retry on producer end 
                  - idempotency on consumer end (use rolling bloom filter for checking which does garbage collection based on watermarked time. https://cloud.google.com/dataflow/docs/concepts/exactly-once#latency )
              - To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency and reconciliation.
        25. How to handle spiky load ? 
          - always reserve SPIKE buffer
          - use rate limiter 
          - for multi-tenants, support quota system for demand 
          - use promise cache 
          - use request collapsing in LB
        26. How to tolerant faults ? 
            https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#reliable-scalable-and-maintainable-applications 
            A fault is usually defined as one component of the system deviating from its spec, whereas failure is when the system as a whole stops providing the required service to the user.

            You should generally prefer tolerating faults over preventing faults.

            Hardware faults. 
              Until recently redundancy of hardware components was sufficient for most applications. As data volumes increase, more applications use a larger number of machines, proportionally increasing the rate of hardware faults. There is a move towards systems that tolerate the loss of entire machines. A system that tolerates machine failure can be patched one node at a time, without downtime of the entire system (rolling upgrade).
              It is unlikely that a large number of hardware components will fail at the same time.
            Software errors. 
               Software errors are a systematic error within the system, they tend to cause many more system failures than uncorrelated hardware faults.
            Human errors.
              Humans are known to be unreliable. Configuration errors by operators are a leading cause of outages. You can make systems more reliable:
              1. - Telemetry - Metrics Monitoring set up. Availability + Reliability (latency + error rate).
              2. - Code quality : Test coverage with unit and e2e tests and maturity level from tools like sonarcube.
              3. - Sandbox : Provide fully featured non-production sandbox environments where people can explore and experiment safely.
              4. - Rollback using config : Quick and easy recovery from human error, fast to rollback configuration changes, 
              5. - Rolling or Canary deployment : roll out new code gradually.

              Others
              - Minimising the opportunities for error, peg: with admin interfaces that make easy to do the "right thing" and discourage the "wrong thing".
              - Implement good management practices and training.

          27. Whats the reasonable performance SLA ? 
              Service level objectives (SLOs) and service level agreements (SLAs) are contracts that define the expected performance and availability of a service. An SLA may state the median response time to be less than 200ms and a 99th percentile under 1s. These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met.
          28. Approaches for coping with the load ? 
                  Scaling up or vertical scaling: Moving to a more powerful machine
                  Scaling out or horizontal scaling: Distributing the load across multiple smaller machines.
                  Elastic systems: Automatically add computing resources when detected load increase. Quite useful if load is unpredictable.
                Distributing stateless services across multiple machines is fairly straightforward. Taking stateful data systems from a single node to a distributed setup can introduce a lot of complexity. Until recently it was common wisdom to keep your database on a single node.
          29. 3 principles of Maintainability (SEO)
                  
                  Simplicity. 
                    Easy for new engineers to understand the system by removing as much complexity as possible. (abstraction that hides the implementation details behind clean and simple to understand APIs and facades.)
                  Evolvability. 
                    Make it easy for engineers to make changes to the system in the future. (Agile working patterns provide a framework for adapting to change.)
                  Operability. 
                    Make it easy for operation teams to keep the system running. (monitoring, logging, security, up to date, etc.)
              How to measure ? - Varies by domain to domain. Its advisable to define "Maturity level"
          30. Relational vs Document model
              https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#query-languages-for-data 
              The main arguments in favour of the document data model are schema flexibility, better performance due to locality, and sometimes closer data structures to the ones used by the applications. The relation model counters by providing better support for joins, and many-to-one and many-to-many relationships.
              Document databases are sometimes called schemaless, but maybe a more appropriate term is schema-on-read, in contrast to schema-on-write.Schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking.
              Convergence of document and relational databases
                PostgreSQL does support JSON documents. RethinkDB supports relational-like joins in its query language and some MongoDB drivers automatically resolve database references. Relational and document databases are becoming more similar over time.
          31. What are Query languages for data
                SQL is a declarative query language. In an imperative language, you tell the computer to perform certain operations in order.
                In a declarative query language you just specify the pattern of the data you want, but not how to achieve that goal. A declarative query language hides implementation details of the database engine, making it possible for the database system to introduce performance improvements without requiring any changes to queries.

                  A usability problem with MapReduce is that you have to write two carefully coordinated functions. A declarative language offers more opportunities for a query optimiser to improve the performance of a query. For there reasons, MongoDB 2.2 added support for a declarative query language called aggregation pipeline
          32. When to use graph like data model 
              If many-to-many relationships are very common in your application, it becomes more natural to start modelling your data as a graph. A graph consists of vertices (nodes or entities) and edges (relationships or arcs).
              There are several ways of structuring and querying the data. The property graph model (implemented by Neo4j, Titan, and Infinite Graph) and the triple-store model (implemented by Datomic, AllegroGraph, and others). There are also three declarative query languages for graphs: Cypher, SPARQL, and Datalog. Cypher is a declarative language for property graphs created by Neo4j
          33. Types of indexes ? 
              https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sstables-and-lsm-trees 
              Hash tables index - Riak (Bitcask)
                The simplest indexing strategy is to keep an in-memory hash map where every key is mapped to a byte offset in the data file. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote.
                Bitcask (the default storage engine in Riak) does it like that. The only requirement it has is that all the keys fit in the available RAM. Values can use more space than there is available in memory, since they can be loaded from disk.
                As we only ever append to a file, so how do we avoid eventually running out of disk space? A good solution is to break the log into segments of certain size by closing the segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform compaction on these segments. Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.
                Hash table has its limitations too:
                The hash table must fit in memory. It is difficult to make an on-disk hash map perform well.
                Range queries are not efficient.

              LSM Tree + SSTable (ElasticSearch's Lucene indexing engine, Solr, Cassandra, HBase, RocksDB)
                We introduce a new requirement to segment files: we require that the sequence of key-value pairs is sorted by key.
                We call this Sorted String Table, or SSTable. We require that each key only appears once within each merged segment file (compaction already ensures that). SSTables have few big advantages over log segments with hash indexes.
                  - Merging segments is simple and efficient (we can use algorithms like mergesort). When multiple segments contain the same key, we can keep the value from the most recent segment and discard the values in older segments.
                  - You no longer need to keep an index of all the keys in memory. For a key like handiwork, when you know the offsets for the keys handback and handsome, you know handiwork must appear between those two. You can jump to the offset for handback and scan from there until you find handiwork, if not, the key is not present. You still need an in-memory index to tell you the offsets for some of the keys. One key for every few kilobytes of segment file is sufficient.
                  - Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk.

              B Tree
                One page is designated as the root and you start from there. The page contains several keys and references to child pages.
                If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk. If you want to add new key, find the page and add it to the page. If there isn't enough free space in the page to accommodate the new key, it is split in two half-full pages, and the parent page is updated to account for the new subdivision of key ranges.

                Trees remain balanced. A B-tree with n keys always has a depth of O(log n).

                The basic underlying write operation of a B-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page, all references to that page remain intact. This is a big contrast to log-structured indexes such as LSM-trees, which only append to files.

                Some operations require several different pages to be overwritten. When you split a page, you need to write the two pages that were split, and also overwrite their parent. If the database crashes after only some of the pages have been written, you end up with a corrupted index.

                It is common to include an additional data structure on disk: a write-ahead log (WAL, also know as the redo log).

                Careful concurrency control is required if multiple threads are going to access, typically done protecting the tree internal data structures with latches (lightweight locks).
            34. In memory DBs 
                  - Products such as VoltDB, MemSQL, and Oracle TimesTime are in-memory databases with disk access. Redis and Couchbase provide weak durability. Although redis can be extended to periodic (1 min) snapshotting to disk.
            35. Data warehouse examples 
                  Amazon RedShift is hosted version of ParAccel. Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill. Some of them are based on ideas from Google's Dremel.
                  Data warehouses are used in fairly formulaic style known as a star schema.
                  The name "star schema" comes from the fact than when the table relationships are visualised, the fact table is in the middle, surrounded by its dimension tables, like the rays of a star.
            36. How does column oriented DB work 
                  Column-oriented storage is simple: don't store all the values from one row together, but store all values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in a query, which can save a lot of work.

                  Column-oriented storage often lends itself very well to compression as the sequences of values for each column look quite repetitive, which is a good sign for compression. A technique that is particularly effective in data warehouses is bitmap encoding.
                  Challenge with bitmap encoding : 
                    1. High Cardinality: Columns with many distinct values (e.g., UserID) are poorly suited, as storing a bitmap per value becomes inefficient.
                    2. Sparse Data: If most bits in a bitmap are 0, compression gains diminish (though techniques like run-length encoding can mitigate this).

                  Remember : Cassandra and HBase have a concept of column families, which they inherited from Bigtable. THIS IS DIFFERENT FROM COLUMN-ORIENTED STORAGE
            37. How messages can be transfered over the network ? - Pg 116
                say JSON message : { key : value} 
                  1. binary encoding (REST which uses base64)
                  2. thrift
                      Thrift offers two different protocols:

                        - BinaryProtocol, there are no field names like userName, favouriteNumber. Instead the data contains field tags, which are numbers (1, 2)
                        - CompactProtocol, which is equivalent to BinaryProtocol but it packs the same information in less space. It packs the field type and the tag number into the same byte.
                  3. protocol buffers (gRPC)
                      Protocol Buffers are very similar to Thrift's CompactProtocol, bit packing is a bit different and that might allow smaller compression.
                  4. Avro (Hive, Kafka)
                      Same as above and supports two schema languages : Avro IDL (same as thrift IDL) and one is JSON based (machine-readable.).
                      It compresses even further - it does not even specify the data type in its encoding. So you go go through the fields in the order they appear in the schema and use the schema to tell you the datatype of each field. Any mismatch in the schema between the reader and the writer would mean incorrectly decoded data.
                      The key idea with Avro is that the writer’s schema and the reader’s schema don’t have to be the same—they only need to be compatible. When data is decoded (read), the Avro library resolves the differences by looking at the writer’s schema and the reader’s schema side by side and translating the data from the writer’s schema into the reader’s schema. The Avro specification [20] defines exactly how this resolution works, and it is illustrated in Figure 4-6.
              38. How to rebalance partitions ? 
                    - How not to do it: Hash mod n. The problem with mod N is that if the number of nodes N changes, most of the keys will need to be moved from one node to another.
                    - Fixed number of partitions. Create many more partitions than there are nodes and assign several partitions to each node. If a node is added to the cluster, we can steal a few partitions from every existing node until partitions are fairly distributed once again. The number of partitions does not change, nor does the assignment of keys to partitions. The only thing that change is the assignment of partitions to nodes. This is used in Kafka brokers(refer kafka system design), Riak, Elasticsearch, Couchbase, and Voldemport. You need to choose a high enough number of partitions to accomodate future growth. Neither too big or too small.
                    - Dynamic partitioning. The number of partitions adapts to the total data volume. An empty database starts with an empty partition. While the dataset is small, all writes have to processed by a single node while the others nodes sit idle. HBase and MongoDB allow an initial set of partitions to be configured (pre-splitting).
                    - Partitioning proportionally to nodes. Cassandra and Ketama make the number of partitions proportional to the number of nodes. Have a fixed number of partitions per node. This approach also keeps the size of each partition fairly stable. Example, In consistent hashing when new node is added, only its preceding neighbour node keys are affected.
              39. Is rebalance have to be manual ? 
                    Fully automated rebalancing may seem convenient but the process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.
                    It can be good to have a human in the loop for rebalancing. You may avoid operational surprises.
              40. How does the client knows to connect to right partition at right node ? 
                    This problem is also called service discovery. There are different approaches:
                      - Allow clients to contact any node and make them handle the request directly, or forward the request to the appropriate node.
                      - (Most common)Send all requests from clients to a routing tier first that acts as a partition-aware load balancer.
                      - (Cassandra, Riak) Make clients aware of the partitioning and the assignment of partitions to nodes using Gossip protocal.
                      In many cases the problem is: how does the component making the routing decision learn about changes in the assignment of partitions to nodes?

                      Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. The routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. HBase, SolrCloud and Kafka use ZooKeeper to track partition assignment. MongoDB relies on its own config server. Cassandra and Riak take a different approach: they use a gossip protocol.
              41. How to support lock ownership and leader election
                    Apache ZooKeepr and etcd are often used for distributed locks and leader election.
              42. Why is CAP theorem unhelpful ? 
                    https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#the-unhelpful-cap-theorem 
                    CAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. Or being said in another way either Consistency or Available when Partitioned.

                    CAP only considers one consistency model (linearizability) and one kind of fault (network partitions, or nodes that are alive but disconnected from each other). It doesn't say anything about network delays, dead nodes, or other trade-offs. CAP has been historically influential, but nowadays has little practical value for designing systems.
              43. What is "total order" ? 
                    Total order allows any two elements to be compared. Peg, natural numbers are totally ordered.
                    Also "Causal order" in the system is what happened before what (causally consistent).
                    Linearizability
                      In a linearizable system, we have a total order of operations: if the system behaves as if there is only a single copy of the data, and every operation is atomic, this means that for any two operations we can always say which one happened first. This total ordering is illustrated as a timeline in Figure 9-4.
                    Causality
                      We said that two operations are concurrent if neither happened before the other (see “The “happens-before” relationship and concurrency” on page 186). Put another way, two events are ordered if they are causally related (one happened before the other), but they are incomparable if they are concurrent. This means that causality defines a partial order, not a total order: some operations are ordered with respect to each other, but some are incomparable.

                     Total order broadcast: needs two properties to hold below 
                      - Reliable delivery: If a message is delivered to one node, it is delivered to all nodes.
                      - Totally ordered delivery: Mesages are delivered to every node in the same order.
                      ZooKeeper and etcd implement total order broadcast.
              44. How to ensure linearizable writes and reads ? 
                    Another way of looking at total order broadcast is that it is a way of creating a log. Delivering a message is like appending to the log.
                    If you have total order broadcast, you can build linearizable storage on top of it.

                    Because log entries are delivered to all nodes in the same order, if therer are several concurrent writes, all nodes will agree on which one came first. Choosing the first of the conflicting writes as the winner and aborting later ones ensures that all nodes agree on whether a write was commited or aborted.

                    This procedure ensures linearizable writes, it doesn't guarantee linearizable reads.

                    To make reads linearizable:

                    You can sequence reads through the log by appending a message, reading the log, and performing the actual read when the message is delivered back to you (etcd works something like this).
                    Fetch the position of the latest log message in a linearizable way, you can query that position to be delivered to you, and then perform the read (idea behind ZooKeeper's sync()).
                    You can make your read from a replica that is synchronously updated on writes.
              45. How 2-PC works ? - PostgreSQL, MySQL, DB2, SQL Server, and Oracle
                    A transaction either succesfully commit, or abort. Atomicity prevents half-finished results.

                    On a single node, transaction commitment depends on the order in which data is writen to disk: first the data, then the commit record.

                    2PC uses a coordinartor (transaction manager). When the application is ready to commit, the coordinator begins phase 1: it sends a prepare request to each of the nodes, asking them whether are able to commit.
                      - If all participants reply "yes", the coordinator sends out a commit request in phase 2, and the commit takes place.
                      - If any of the participants replies "no", the coordinator sends an abort request to all nodes in phase 2.
                      When a participant votes "yes", it promises that it will definitely be able to commit later; and once the coordiantor decides, that decision is irrevocable. Those promises ensure the atomicity of 2PC.

                    If one of the participants or the network fails during 2PC (prepare requests fail or time out), the coordinator aborts the transaction. If any of the commit or abort request fail, the coordinator retries them indefinitely.

                    If the coordinator fails before sending the prepare requests, a participant can safely abort the transaction.

                    The only way 2PC can complete is by waiting for the coordinator to revover in case of failure. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort requests to participants.

                    2PC is also called a blocking atomic commit protocol, as 2Pc can become stuck waiting for the coordinator to recover.

                    XA (X/Open XA for eXtended Architecture) is a standard for implementing two-phase commit across heterogeneous technologies. Supported by many traditional relational databases (PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and message brokers (Kafka, ActiveMQ, HornetQ, MSQMQ, and IBM MQ).

                  The problem with locking is that database transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty writes.While those locks are held, no other transaction can modify those rows.

                  When a coordinator fails, orphaned in-doubt transactions do ocurr, and the only way out is for an administrator to manually decide whether to commit or roll back the transaction.
              46. best fault tolerance concensus algorithms ? 
                    The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft and Zab(Zookeeper).
              47. How fencing token (used in zookeeper) can help with linearizability / concensus ? 
                    First why do you need fecning token ? 
                      example, you need a lock which is time based to avoid lock owner to hold it indefinitely like GC pauses. Now if the expired client who went on GC pause still thinks it owns the lock, then it arises split brain situation (https://www.youtube.com/watch?v=1At8wV8fwp8). This means you have multiple lock holders.

                    Now how fencing token works ? 

                    Assume every time the lock server grants a lock or a lease, it also returns a fencing token, which is a number that increases every time a lock is granted (incremented by the lock service). Then we can require every time a client sends a write request to the storage service, it must include its current fencing token.

                    The storage server remembers that it has already processed a write with a higher token number, so it rejects the request with the last token.

                    If ZooKeeper is used as lock service, the transaciton ID zcid or the node version cversion can be used as a fencing token.
              48. How Zookeeper supports total order broadcast (and hence concensus) ? Pg - 370 DDIA
                    Linearizable atomic operations
                      Using an atomic compare-and-set operation, you can implement a lock: if several nodes concurrently try to perform the same operation, only one of them will suc‐ ceed. The consensus protocol guarantees that the operation will be atomic and linearizable, even if a node fails or the network is interrupted at any point. A dis‐ tributed lock is usually implemented as a lease, which has an expiry time so that it is eventually released in case the client fails (see “Process Pauses” on page 295).
                    Total ordering of operations (similar to assigning the order based on LAMPORT TIMESTAMP)
                      As discussed in “The leader and the lock” on page 301, when some resource is protected by a lock or lease, you need a fencing token to prevent clients from con‐ flicting with each other in the case of a process pause. The fencing token is some number that monotonically increases every time the lock is acquired. ZooKeeper provides this by totally ordering all operations and giving each operation a monotonically increasing transaction ID (zxid) and version number (cversion) [15].
                    Failure detection
                      Clients maintain a long-lived session on ZooKeeper servers, and the client and server periodically exchange heartbeats to check that the other node is still alive. Even if the connection is temporarily interrupted, or a ZooKeeper node fails, the session remains active. However, if the heartbeats cease for a duration that is longer than the session timeout, ZooKeeper declares the session to be dead. Any locks held by a session can be configured to be automatically released when the session times out (ZooKeeper calls these ephemeral nodes).
                    Change notifications
                      Not only can one client read locks and values that were created by another client, but it can also watch them for changes. Thus, a client can find out when another client joins the cluster (based on the value it writes to ZooKeeper), or if another client fails (because its session times out and its ephemeral nodes disappear). By subscribing to notifications, a client avoids having to frequently poll to find out about changes.
              49. Use of stream processor over microservice ? 
                  https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#stream-processors-and-services 
                    A customer is purchasing an item that is priced in one currency but paid in another currency. In order to perform the currency conversion, you need to know the current exchange rate.

                    This could be implemented in two ways:

                    Microservices approach, the code that processes the purchase would probably wuery an exchange-rate service or a database in order to obtain the current rate for a particular currency.
                    Dataflow approach, the code that processes purchases would subscribe to a stream of exchange rate updates ahead of time, and record the current rate in a local database whenever it changes. When it comes to processing the purchase, it only needs to query the local database.
                    The dataflow is not only faster, but it is also more robust to the failure of another service.
              50. How do you ensure correctness of a stream or batch processing ? 
                    https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#aiming-for-correctness 
                    - do not go for complex operations like atomic commit / 2PC XA 
                    - instead look for ways for exactly once semantics - idempotence with help of offset id or UUID 
              51. How do you ensure unique constraints on high scaled application like Google / Tiktok ? 
                    https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#enforcing-constraints 
                    Uniqueness constraints require consensus across multi-leader nodes 
                    Uniqueness in log-based messaging is much easier 

                    Multi-partition request processing - typically like Saga or Try confirm cancel 
                      There are potentially three partitions: the one containing the request ID, the one containing the payee account, and one containing the payer account.
                      The traditional approach to databases, executing this transaction would require an atomic commit across all three partitions.
                      Equivalent correctness can be achieved with partitioned logs, and without an atomic commit.

                      The request to transfer money from account A to account B is given a unique request ID by the client, and appended to a log partition based on the request ID.
                      A stream processor reads the log of requests. For each request message it emits two messages to output streams: a debit instruction to the payer account A (partitioned by A), and a credit instruction to the payee account B (partitioned by B). The original request ID is included in those emitted messages.
                      Further processors consume the streams of credit and debit instructions, deduplicate by request ID, and apply the chagnes to the account balances.

              52. Prioritize integrity over timeliness ? Why ? 
                    Consumers of a log are asynchronous by design, so a sender does not wait until its message has been proccessed by consumers. However, it is possible for a client to wait for a message to appear on an output stream.

                    Consistency conflates two different requirements:

                    Timeliness: users observe the system in an up-to-date state.
                    Integrity: Means absence of corruption. No data loss, no contradictory or false data. The derivation must be correct.
                    Violations of timeless are "eventual consistency" whereas violations of integrity are "perpetual inconsistency".
                    - Integrity assurance is not complex at all as compared to timeliness which needs 2 PC / atomic commit 
                        Stream processing systems can preserve integrity without requireing distributed transactions and an atomic commit protocol, which means they can potentially achieve comparable correctness with much better performance and operational robustness. Integrity can be achieved through a combination of mechanisms:

                        - Representing the content of the write operation as a single message, this fits well with event-sourcing
                        - Deriving all other state updates from that single message using deterministic derivation functions
                        - Passing a client-generated request ID, enabling end-to-end duplicate supression and idempotence
                        - Making messages immutable and allowing derived data to be reprocessed from time to time
                        In many businesses contexts, it is actually acceptable to temporarily violate a constraint and fix it up later apologising. The cost of the apology (money or reputation), it is often quite low.

                        Coordination-avoiding data-systems
                        - Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizability, or synchronous cross-partition coordination.
                        - Although strict uniqueness constraints require timeliness and coordination, many applications are actually fine with loose constraints than may be temporarily violated and fixed up later.
                        Dataflow systems can provide the data management services for many applications without requiring coordination, while still giving strong integrity guarantees. Coordination-avoiding data systems can achieve better performance and fault tolerance than systems that need to perform synchronous coordination.

              52. How integrity checking is done today (using merklee tree)
                  https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#trust-but-verify 
                  ACID databases has led us toward developing applications on the basis of blindly trusting technology, neglecting any sort of auditability in the process.
                  By contrast, event-based systems can provide better auditability (like with event sourcing).
                  Pg 532 DDIA
                  Cryptographic auditing and integrity checking often relies on Merkle trees [74], which are trees of hashes that can be used to efficiently prove that a record appears in some dataset (and a few other things). Outside of the hype of cryptocurrencies, certificate transparency is a security technology that relies on Merkle trees to check the val‐ idity of TLS/SSL certificates.
                  I could imagine integrity-checking and auditing algorithms, like those of certificate transparency and distributed ledgers, becoming more widely used in data systems in general. Some work will be needed to make them equally scalable as systems without cryptographic auditing, and to keep the performance penalty as low as possible. But I think this is an interesting area to watch in the future.
                  dynamo anit-entropy relies on Merklee tree
              53. How to distribute configs to thousands of servers in multiple datacenters reliably ? 
                  http://tinyurl.com/2x6ben4m 
                  - Use 3 level distribution tree Leader (zookeeper) -> Follower (zookeeper) -> cluster/datacenter observer -> server proxy binary inside host
                  - If configs are in GBs, use peer to peer network or circular topology to avoid network contention
                  - What if one of the observers fail ? 
                    - it remembers the latest transaction id and requires all the missing ids to be sent by follower.
                  - How Zookeeper ensures messages are in order ? 
                    - The commit log of ZooKeeper’s consensus protocol (ZAB) helps guarantee in-order delivery of config changes. Read this how : https://medium.com/@adityashete009/the-zab-algorithm-502781c54498

              54. Techniques to optimize and scale your application code to millions of requests ?
                  - Actor model (like Akka support in Scala and Java, Elixir for Erlang) : http://tinyurl.com/2c4a2xtn 
                      - helps with avoid race conditions because it process only one message at a time (http://tinyurl.com/26lajp6p)
                      - its asynchronous, so no blocking io 
                      - Cons
                        - large learning curve for developers
                  - Reactors based Pub Sub model in Java which supports backpresssure : https://www.baeldung.com/reactor-core 

              55. Different types of replication logs :
                Statement-based replication
                  The leader logs every statement and sends it to its followers (every INSERT, UPDATE or DELETE).

                  This type of replication has some problems:

                  Non-deterministic functions such as NOW() or RAND() will generate different values on replicas.
                  Statements that depend on existing data, like auto-increments, must be executed in the same order in each replica.
                  Statements with side effects may result on different results on each replica.
                  A solution to this is to replace any nondeterministic function with a fixed return value in the leader.

                Write-ahead log (WAL) shipping
                  The log is an append-only sequence of bytes containing all writes to the database. The leader can send it to its followers. This way of replication is used in PostgresSQL and Oracle.

                  The main disadvantage is that the log describes the data at a very low level (like which bytes were changed in which disk blocks), coupling it to the storage engine.

                  Usually is not possible to run different versions of the database in leaders and followers. This can have a big operational impact, like making it impossible to have a zero-downtime upgrade of the database.

                Logical (row-based) log replication
                    Basically a sequence of records describing writes to database tables at the granularity of a row:

                    For an inserted row, the new values of all columns.
                    For a deleted row, the information that uniquely identifies that column.
                    For an updated row, the information to uniquely identify that row and all the new values of the columns.
                    A transaction that modifies several rows, generates several of such logs, followed by a record indicating that the transaction was committed. MySQL binlog uses this approach.

                    Since logical log is decoupled from the storage engine internals, it's easier to make it backwards compatible.

                    Logical logs are also easier for external applications to parse, useful for data warehouses, custom indexes and caches (change data capture).

                Trigger-based replication
                  There are some situations were you may need to move replication up to the application layer.

                  A trigger lets you register custom application code that is automatically executed when a data change occurs. This is a good opportunity to log this change into a separate table, from which it can be read by an external process.

                  Main disadvantages is that this approach has greater overheads, is more prone to bugs but it may be useful due to its flexibility.
                   

      e. SOME POPULAR OPTIONS BY COMPANIES
        - Facebook uses master-slave per shard(partition) for its DB and 2-level cache 
          - https://research.facebook.com/publications/existential-consistency-measuring-and-understanding-consistency-at-facebook/ 
          - https://www.cs.utexas.edu/~vijay/cs380D-s18/feb6-fb.pdf 
            - Leaderless replication with leaf (top level) cache. So it gives per-object sequential consistency (sequential consistency is total order BUT per object level in FB paper which is same as monotonic read which is client will always see newer version of object but different client may see different versions) and read-after-write consistency by always connecting the user to the same cache. 
            - Leader based replication on root cache and DB.

        - Paypal increased its scalability : 1 Billion transaction with just 8 VMs using Akka (Actor model framework)
          - https://newsletter.systemdesign.one/p/actor-model

        - Facebook configerator system http://tinyurl.com/2x6ben4m 
          - 3 level distribution
          - Zookeeper guarantee for total order broadcast using transaction id. If observer fails, then it can query for previous transaction ids. 

        - Doordash async IO using Gevent improved scalability and reliability of doordash orders https://doordash.engineering/2021/01/19/scaling-efficienc-of-a-python-service-with-gevent/ 
          - from blocking I/O to async I/O
          - gradual canary roll out

          Nice article about Java threads limitation -> Go threads : https://rcoh.me/posts/why-you-can-have-a-million-go-routines-but-only-1000-java-threads/ 



























  



  
    
System design thinking framework
  https://www.youtube.com/watch?v=i7twT3x5yv8 

   

Distributed Systems estimate

    Latency video
      https://www.youtube.com/watch?v=FqR5vESuKe0
      https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation
      https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05 

    Estimates in KB

    Send 1 KB sequential from 1 Gbps network = 10 us  
    Send 1 KB sequential from memory  = 0.25 us   (100 GB/s = 0.25 *  10 ^ -6 s) 
    Send 1 KB sequential from SSD   = 1 us    (10 GB/s = 1 * 10 ^ -6 s )
    Send 1 KB sequential from disk    = 30 us     (100 MB/s = 0.03 * 10 ^ -3 s ) 

    1. DRAM
    Memory is the fastest storage tier, designed for low latency and high throughput.

      Metric	Typical Values
      Latency	100 nanoseconds (0.05–0.1 µs)
      Bandwidth	~100 GB/s per channel (DDR4/DDR5)
      IOPS	N/A (not block-oriented like storage devices)

      Notes:

      Modern DDR5 can exceed 100 GB/s in bandwidth with multiple channels.
      Latency is ultra-low, as DRAM is directly connected to the CPU.

    2. SSD
    SSDs have much lower latency and higher throughput compared to HDDs.

    SATA SSDs (connected via SATA 3.0 interface):
      Metric	Typical Values
        Latency	~100 µs (0.05–0.1 ms)
        Bandwidth	~500 MB/s (limited by SATA 3.0)
        IOPS	~100,000 IOPS (4K reads/writes)
      NVMe SSDs (connected via PCIe interface):
        Metric	Typical Values
        Latency	~10 µs (0.01–0.05 ms)
        Bandwidth	2–7 GB/s (PCIe Gen3 x4); 7–14 GB/s (PCIe Gen4 x4)
        IOPS	~1,000,000+ IOPS (4K reads/writes)

        Notes:

            NVMe SSDs are orders of magnitude faster than SATA SSDs due to direct communication with the CPU over PCIe lanes.
            Random IOPS performance is critical for workloads like databases and virtual machines.

    3. HDDs (Hard Disk Drives)
        HDDs are mechanical devices, slower than both memory and SSDs.

        Metric	Typical Values
        Latency	~10 ms (combination of seek + rotational delay)
        Bandwidth	~100 MB/s (sequential reads/writes)
        IOPS	~100 IOPS (4K reads/writes)
        Notes:

        HDD latency is dominated by mechanical delays (seek time and rotational latency).
        Throughput improves with higher RPM and denser platters but is still far slower than SSDs.



    L1/L2 cache = 1 ns
    L3 shared cache = 10 ns
    Memory call = 100 ns
    Mutex lock/unlock =	100 ns
    System call = 1 us
    Context switching between linux threads : 10 us
    Ngnix to process http request : 100 us
    SATA SSD write latency : ~200~ us
    SATA SSD read latency : ~100 us
    NVME SSD write latency : ~20 us
    NVMe SSD read latency : ~10us
    Round trip within the same datacenter 	 = 500 us 
    Redis cache latency including network : 1 ms (usually 500 us by internal redis)
    Between two availability zones : 5 ms
    Disk seek	= 10 ms
    For 7200 RPM disk, Sequential write latency (typical small data block): ~5-10 ms
    Human brain to process an object = 10-20 ms. So 25 fps is more than enough
    NTP error delay threshold = 30-50 ms
    Send packet CA (California) ->Netherlands->CA	= 150 ms 
    DNS response time ranges : 10ms to 200ms.
    Replication lag guarantee for AWS RDS : < 100 ms 
      https://www.bluematador.com/docs/troubleshooting/rds-replica-lag#:~:text=Replication%20lag%20measures%20how%20far,than%20100ms%20of%20replication%20lag. 
    TLS handshake time : 250 - 500 ms 
        https://zoompf.com/blog/2014/12/optimizing-tls-handshake/#:~:text=This%20handshake%20will%20typically%20take,is%20when%20the%20handshake%20happens. 
        So suggest to use nginx for keep alive and caching session key to avoid SSL/TLS handshake
    Multi-region failover time for AWS RDS : 60-100s  
      https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html
    Hard disk random seeks 
      https://bytebytego.com/courses/system-design-interview/s3-like-object-storage
      100 ~ 150 IOPS (per disk seeks ~ 10ms)

    AWS availability ~ 99.99% https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/appendix-a-designed-for-availability-for-select-aws-services.html
    AWS A100 GPU Lambda ~ 1$ per hour https://lambdalabs.com/service/gpu-cloud 
    AWS memory limit    ~ 32 TB RAM ~ High mem platform
    AWS DB (MySQL) limit ~ 64 TB storage
    AWS disk size ~ unlimited via EBS
    Max socket connections : 65,536 https://www.ibm.com/docs/en/zos/2.1.0?topic=domain-maximum-number-sockets 
    Max connections on the server : 
        https://josephmate.github.io/2022-04-14-max-connections
        Some think the limit 216=65,536 because that’s all the ports available in the TCP spec.
        Then theoretical limit a server can support on a single port is 2^48 which is about 1 quadrillion. This is huge and not possible.
        So basically, your # of connections depends on CPU, Mem, Network and other resources on the server.
        So to estimate max connections, use request size to memory mapping. Example, At 1M concurrent users, assuming each user connection needs 10KB of memory on the server (this is a very rough figure and very dependent on the language choice - 256 KB for java), it only needs about 10GB of memory to hold all the connections on one box.
        Best idea is to load test your services 
          AWS distributed load tests - https://aws.amazon.com/solutions/implementations/distributed-load-testing-on-aws/ 
            uses containers
            runs on fargate
            supports jmeter scripts


    Modern fiber optic cable vs ethernet (copper) cable
      - 10 Gbps vs 1 Gbps
      - https://www.cablewholesale.com/blog/index.php/2020/09/16/fiber-optic-vs-copper-ethernet-cables-the-difference/#:~:text=A%20more%20modern%20take%20on,much%20faster%20than%20copper%20cables.

    Common Systems : 
    - Zookeeper avg req latency ~ 10 ms 
        - 2-core 3 node cluster
        https://ramcloud.atlassian.net/wiki/spaces/RAM/pages/6848719/ZooKeeper+Performance
    - Kafka
        5 ms
        600 MB/s on 25 Gbps network on AWS 
        1 million messages per second
        i3en.2xlarge instance type (with 16 vCores, 64 GB RAM, 2 x 2,500 GB NVMe SSDs)
    - Azure Redis Cache : 
        53 GB, with 99.9% availability (https://github.com/Huachao/azure-content/blob/master/articles/redis-cache/cache-faq.md)
        230 us (https://redis.io/docs/management/optimization/latency/)
        Redis QPS - 50K https://redis.io/docs/management/optimization/benchmarks/
        1 million TPS - https://bytebytego.com/courses/system-design-interview/digital-wallet
        Max Size of each redis instance from AWS : 500 GB
        you can configure to be persistent ~ 1 min snapshotting
    - MySQL benchmarking
        48 cores, single node
          4 x # of cores ~ 200
            200 reads ~ 200 us
            200 writes ~ 1 ms (writes 4x slower)
          load test
            48 cores, 3 node cluster
            100K reads ~ 10-20 ms 
            100K writes ~ 100 ms (writes 4x slower)
    -  CockroachDB
         - Single row reads and writes
          https://www.cockroachlabs.com/docs/v22.2/performance.html
          across availability zones - 3 node cluster, c5d.9xlarge (16v CPUs, 72 G RAM)
            100K-200K QPS
            single-row reads in 50-100 ms and processes 
            single-row writes in 50-100 ms
        - Batch
          https://www.cockroachlabs.com/docs/v22.2/performance-benchmarking-with-tpcc-small   
            - p95 in 250-300 ms
    - Timeseries : OpenTSDB / InfluxDB
        Influx DB benchmarking from bytebytego : 
            8 cores and 32GB RAM can handle over 250K writes per second, support storage greater than 1 M series,  25 burst read QPS
            (https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system)
    - S3 
        3K-5K
        100–200 milliseconds
    - Cassandra
        i3en.2xlarge (16 vCPUs, 64 GB), 3 node cluster
          Read : 100 K ops/s
          Write : 100 K ops/s
          Read Latency : ~10 ms
          Write Latency : ~10 ms
    - Neo4j
        -> depth of 2 ~ 10 ms
        -> depth of 3 ~ 150ms
        -> depth of 5 ~ 2s vs 1 hour in MySQL
    - RocksDB (SSD) (Facebook Laser is built on the top of RocksDB)
        -> ~1-10ms
    - Flink 
        -> as low as 5-10 ms
    - Ngnix
      -> Scale upto 1 million QPS (1 million QPS, 65K SSL TPS, 70 Gbps throughput for Netflix, Hulu, Airbnb, Pinterest)
      -> ~ 100us
    - Resolution vs FPS vs bitrate
          https://www.youtube.com/watch?v=_XzGhc9mPVk&ab_channel=JamesArcher
          Resolution - no of pixels both lengthwise and breadthwise 
            - 4K Ultra HD : 3840 X 2160 px
            - 2K HD : 2800 X 1600 px
            - Full HD : 1920 X 1080 px 
            - HD : 1280 X 720 px

            For each pixel, it has color depth which is usually 8-bit. More the color depth better is the video. 

          Frames rate - no of frames per second 
            - 24 fps : cinematic like Netflix 
            - 60 fps : gaming which gives immersive experience. Avoid more fps in movie though 

          Bit rate - amount of data per second ( Resolution * color depth * frame rate )
            - Compression can help condense the data 
            - So higher the bit rate, better the quality 
            - So 4k is 35-45 Mbps(3-4 MBps) and 2K is 16 Mbps (1 MBps)



  Types of Memory
  https://www.youtube.com/watch?v=lX4CrbXMsNQ
  RAM
  ROM 
  Hard drive
  SSD
  NVMe 
    - high performance interface for SSDs thats connects to CPU via PCIe 
    - NVMe (Non-Volatile Memory Express) is a protocol that uses the PCI Express (PCIe) bus to connect SSD (solid-state drive) storage to servers or CPUs.


  Types of Cache
  https://www.youtube.com/watch?v=dGAgxozNWFE

    HW cache : 
      L1 > L2 > L3
      Virtual memory - 
        maps program address to RAM + disk address  https://www.youtube.com/watch?v=qlH4-oHnBb8
        is equal to RAM + Disk size
      TLB cache for mapping above -  virtual to physical memory address
      OS level 
        Page cache  - recently used disks blocks in memory
        File cache  - recently used file data in memory
        iNode cache
          - For context, INode is a file block pointer where filename is stored and looked up. Then using iNode, When we access a local file, we first fetch the metadata in the inode. We then read the file data by following the file block pointers to the actual disk locations.https://bytebytego.com/courses/system-design-interview/s3-like-object-storage   
          - iNode cache speed up file system operations by reducing disk access because if both iNode and file are in disk, then operation is super slow. (https://www.bluematador.com/blog/what-is-an-inode-and-what-are-they-used-for )

    Front-end :
      Browser 
        cache http responses with cache expiration policy
    Back-end : 
      CDN
        improve delivery of static content by caching it from the origin server
        https://cloud.google.com/cdn/pricing 
        remember CDN charges for egress (0.02$ per GiB - 1.024 GB), cache fill (same charge - 0.01$ per GiB) and 0.0075 per 10K req
      Load balancer cache
        to reduce the load on back-end servers and also improve response time.
      Messaging cache such as Kafka
        caches on disk using sequential IO (pretty fast)
        allows consumer to consume at their own pace
      Distributed caches
        Redis : in-memory single threaded with lot of other features(sorted set, skiplist, geohash, CRDT, atomic) and can be attached to persistance storage.
        Memcache : in-memory multi threaded for larger datasets but cannot be attached to persistence storage 
        RocksDB : LSM tree to leverage memory + SSD 
      Full text search
        Elastic search - does reverse indexing which indexes terms to doc id
      Database cache
        WAL (sequence of bytes in Postgresql, Oracle)/binlog (row level log in MySQL) : data is first written to WAL before being indexed in B tree. Retention is 30 days for MySQL
        Buffer pool : cache query results
        Materialized views : precompute query results
        replication log : tracks replication state (MySQL uses binlog as its replication log). Retention is 30 days for MySQL
          What is replication log
          https://help.serena.com/doc_center/cm/ver14_2_0_1/admin_console/adminhelp/ddevg_replication_logs/what_is_a_replication_log.htm
      Promise cache
        Cache which provides the promise that it will get the value from source DB and avoids forwarding all request to DB for that value. It helps reduce load on the DB asking for the same value. You can implement the promise cache using Redis pub sub.



=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================


Distributed best practices
  Top 7 distributed design patterns 
    https://www.youtube.com/watch?v=nH4qjmP2KEE&t=12s&ab_channel=ByteByteGo
    Ambassador (sidecar)
      - proxy to offload work like handle logging, monitoring, handling retries
      - example, kubernetes uses envoy proxy to simply communication between services. You can upgrade this feature with Istio which uses service mesh (network layer between control plane and envoy proxy in each pod )
    Circuit Breaker 
    CQRS
    Event Sourcing (Immutable events)
    Leader Election 
    Pub/Sub
    Strangler
      - helps in migration esp. monolithic to microservice. It has Strangler Facade which takes the decision whethere this API call should be made to monolithic or microservice. 
    Sharding 
      - Consistent hashing, Range based 
  
  Algorithms for System Design Interviews
    Bloom Filter: Check if a requested item is in the cache before performing disk operations.
    Geohash: Used to build location-based services.
    HyperLogLog: Counting unique queries performed by users in a search.
    Consistent Hashing: Used for efficient data distribution between the cluster’s nodes.
    Merkle Tree: Used to detect inconsistencies between data replicas across multiple nodes.
    Raft Algorithm: Used to achieve consensus on log replication.
    Lossy Count: Network traffic analysis, web analytics, and identifying heavy hitters.
    QuadTree: Used to build location-based services.
    Operational Transformation: Used to support collaborative editing systems.
    Leaky Bucket: Used for rate limiting
    Rsync: Synchronizing files and directories between two different systems.
    Ray Casting: Used for geospatial analysis, collision detection in video games, and computer graphics.

  Smallest short length required ( say for tiny url )
    The hashValue consists of characters from [0-9, a-z, A-Z], containing 10 + 26 + 26 = 62 possible characters. To figure out the length of hashValue, find the smallest n such that 62^n ≥ 365 billion. 

  Estimation of length based on no of bits 
    For 64 bit unique id generator, 
      -> ~21 decimal long (base 10 since 0-7 (9 is 1001 but 10 us 1111 > 9, So take 7) can accomodate 3 bits)
      -> 16 hex long (base 16 since 0-15 (15 is 1111) can accomodate 4 bits)
      -> 8 chars long (base 256 since 0-255 can accomodate 8 bits)

    So SHA-1 which is 160 bits long can be 40 hex chars long and SHA-256 which is 256 bits long can be 64 hex chars long

  Choices with protocol
    - TCP vs UDP
        https://www.spiceworks.com/tech/networking/articles/tcp-vs-udp/
        1. TCP is connection-oriented so you can get an ack (like success 200 OK in HTTP over TCP) while UDP is connectionless
        So UDP protocol is NOT suitable for sending electronic mail, viewing a web page, or downloading a file. However, it is preferred mainly for real-time applications like broadcasting or multitasking network traffic.

        Then why TCP is a still good choice on sender of live streaming protocols like HLS, DASH ? 
          - because UDP is lossy and you should not loose data on sender's side since you need it for transcoding (converting to different resolutions, color depth, bitrate). 

        Then what are the scenarios where UDP is recommended ? 
          Video calling (app like Zoom): UDP can support video 30 frames per second refresh rates (frame is single image in the video - https://darvideo.tv/dictionary/frame/ and min is 24 fps and max is 60 fps even better). The data transmission is so fast that a few dropped packets do not affect the user experience. 
          Online gaming: TCP’s many checklists and balances will significantly impact gaming experiences. Without perfect network conditions, frames will frequently freeze, and connections will restart if using TCP. That is why UDP is recommended. 

  Remove inconsistency caused by writes through multiple servers - versioning using vector clocks (Different from version vectors)
    https://bytebytego.com/courses/system-design-interview/design-a-key-value-store
    This is very applicable for leaderless replication like Cassandra
    Vector clocks
      https://www.waitingforcode.com/big-data-algorithms/conflict-resolution-distributed-applications-vector-clocks/read 

        Local vector	                            Sent vector	                                New vector
        [(N1, 1), (N2, 1), (N3, 3)]	      [(N1, 1), (N2, 0), (N3, 3)]	                    [(N1, 1), (N2, 1), (N3, 3)]
        [(N1, 1), (N2, 0), (N3, 3)]	      [(N1, 1), (N2, 1), (N3, 3)]	                    [(N1, 1), (N2, 1), (N3, 3)]
        As you can see, during the merge of vectors, the largest values are taken. The whole process is illustrated more clearly in the last section vector clock example.

        Until now we saw that vector clocks can easily detect conflicts. But what happens after detecting one ? The first solution delegates the conflict resolution to the client. The client application receives all conflicted values and it decides how to deal with them (merge, discard, etc.). Another approach is server-based. One of its implementations can be last-write-wins strategy where the most recent value is returned to the client. It's only a short introduction to available solutions that will probably be detailed more in further posts.

        Cassandra use LWW 
        Dynamo uses Read-Repair (resolve conflicts during read) and anit-entropy 

        Pg 191, DDIA 
        Even though vector clocks can resolve conflicts, there are two notable downsides. First, vector clocks add complexity to the client because it needs to implement conflict resolution logic.
        Second, the [server: version] pairs in the vector clock could grow rapidly. To fix this problem, we set a threshold for the length, and if it exceeds the limit, the oldest pairs are removed. This can lead to inefficiencies in reconciliation because the descendant relationship cannot be determined accurately. However, based on Dynamo paper [4], Amazon has not yet encountered this problem in production; therefore, it is probably an acceptable solution for most companies.

  Failure detection algo 
    - gossip prototocol to detect availability failures in distributed systems (Cassandra)
    - Raft (description below) using used for master-replica model does both detect failures and resolve conflicts
    - Zookeeper
      

  Master election concensus algo
      Ensure all replicas are always in-sync and master->slave is elected correctly when master goes down. We could use consensus algorithms such as Paxos [21] and Raft [22], or use consensus-based distributed databases such as YugabyteDB [23] or CockroachDB [24].
          https://bytebytego.com/courses/system-design-interview/stock-exchange 
          The leader sends heartbeat messages (AppendEnties with no content as shown in Figure 21) to its followers. If a follower has not received heartbeat messages for a period of time, it triggers an election timeout that initiates a new election. The first follower that reaches election timeout becomes a candidate, and it asks the rest of the followers to vote (RequestVote). If the first follower receives a majority of votes, it becomes the new leader.
      Nice visual - https://raft.github.io/
      Raft is one of the best concensus algo since it also helps in ensuring strong consistency for distributed transaction in cockroachDB https://github.com/cockroachdb/cockroach/blob/master/docs/design.md
      Even when new node is lagging behind or came live, leader in raft sends replication log along with the heartbeat to keep it consistent http://thesecretlivesofdata.com/raft/#replication 
        What is replication log
          https://help.serena.com/doc_center/cm/ver14_2_0_1/admin_console/adminhelp/ddevg_replication_logs/what_is_a_replication_log.htm
      Raft is used in Kafka, etcd, CockroachDB
      Other alternative is Zookeepeer which uses ZAB(zookeeper atomic broadcast) which works similar to Raft
        - Read this how : https://medium.com/@adityashete009/the-zab-algorithm-502781c54498


  Failure handling algo
  - for temporary failures
      1. Sloppy quorum and hinted handoff (for Leaderless replication)
          https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sloppy-quorums-and-hinted-handoff
          Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads.

          How do you keep strong consistency if there is network failure ? 

            It's likely that the client won't be able to connect to some database nodes during a network interruption.

              1. Is it better to return errors to all requests for which we cannot reach quorum of w or r nodes?
              2. Or should we accept writes anyway, and write them to some nodes that are reachable but aren't among the n nodes on which the value usually lives?

            The latter is known as sloppy quorum: writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n "home" nodes for a value.

            Once the network interruption is fixed, any writes are sent to the appropriate "home" nodes (hinted handoff).

            Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n


        
  - detecting inconsistency          
      1. Merklee tree - used by Cassandra, DynamoDB during anti-entropy, Blockchain (can detect and handle failures by syncing remaining replicas)
          What if a replica is permanently unavailable? To handle such a situation, we implement an anti-entropy protocol to keep replicas in sync. Anti-entropy involves comparing each piece of data on replicas and updating each replica to the newest version. A Merkle tree is used for inconsistency detection and minimizing the amount of data transferred.
          Quoted from Wikipedia [7]: “A hash tree or Merkle tree is a tree in which every non-leaf node is labeled with the hash of the labels or values (in case of leaves) of its child nodes. Hash trees allow efficient and secure verification of the contents of large data structures”.
                Step 1: Divide key space into buckets (4 in our example) as shown in Figure 13. A bucket is used as the root level node to maintain a limited depth of the tree.
                Step 2: Once the buckets are created, hash each key in a bucket using a uniform hashing method (Figure 14).
                Step 3: Create a single hash node per bucket (Figure 15).
                Step 4: Build the tree upwards till root by calculating hashes of children (Figure 16).
          https://bytebytego.com/courses/system-design-interview/design-a-key-value-store

  Merging concurrently written values
    https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sloppy-quorums-and-hinted-handoff
    No data is silently dropped. It requires clients do some extra work, they have to clean up afterward by merging the concurrently written values. Riak calls these concurrent values siblings.

    Merging sibling values is the same problem as conflict resolution in multi-leader replication. A simple approach is to just pick one of the values on a version number or timestamp (last write wins). You may need to do something more intelligent in application code to avoid losing data.

    If you want to allow people to remove things, union of siblings may not yield the right result. An item cannot simply be deleted from the database when it is removed, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings (tombstone).

    Merging siblings in application code is complex and error-prone, there are efforts to design data structures that can perform this merging automatically (CRDTs).

  Directed acyclic graph (DAG) model
        To support different video processing pipelines and maintain high parallelism, it is important to add some level of abstraction and let client programmers define what tasks to execute. For example, Facebook’s streaming video engine uses a directed acyclic graph (DAG) programming model, which defines tasks in stages so they can be executed sequentially or parallelly [8]. In our design, we adopt a similar DAG model to achieve flexibility and parallelism. Figure 8 represents a DAG for video transcoding.


  Symmetric vs Asymmetric encryption
    https://blog.mailfence.com/symmetric-vs-asymmetric-encryption/ 
    RSA, DSA, Diffie-Hellman - key exchange is asymmetric
    AES, DES - is symmetric
    Hash - SHA-1 <  Murmur hash <  SHA-256 in terms of collision
    SHA-1 is hash function and not encryption one
        https://stackoverflow.com/questions/31532498/is-sha-1-length-always-40-symbols
        SHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 chars long.


    SHA-256 is more secure https://www.keycdn.com/support/sha1-vs-sha256
          A sha256 is 256 bits long -- as its name indicates.
          Since sha256 returns a hexadecimal representation, 4 bits are enough to encode each character (instead of 8, like for ASCII), so 256 bits would represent 64 hex characters, therefore you need a varchar(64), or even a char(64), as the length is always the same, not varying at all. (https://stackoverflow.com/questions/2240973/how-long-is-the-sha256-hash)
 
    BCrypt / Scrypt (slow hashes than SHA-256 but more secure)

  Eliminate disk access
    Disk accesses can be eliminated using mmap. `mmap(2)` provides a mechanism for high-performance sharing of memory between processes. 
    Modern exchanges take advantage of this to eliminate as much disk access from the critical path as possible. `mmap(2)` is used in the server to implement a message bus over which the components on the critical path communicate. The communication pathway has no network or disk access, and sending a message on this mmap message bus takes sub-microsecond. By leveraging mmap to build an event store, coupled with the event sourcing design paradigm which we will discuss next, modern exchanges can build low-latency microservices inside a server.

    It basically just tells the OS to put your requested file in main memory shared by multiple process - https://db.cs.cmu.edu/mmap-cidr2022/#:~:text=Memory%2Dmapped%20(MMAP)%20file,file%20resided%20entirely%20in%20memory.  

  Change Data Capture
  CDC is a mechanism that reads data changes from the database and applies the changes to another data system. 
    -  One common solution is Debezium [9]. It uses a source connector to read changes from a database and applies them to cache solutions such as Redis [10]. Also supported in SQL(Postgresql, Oracle, MySQL) and noSQL DB like Cassandra.
    - Another is Kafka Connect (pg 457)
    - Walmart's CDC solution from Cassandra (https://medium.com/walmartglobaltech/walmarts-cassandra-cdc-solution-6fc650031a3 )
  Pg 455 - DDIA 
    Rather than database triggers, Parsing replication log can be a more robust approach. Example Debezium uses MySQL binlog (same as replication log for MYSQL). Retention is 30 days for MySQL.
    Along with this you also need "Initial snapshot" since replication log cannot carry entire DB changes.(30 day retention)

  In-sync replicas
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue
    We mentioned that messages are persisted in multiple partitions to avoid single node failure, and each partition has multiple replicas. Messages are only written to the leader, and followers synchronize data from the leader. One problem we need to solve is keeping them in sync.
    In-sync replicas (ISR) refer to replicas that are “in-sync” with the leader. The definition of “in-sync” depends on the topic configuration. For example, if the value of replica.lag.max.messages is 4, it means that as long as the follower is behind the leader by no more than 3 messages, it will not be removed from ISR [10]. The leader is an ISR by default.

  Data mirroring with replicas
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue 
    If all the replicas of a partition crash, the data for that partition is lost forever. When choosing the number of replicas and replica locations, there’s a trade-off between data safety, resource cost, and latency. It is safer to distribute replicas across data centers, but this will incur much more latency and cost, to synchronize data between replicas. As a workaround, data mirroring can help to copy data across data centers, but this is out of scope. The reference material [14] covers this topic.
    Mirroring refers to keeping a backup database server for a master database server. It is not meant for distributed systems like (master(write)->replica(read)) but it morese serves as a DR result. So this is reason why there is always mirroring on replicas
    https://www.tutorialspoint.com/difference-between-mirroring-and-replication 

  HTTP Encrpytion, Authentication, Authorization and Security 
        Asymmetric encryption could be used here for payment gateway
          https://www.youtube.com/watch?v=AQDCe585Lnc
          also used in SSH, Bitcoin, Emails using PGP(Pretty Good Privacy) protocol

        How HTTPS works ? How it uses SSL and TLS
          https://www.youtube.com/watch?v=hExRDVZHhig
          HTTPS uses public key encryption to secure data using SSL (Secure socket layer) protocol. Basically server gives SSL certicate to client and acknowledgement is established between the two. Then info can be transferred securely using encryption.
          TLS is latest and successor of SSL
          Today most websites supports https because of google standards

          ByteByteGo : https://www.youtube.com/watch?v=j9QmMEWmcfo
            Moderm HTTPS uses TLS - https://excalidraw.com/#room=c8d3386f5073e7a28986,MAgpFTKB8sh-C5IBZx4tPg 
            1. first establishes TCP handshake at transport (helps in getting the sequence number)
            2. then client sends hello and gets the certificate from server which has public key of server (Asymmetric) 
            3. then client encryptes his/her session key with server's public key and then on server, it gets clients sesssion key by decrypting it with server's private key. This is Asymmetric encryption
            4. Now both has session key and uses session key as cipher to encrypt and decrypt at both sides. This is Symmetric encryption.

            SSL uses public key encryption (Asymmetric encryption) only. However, this website claims it supports both Asymmetric and symmetric - https://www.trentonsystems.com/blog/symmetric-vs-asymmetric-encryption
          

        TLS handshake latency 
          TLS handshake time : 250 - 500 ms https://zoompf.com/blog/2014/12/optimizing-tls-handshake/#:~:text=This%20handshake%20will%20typically%20take,is%20when%20the%20handshake%20happens.  
          Suggestion is to use nginx which uses keep-alive and stores encrypted session key 

        How SSO work ? 
            https://www.youtube.com/watch?v=O1cRJWYF-g4
            
            Two ways 
              using SAML
                - uses XML 
                - uses public key encryption 
              Open id connect
                - built on top of OAuth 2.0 which uses json web token (JWT) as access token
        
        What is SAML ? 
          https://www.youtube.com/watch?v=7RrFhC-e8QA&pp=ygUEU0FNTA%3D%3D&ab_channel=JumpCloud

        Authentication and Authorization
          https://blog.bytebytego.com/p/password-session-cookie-token-jwt 
          modern websites uses session id
          
          Oauth 2.0 work ? 
            https://www.youtube.com/watch?v=CPbvxxslDTU
            JWT is data format for user information in the OpenID Connect standard, which is the standard identity layer on top of the OAuth 2.0 protocol. Nginx plus supports this - https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-jwt-authentication

            Related to above 
            How payment gateway work ?
              https://www.youtube.com/watch?v=GUurzvS3DlY

        Session id based authenitication vs JWT
          https://stytch.com/blog/jwts-vs-sessions-which-is-right-for-you/ 
          - session id needs db look up after authentication although nginx can cache it
          - jwt does not need db look up since all necessary info is contained in the client request. Invalidating JWT token is complex so token is valid even after log out.
      
      How to send passwords from client to server
        https://github.dev/ppatel03/System-design/blob/main/back-of-envelope 
        before you send the password from browser, you should know the server is already https which uses TLS encryption. So no need to encrypt on client side code.  
        - this is why you observe that client passwords are sent in plaintext from developer tools at application layer (browser) which is layer 7
        - Encryption takes place in between application and transport which could be presentation layer or session layer in OSI or Transport layer in TCP/IP
            (https://community.fs.com/blog/tcpip-vs-osi-whats-the-difference-between-the-two-models.html)
            (https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place)

      Difference between Oauth 2.0 vs SAML and OpenID Connect
        https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/
          OAuth 2.0 is a framework that controls authorization to a protected resource such as an application or a set of files, while OpenID Connect and SAML are both industry standards for federated authentication.

          OpenID Connect is built on the OAuth 2.0 protocol and uses an additional JSON Web Token (JWT), called an ID token, to standardize areas that OAuth 2.0 leaves up to choice, such as scopes and endpoint discovery. It is specifically focused on user authentication and is widely used to enable user logins on consumer websites and mobile apps.
          SAML is independent of OAuth, relying on an exchange of messages to authenticate in XML SAML format, as opposed to JWT. It is more commonly used to help enterprise users sign in to multiple applications using a single login.

          Okta uses SAML 

      What Is IAM (Identity & Access Management)?
        GREAT VIDEO HOW AWS HANDLES IAM (policy, roles, users and groups) - https://www.youtube.com/watch?v=VclOgMtBXN4
        - policy are defined for each resources as an action on resource (example, allaccess, read, write)
        - users are for people while roles are for aws resources 
        - So For example, lambda needs access to Dynamo DB. So lambda needs a role to which policy is attached
        - For user, policy can be attached directly to user 
        - Sometimes, user wants to give temp access to other external user. So in this case, it assumes the role for other external user and grants temp access.
        
        https://spectralops.io/blog/top-11-identity-access-management-tools/
          Identity and Access Management (IAM) tools are designed to manage identities (users) and access (authentication and authorization). The goal of IAM tools is to streamline the management of user accounts and privileges from all aspects.
          In most cases, an IAM solution will let you define a policy. This policy will, in turn, determine the roles of users. Each role defined will have permissions set. These permissions allow access to specific resources. 
          Example, Okta, Auth0 and other cloud service providers (AWS IAM, Microsoft Azure)
            Small disadvantage with Okta
              Currently, Okta falls short on passwordless solutions, prompting users to change their passwords often. In addition, users also report some technical issues with logins.
            While Auth0 could be integrated with Google / Twitter / FB sign-in (which uses open id connect) - https://www.youtube.com/watch?v=yufqeJLP1rI&ab_channel=Fireship 

      Not important : 
      So what tools usually company use for authorization internally ? 
        https://www.reddit.com/r/sysadmin/comments/k09crr/what_tools_does_your_company_use_for/ 
          SAML and LDAPS would cover most use cases which are standardized authentication protocols
          SAML can be offered via Okta 
          SAML vs LDAP
            https://www.youtube.com/watch?v=_NCcLJin30E&ab_channel=JumpCloud
            Both are used to access IT resources
            LDAP -                                                                            SAML 
            1990s (still used today)                                2000s has SSO solutions which federates identities through web applications
            mostly used in back-end of user auth such user info     extends user creds to the cloud  
            source of truth for IDP                                 its not SOT but protocol for exchanging authenication between directories and web apps. 
            used in Linux applications like OpenVPN, docker         used by IAM providers like Okta 

  Network/s / Networking
    OSI model
      https://www.youtube.com/watch?v=0y6FtKsg6J4
      better to recap in https://excalidraw.com/#room=c8d3386f5073e7a28986,MAgpFTKB8sh-C5IBZx4tPg  

    TCP/IP model
      https://www.techtarget.com/searchnetworking/definition/TCP-IP 
      Modern day uses TCP/IP over OSI model

    HTTP over TCP/IP 
      https://www.goanywhere.com/blog/http-vs-tcp-whats-the-difference
      In the case of HTTP which applies at layer 7, before a client and server can exchange an HTTP request/response, they must establish a TCP connection first to exchange sequence numbers(segment sequencing) and port number and acknowlegement mechanism. Therefore, HTTP relies on the TCP standard in order to successfully do its job.

      HTTP 1.0, 1.1 and 2.0
        https://www.youtube.com/watch?v=a-sBfyiXysI
        1.0 -> 
          tcp handshake all the time. Very low performat
        1.1 -> 
          keep alive, persistent conenection avoids tcp handshake. But suffers from head of line blocking - if a request is blocked for any reason them all subsequent requests are blocked. So applications like browsers send muliple requests by opening many TCP connections in parallel.
        2.0 -> 
          brought concept of multiple independent streams on same tcp connection which resolves head of line blocking at application layer. But the issue still in transport layer in TCP.
          it brought push capabiliy (SSE) to send update to the client from  the server
        3.0 -> Quic which uses UDP
          recent and used Quic protocol which uses UDP instead of TCP
          multiple streams share the same Quic connection. Due to use of UDP, it resolves head of line blocking even at transport layer
          Application : 
            designed for mobile heavy internet usage where user keeps connecting between different networks.
            How ? uses the concept of connection id between client and server which always remains same even if different network is selected.
        https://www.linkedin.com/posts/gkcs_http3-systemdesign-networkprotocols-activity-7041095963335139329-uV44/?utm_source=share&utm_medium=member_desktop 

    Video streaming protocol
      https://linuxhit.com/rtmp-vs-hls-vs-dash-streaming-protocols/#0-rtmp-vs-hls-vs-dash-streaming-protocols 
      RTMP
        RTMP stands for Real Time Messaging Protocol. Sounds more generic than streaming doesn’t it? RTMP is a creation of Macromedia and through Adobe’s acquisition of Macromedia it now belongs to Adobe. Remember all those flash videos and the flash player?
        RTMP is a TCP based protocol designed for low latency.
        Low latency vs HLS and DASH.
        Used heavily in production or broadcasting of video streams from streamer's device
      HLS
        HLS is Apple’s streaming protocol. HLS stands for HTTP Live Streaming. Leveraging HTTP has many benefits. Internet infrastructure like firewalls and content delivery networks already handle HTTP.
        - key benefit of HLS is adaptive streaming. Adaptive streaming enables changing the quality of the video mid-stream. For example, if your internet connection starts slowing down, the bit rate can adapt to ensure the stream stays viewable. Despite the degradation in bandwidth.
        - HLS only supports the H264 codec for video and AAC for audio.
      DASH
        DASH stands for Dynamic Adaptive Streaming is an open standard and similar to HLS. It uses HTTP, small chunks and also supports adaptive streaming. A key difference between HLS and DASH is that DASH is codec agnostic. Hence you can use any codec you want.

      Why HLS/DASH ? 
        Both of these format consist of the manifest file  and the series of video chunks where each chunk contains video of few seconds. This manifest file is like a directory to tell the client video player what all output formats are, where to load the video chunks over HTTP. Both manisfest file and video chunks are usually cached in CDN for faster streaming.

    DNS
      https://www.youtube.com/watch?v=27r4Bzuj5NQ
      queries from browser go to DNS resolver (ISP, Cloudfare, Google) which fetches authoritative name server in hierarchial fashion.
      DNS lookups are cached at client machine, resolver

    How Wifi works
      https://computer.howstuffworks.com/wireless-network.htm
      A wireless network uses radio waves, just like cell phones, televisions and radios do.
        1. A computer's wireless adapter translates data into a radio signal and transmits it using an antenna.
        2. A wireless router receives the signal and decodes it. The router sends the information to the internet using a physical, wired ethernet connection.
      The radios used for WiFi communication are very similar to the radios used for walkie-talkies, cell phones and other devices. They can transmit and receive radio waves, and they can convert 1s and 0s into radio waves and convert the radio waves back into 1s and 0s.
      2.4 GHz connections are now considered somewhat obsolete because they carry lower data speeds than 5 GHz.
      Remember 2.4 GHz vs 5 GHz this is different from 4G vs modern 5G (upto 1Gbps) which is based on bandwidth to tranfer data

    Anycast 
      https://www.youtube.com/watch?v=vOYjcOs1dUU
      multiple servers with same IP address so that nearest one can serve. Example, CDN or multiple load balancers.
      It also helps in security to avoid DDOS attacks

  API design for websocket connection
    https://bytebytego.com/courses/system-design-interview/nearby-friends
    - do not add paths else it will look like REST which is HTTP
    format
      1. Initiated by Client : Request and Response
      2. Initiated by Server : Data sent 
    Websocket tunneling is possible with HaProxy LB : http://tinyurl.com/yurdrom2 

  REST API naming conventions 
    https://restfulapi.net/resource-naming/ 
    - use nouns
    - avoid verbs 
    - do not use CRUD in naming since it would be obvious with HTTP methods - GET, POST , PUT , DELETE
    - difference between POST and PUT
        - Purpose : 
          - POST: Used to create a new resource. The server decides the URI (Uniform Resource Identifier) of the created resource.
              Example: Submitting a form to create a new user.
              URI: POST /users
          - PUT: Used to update an existing resource or create a resource at a specific URI if it does not already exist.
              Example: Updating the profile of a specific user or creating it if absent.
              URI: PUT /users/{id}
        - Idempotence
          - POST: Not idempotent. Multiple POST requests with the same data can result in multiple resources being created.
              Example: Sending the same POST twice could create two different users.
          - PUT: Idempotent. Multiple PUT requests with the same data result in the same resource state.
              Example: Sending the same PUT request twice updates the resource to the same state.
        - URL Structure
          - POST: The server assigns the URI for the created resource.
            Example: POST /articles -> Creates a new article with a server-defined ID.
          - PUT: The client specifies the exact URI for the resource.
            Example: PUT /articles/123 -> Updates the article with ID 123 or creates it if it doesn’t exist.
    - use plural for accessing the collection
        /customers - gets all customers
    - use {} for accessing by id 
        /customers/{id} - gets customer by id 
    - use hyphens(-) instead of underscores
        /managed-devices

  API testing 
    https://youtu.be/qquIJ1Ivusg?t=224 
  
    1. Smoke Testing (check if nothing breaks - just checks if API returns basic response with status code)
    2. Functional Testing
    3. Integration Testing
    4. Regression Testing
    5. Load Testing (increasing load). Tool - Jmeter
    6. Stress Testing ( spikes). Tool - Jmeter
    7. Security Testing - mostly auth related
    8. UI Testing . Tool - Jeste2e
    9. Fuzz Testing - test edge cases and other attacks - SQL injection, XSS, buffer overflow
      - XSS (Cross-Site Scripting) is a type of security vulnerability in web applications where an attacker injects malicious scripts (usually JavaScript) into content that is then executed in the user's browser.
        - Stored XSS (Persistent XSS):
            The malicious script is permanently stored on the target server (e.g., in a database, message board, or comment field).
            Example:
            An attacker posts a malicious JavaScript snippet in a comment section.
            When a user views the comment, the script is executed in their browser.
        - Reflected XSS (Non-Persistent XSS):
            The malicious script is embedded in a URL or request sent to the server, and the server reflects it back in its response.
            Example:
            A user clicks on a crafted link containing malicious code.
            The server includes the code in its response, which the browser executes.
        - DOM-Based XSS:
            The malicious script is executed directly in the browser by manipulating the client-side Document Object Model (DOM).
            Example:
            1. The attacker sends malicious input to a user, often through a crafted URL.
            2. The browser loads the web page, and the JavaScript code on the page processes the malicious input (directly from the URL or another source) without proper sanitization.
            3. The script modifies the DOM, and the malicious payload is executed in the user's browser.

  Physical cores, Virtual cores and Logical cores
    https://www.youtube.com/watch?v=O2g6381An_k 
    Example, i7 has 4 physical cores and Each core can have ability to run 2 threads simultaneously. So 4 virtual cores. So 8 logical cores

  Processes vs Thread
    https://www.youtube.com/watch?v=4rLW7zg21gI 
      Process is responsible for executing set of instructions from an application
      Process will contain multiple threads
    If interested, how computer boot process work : https://www.youtube.com/watch?v=XpFsMB6FoOs&ab_channel=ByteByteGo 


  Logging (errors), Metrics Monitoring (system level, business) and automated Deployment
    Logging (errors)
      - datadog / envoy proxy -> control plance -> datadog or prometheus
      - logging errors to trigger alerts, sev
    Metrics Monitoring (system level, business)
      - datadog, 
    Alerting
      - datadog
    Continuous integration
        develop (-> PR -> build -> test optional as best practices)  merge (commit) -> build -> test -> deploy
      https://blog.inedo.com/continuous-integration-performance-testing-best-practices

  Failure/Error handling to make fault tolerant and recover it making it resilient and durable
    - Common techniques are retry the processing for queue consumer or offload to another server 
    - try maintaining the commit log (checkpointing)
    - explore event sourcing technique since events are immutable and you can generate the desired state (CQRS)
    - Avoid frequent retries
        Exponential backoff [14] might be a good retry strategy (https://bytebytego.com/courses/system-design-interview/distributed-email-service)
        Use Circuit breaker
    - Using Kafka Queue which can persist messages even when downstream services are down
    - retry logic by storing last online state in S3 / KV store (checkpointing)
    - save computational work in snapshots storage for low latency (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation)
     


  Event sourcing
      https://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/
      https://bytebytego.com/courses/system-design-interview/digital-wallet
    One design philosophy that systematically answers those questions is event sourcing, which is a technique developed in Domain-Driven Design (DDD) [9].
      Advantages :- https://www.youtube.com/watch?v=i2eVTk2Fb40&t=137s
          - complete rebuild to get state of system
          - temporal queries (what happened at day x)
          - event replay for debugging
          Example, nice examples of event sourcing design 
          - https://www.youtube.com/watch?v=lg6aF5PP4Tc
              Event sourcing is an alternative means of storing data as log of events rather than table that is updated,read and deleted. In event sourcing, you create a table and append events to it as they occur building up an event log. So reading data has an additional step because you need to pull up all the events and reduce it to derive into table format. Doing this process in real-time for large number of events is complicated and thus needs pre-processing of table format data from event log. This technique is an example of CQRS where you write in event log and store it immediately in read-only table or materialized views for reading.  
              Confluence using Kafka as event log for writes and generates materialized views for read 

  Verify integrity through blockchain
    https://www.youtube.com/watch?v=SSo_EIwHSd4 

  Using LSMTree + SSTable are efficienct for heavy writes and heavy reads on noSQL databases. B tree is not
      Great video : https://www.youtube.com/watch?v=I6jB0nM9SKU 
      Good read : https://rahulpradeep.medium.com/sstables-and-lsm-trees-5ba6c5529325
      This technique is used by Cassandra, RocksDB, Google BigTable, Apache HBase
      MongoDB can be tuned to use LSM treee
    
  Algo if element is the member of the set
    Use Bloom filter
      https://www.youtube.com/watch?v=V3pzxngeLqw
      A bloom filter is a space-efficient probabilistic technique to test if an element is a member of a set. Refer to the reference material [2] for more details.
      Applications
        key value store to check in SSTable
        crawlers to check if url in malicious 
        CDN if page is in cache
  
  Clock synchronization amongst different servers in distributed systems
    https://www.youtube.com/watch?v=f1hlCZB0GDA 
    - Use network time protocol (NTP)
    - How does NTP gets the time from GPS 
        https://www.galsys.co.uk/time-reference/development-of-atomic-clocks-and-timescales/atomic-clock-ntp-servers.html 
        Atomic clocks are also the basis of GPS (Global Positioning System) as each satellite contains an atomic clock as accurate time is integral for positioning (a position anywhere is made up of a direction, a velocity and time).
        GPS signals can also be used to capture a time signal. This is now the most common way computer networks retain accurate time which is also essential in many communications and applications.Most computer networks use a NTP server (Network Time Protocol) to synchonise their devices to an atomic time signal received via the GPS network.
    - NTP has some challenges but its still the reliable way to synchronize clocks over the network with the error delay of 35 ms - Pg 288 DDIA. So applicable for scheduling systems with 1s - 1 min granularity
    However, some trading firms enforce accuracy of no more than 100 us microseconds and worst case 100 ms for network congestion. Such accuracy can be achieved with GPS (the precision time protocol PTP) with careful deployment and monitoring but requires effort and expertise.
    For systems which highly relies on time like "Scheduler", any server(node) which drifts too far at NTP sync (say 10s or even 1 min) should be removed.
    - LWW (last write wins) is common issue with multi-leader replication and leaderless databases like Cassandra - Pg 292 DDIA
    Google achieving accuracy with time synchronization
    - Google Spanner (the next CockroachDB) provides time intervals using Google's Truetime API(deploys GPS receiver or atomic clock in each datacenter) to support visibility into accuracy of its time. - Pg 294 DDIA
    - Using above, Google Spanner implements snapshot isolation across datacenters based on timestamp with intervals reported above
    - To improve accuracy, Google deploys GPS or atomic clock in each datacenter with guarantee SLA of 10 ms

  Dynamic rendering to allow search engines to parse content
    https://developers.google.com/search/docs/crawling-indexing/javascript/dynamic-rendering 
    Dynamic rendering is a workaround and not a recommended solution, because it creates additional complexities and resource requirements.

  Scale through Kafka
    Kafka is known for high throughput, low latency streaming, message persistence, high availability, fault tolerant system
    https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system
    There are a couple of ways that we can leverage Kafka’s built-in partition mechanism to scale our system.
      Configure the number of partitions based on throughput requirements.
      Partition metrics data by metric names, so consumers can aggregate data by metrics names.
      Further partition metrics data with tags/labels.
      Categorize and prioritize metrics so that important metrics can be processed first - possible with kafka

  Scale map reduce operations 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 
    If you are interested in the details, please refer to reference material [20]. Aggregation service is horizontally scalable by adding or removing nodes.
    Question is how you increase the throughput ? 
        1. assign each thread in the server
        2. deploy aggregation service nodes on resource providers like Apache Hadoop YARN [21]
      Option 1 is easier to implement and doesn’t depend on resource providers. In reality, however, option 2 is more widely used because we can scale the system by adding more computing resources.
  Using star schema as pre-filtered form - Pg 94 DDIA 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    To support data filtering like “show me the aggregated click count for ad001 within the USA only”, we can pre-define filtering criteria and aggregate based on them. This technique is called the star schema [11], which is widely used in data warehouses. The filtering fields are called dimensions. 
    A limitation with this approach is that it creates many more buckets and records, especially when we have a lot of filtering criteria.


  Combine batch processing and streaming with one service 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    For a system that contains two processing paths (batch and streaming) simultaneously, this architecture is called lambda [14]. A disadvantage of lambda architecture is that you have two processing paths, meaning there are two codebases to maintain. Kappa architecture [15], which combines the batch and streaming in one processing path, solves the problem. The key idea is to handle both real-time data processing and continuous data reprocessing using a single stream processing engine. 
    Why Kappa - no need to maintain two codebases for stream processing and batch processing. Application is Apache Flink
      https://www.oreilly.com/radar/questioning-the-lambda-architecture/

  Process delayed events 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    One way to mitigate this problem is to use “watermark” (the extended rectangles in Figure 14), which is regarded as an extension of an aggregation window. This improves the accuracy of the aggregation result. By extending an extra 15-second (adjustable) aggregation window, window 1 is able to include event 2, and window 3 is able to include event 5.
    Remember, this is timestamp based - so it avoids overlap between two windows. Example, window 1 is to accept data which has timestamp from 0 to 10s and window 2 is for 10s to 20s. But packets in window 1 coould be delayed, so wait for extra 2 seconds.

  Aggregation window
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    According to the “Designing data-intensive applications” book by Martin Kleppmann [16], there are four types of window functions: tumbling (also called fixed) window, hopping window, sliding window, and session window. We will discuss the tumbling window and sliding window as they are most relevant to our system.
      - In the tumbling window (highlighted in Figure 15), time is partitioned into same-length, non-overlapping chunks. The tumbling window is a good fit for aggregating ad click events every minute (use case 1).
      - In the sliding window (highlighted in Figure 16), events are grouped within a window that slides across the data stream, according to a specified interval. A sliding window can be an overlapping one. This is a good strategy to satisfy our second use case; to get the top N most clicked ads during the last M minutes.
    

  Monitoring metrics
    total Latency and also at each stage
    Queue size if used
    system resources : CPU, disk, JVM, etc.

  Reconcilliation for data consistency
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    Reconciliation means comparing different sets of data in order to ensure data integrity. Unlike reconciliation in the banking industry, where you can compare your records with the bank’s records, the result of ad click aggregation has no third-party result to reconcile with.

  Alternative designs for aggregation
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 
    In a generalist system design interview, you are not expected to know the internals of different pieces of specialized software used in a big data pipeline. Explaining your thought process and discussing trade-offs is very important, which is why we propose a generic solution. Another option is to store ad click data in Hive, with an ElasticSearch layer built for faster queries. Aggregation is usually done in OLAP databases such as ClickHouse [24] or Druid [25]. Figure 29 shows the architecture.


  Line protocol
    https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system
      CPU.load host=webserver01,region=us-west 1613707265 76
      CPU.load host=webserver01,region=us-west 1613707265 83

      The average CPU load could be computed by averaging the values at the end of each line. The format of the lines in the above example is called the line protocol. It is a common input format for many monitoring software in the market. Prometheus [6] and OpenTSDB [7] are two examples.

          A metric name	String
          A set of tags/labels	List of <key:value> pairs
          An array of values and their timestamps	An array of <value, timestamp> pairs

  Push vs Pull model
      https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system 
      Metrics monitoring : 
        Examples of pull architectures include Prometheus.
        Examples of push architectures include Amazon CloudWatch [16] and Graphite [17].

      Easy debugging: pull
      Health check : pull
      Short lived jobs : push
          Some of the batch jobs might be short-lived and don’t last long enough to be pulled. Push wins. But This can be fixed by introducing push gateways for the pull model [22].
      Firewall or complicated network setups : push
      Performance : Push
        since push uses UDP, it wins vs pull which relies on TCP
      Data authenticity	: Pull
        zookeeper can provide valid servers to pull from
      Fault tolerant and reliability : Pull
        since failed messages can be queried again

  gRPC
    For production systems, inter-service communication often employs a modern and high-performance remote procedure call (RPC) framework like gPRC. There are many benefits to using such frameworks. To learn more about gPRC in particular, check out [3].
    NOT recommended for browser -> server communication

  Avoid Risk of double booking in transaction systems like hotel rooms, payment systems
    There are two common approaches to solve this problem:

      Client-side implementation. A client can gray out, hide or disable the “submit” button once a request is sent. This should prevent the double-clicking issue most of the time. However, this approach is not very reliable. For example, users can disable JavaScript, thereby bypassing the client check.

      Idempotent APIs. Add an idempotency key in the reservation API request. An API call is idempotent if it produces the same result no matter how many times it is called. Figure 8 shows how to use the idempotency key (reservation_id) to avoid the double-reservation issue. The detailed steps are explained below.

  Different types to avoid race conditions during concurrency
  applications : hotel booking for last room, uber drivers all responding to request
    1. Pessimistic locking 
        - as name suggests, it locks the record while read + update
        - good when data contention is high
        - slows down performance
    2. Optimistic locking
        - has validation check based on version number. So no locking is needed. 
        - good when data contention is low and improves performances
        - bad when data contention is high 
    3. database constraint
        - add constraint at table level
        - same as Optimistic and its easier to implement
    another example when you need locking for optimistic or database constraint : when all uber drivers confirm the ride

  CI/CD practices
    https://www.youtube.com/watch?v=42UP1fxi2SY
    - CI stands for continuous integration and is more common. Its the practice of using automation to enable teams to merge code changes in shared repository. Each commit triggers an automated workflow on the CI server that builds and runs series of tests to make sure commit is safe to merge into main branch.
      - CI process : code -> build -> test -> merge/commit 
      - tools for CI - github
      - tools to manage CI process: code (github), build (gradle, webpack, github actions), test (jest), release (jenkins, buildkite)

    - CD stands for continuous deployment but it is hard and many companies only apply to stateless systems. Needs good production monitoring for CD like datadog. Blue green deployments is common. Canary deployment is also possible for products with 100 million users.
    - CD is hard for database backend clusters or websocket cluster. So deployment process is manual here and needs dedicated platform team
      - CD process : code -> build -> test -> acceptance -> deploy in production
      - CD tools : github actions, jenkins, argoCD for kubernetes 

    How  companies perform modern CI/CD today ? 
          https://levelup.gitconnected.com/gitops-ci-cd-using-github-actions-and-argocd-on-kubernetes-909d85d37746 
      CI : code -> build -> test -> merge/commit -> push image to ECR and update image reference(version) in Helm chart (yaml file for kubernetes)
          - with help of github actions, it can build and run tests on the PR. Once commit is merged, it pushes the image in the respository

          ECR vs S3 for images - ECR is just an optimization on top of S3 for images
            https://www.reddit.com/r/aws/comments/eftq6u/difference_between_aws_s3_vs_aws_ecr/
            ECR basically operates as a proxy in front of S3 to facilitate container build-operations. Same as DockerHub or Docker Registry, this can reduce the amount of stored data by storing each layer of a container image and not duplicating the same layer over and over again (saves you storage space).
            If you were to just store a container image (say .tar or .jar) and directly upload to S3 you'll quickly surpass the amount of storage you'd have used in ECR due to the lack of layer de-duplication that would be occurring when using ECR. 
                Example: you build version 1 of a container image for your app, total size is 50MB. Then you make an update to you image that only affects the last RUN statement in your Dockerfile (only one layer of the image is changed)... that one layer is 2MB, so when you push that new version ECR will upload the 2MB layer, not the other 48MB that hasn't changed. If direct upload to s3 was used, this would have amount to 100MB of storage.

      CD : argoCD polls Helm chart and deploys to Kubernetes (AWS EKS)
          - argoCD detects this image via polling and deploys on kubernetes using helm charts (yaml file for kubernetes deployment - https://www.youtube.com/watch?v=w51lDVuRWuk&ab_channel=DevOpsJourney) 
  

  How to provison resources in the could
    https://www.youtube.com/watch?v=tomUWcQ0P3k
    Use terraform as Infrastructure as code

  
  How to store password in the database
    https://www.youtube.com/watch?v=zt8Cocdy15c
    use hash(password + salt(random string attached))
    salting helps in eliminating rainbow attacks - https://www.beyondidentity.com/glossary/rainbow-table-attack
    Even if you keep salt in same DB as password, attacker will never able to guess the actually password since it does not know how salt text is combined with password before hashing.

  Bare metal, VMs and containers
    https://www.youtube.com/watch?v=Jz8Gs4UHTO8
    VMs is on top of hypervisor. VM will have guest OS with applications to run. 
    Containers have container agent (eg., docker) instead of hypervisor to run the application in its own environment but uses instance/host OS to operate and have faster operations and more flexibility but suffers from security sharing common instance/host OS. To achieve security, you can have a container inside the VM but it looses flexibility now. 
    A bare metal server can host more containers than VMs

  Cloud Native practices
    https://www.youtube.com/watch?v=p-88GN1WVs8
    - microservice architecture
    - containers
    - Dev ops (ci/cd)
    - Cloud native open standards (distributed tracing, service mesh)

  Kubernetes
    https://www.youtube.com/watch?v=TlHvYWVUZyc
    - container orchestration system
    - VERY IMPORTANT :  use it when you need 
      - automated rollbacks (deployment) : kubernetes smartly roll back the deploy if the pod with new container image is throwing errors (health checks, exceptions)
      - self-healing (downtime)
      - horizontal scaling 
    - due to its complexity, do not use it if your org is small enough
    https://www.youtube.com/watch?v=VnvRFRk_51k&t=78s
      use master-worker architecture. 
        Control plane - 
          - contains API server which can accept configuration about your application through YAML file
          - controller managers to keep track of what's happening in the cluster
          - scheduler ensures pod placement on workers
          - etcd which has info about workers and also a backing store 
          - has another master as secondary to avoid SPOF 
        Workers 
          - deploys your application into pods which is a wrapper for containers. So one pod can have many containers.
          - usually its one pod per application. You only have multiple applications in one pod when its helper to another - example, envoy proxy or datadog agent.
          - kubelet - communications with control plane
          - container runtime deploys the container by polling the images from registry, start/stop container
          - kube-proxy : helps route to correct pods and load balancing for the pods.
          - If pod dies, new ones gets easily created.
          - Since each pod gets IP address, then won't it creates disruption if a pod dies. Example MySQL pod dies ? 
            - So pod is supported by "service" (may be kube-proxy) sitting in each pod. This concept which is similar to load balancer and IP address is assigned to a service instead of a pod. It also serves as a load balancer. 
            - ANOTHER GREAT ALTERNATIVE OPTION IS ISTIO SERVICE MESH ABSTRACTION
          Similar video : https://www.youtube.com/watch?v=2vMEQ5zs1ko


   What is Istio ? 
      https://www.youtube.com/watch?v=16fgzklcF7Y&t=197s&ab_channel=TechWorldwithNana 
      - First understand what is service mesh ? 
        - with microservices, there are other problems on service-to-service communication, security, retry, metrics and tracing. So service mesh has a control plane which automatically deploys a sidecar proxy besides our main application (likely within the pod) and thus services can talk to each other through those proxies and the network layer for service to service communication along with control plane is called service mesh. This can sidecar proxy is responsible for performing all these tasks. This way developers can only focus on main business logic.
        - It also helps in traffic splitting during Canary deployment. 
        - Service mesh is a paradigm (model) and Istio is one of its implementation. Istio uses open source proxy called Envoy proxy.  
      - Istio ? 
        - Architecture consists of control plane called Istiod and envoy proxies each hosted besides each application in the pod 
        - To configure, 
          - you DO NOT HAVE TO ADJUST DEPLOYMENT OR CONFIG FILES (YAML) within each microservice. 
          - ALL THE CONFIGURATIONS FOR ISTIO COMPONENTS WILL BE DONE IN ISTIOD ITSELF. 
          - Istio can be configured using the syntax of kubernetes yaml files by using CRD (custom component in kubernetes which allows using third party technologies like istio, prometheus). Now to deploy, you can easily apply them using kubectl (command line tools for kubernets) commands. So no need to learn new configuration language syntax.
          - for each CRD (using VirtualService and Desination Rules)
            - configure traffic routing  
            - configure traffic split 
            - retry rules
            - time outs 
            - In Skillz, I have also seen folks adding service specific configurations like feature flags inside istio configurations.
        - During deployment, Istiod converts these high level routing rules in CRDs into envoy specific configurations. So we configure just Istio control plane rather than configuring proxies  
        - Istiod also has helps with service discovery component using service registery
        - Istiod also helps with CA (symmetric encryption like TLS) 
        - gather telemetry data - (metrics and tracing) from envoy proxies which can be consumed later by monitoring services like Prometheus or tracing servers, etc. 
        - Istio also has another component called Istio ingress gateway (alternative to Nginx ingress controller). So it is hosted in the pod and it accepts the traffic from outside world and help redirect to your internal microservices using VirtualService rules in "gateway" CRD. 

  WebSocket - use for bi-directional communication 

     -------------------------------- Interesting open question on websocket ----------------------------

              Client <-> Websocket server vs Client <-> LB <-> Websocker server
                STILL A BIG QUESTION : https://stackoverflow.com/questions/65174175/how-do-websocket-connections-work-through-a-load-balancer 
                Nice implementatiom : https://dzone.com/articles/load-balancing-of-websocket-connections

                  - Websocket through LB is possible through tunneling and is possible with HaProxy LB : http://tinyurl.com/yurdrom2 
                  - NGINX LB also  supports WebSocket by allowing a tunnel to be set up between a client and a backend server. https://www.nginx.com/blog/websocket-nginx/ 
                  - Even companies like Whatsapp goes with this approach ( tried practically)

                Option 1 : Client <-> LB <-> Websocker server
                  Pros
                  - helps in scaling -  creates new socket connections, update existing HTTP connections to socket.  
                  - more security due to LB's reverse proxy capabilities
                  - easy to configure Websocket with available LBs like HaProxy or Ngnix 
                  
                  Cons
                  - connection limits (65K) per network IP interface. So LB becomes a bottleneck

                Option 2: Client <-> Websocket server implmented in https://dzone.com/articles/load-balancing-of-websocket-connections 
                  Pros 
                  - easier to scale
                  - A good way to implement is
                      1. client connects to LB which forwards requests to Service discovery 
                      2. Service discovery selects the best websocket chat server to select 
                      3. Client directly establishes connection to server's public IP
                  Cons
                  - security due to man in middle attack. This can be avoided by establishing Auth first 

                Option 3: connect with L3 LB 
                  https://stackoverflow.com/questions/12526265/loadbalancing-web-sockets?rq=4 

          --------------------------------- Interesting open question on websocket ----------------------------
 

      comparison of websocket vs long polling during scaling up / down  
        - https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o 

      Some systems does support websocket and load balancer https://bytebytego.com/courses/system-design-interview/nearby-friends 

    Types of socket connections
      https://www.ibm.com/docs/en/zos/2.1.0?topic=sockets-socket-types
      Stream - data transmitted in continuous stream with length so that the receiver can read that length and process the data. Its reliable means data is sent without error or duplication
      Datagram -  socket interface defines a connectionless service. Datagrams are sent as independent packets. The service provides no guarantees; data can be lost or duplicated, and datagrams can arrive out of order
      raw socket -  interface allows direct access to lower layer protocols, such as IP and Internet Control Message Protocol (ICMP). This interface is often used for testing new protocol implementations.
    Websocket API
      https://www.geeksforgeeks.org/difference-between-rest-api-and-web-socket-api/ 
      - Unlike REST, Websocket API does not require a new connection to be set up for each message to be sent between clients and servers. Once the connection is set up the messages can be sent and received continuously without any interruption.
      - WebSocket APIs are suitable for IoT Applications with low latency or high throughput requirements. 
      - Websockets could be scaled on single server ~ 65K ports
      - Challenges with websocket are security, browser compactibility are complex to handle
    Difference between socket and a port
      https://www.geeksforgeeks.org/difference-between-socket-and-port/?ref=rp
      Both Socket and Port are the terms used in Transport Layer. To be more precise, Socket is in session layer in OSI model. A port is a logical construct assigned to network processes so that they can be identified within the system. A socket is a combination of port and IP address. An incoming packet has a port number which is used to identify the process that needs to consume the packet.
    How to scale Websocket clusters
      https://bytebytego.com/courses/system-design-interview/nearby-friends
        For the WebSocket cluster, it is not difficult to auto-scale based on usage. However, the WebSocket servers are stateful, so care must be taken when removing existing nodes. Before a node can be removed, all existing connections should be allowed to drain. To achieve that, we can mark a node as “draining” at the load balancer so that no new WebSocket connections will be routed to the draining server. Once all the existing connections are closed (or after a reasonably long wait), the server is then removed.
         
        Releasing a new version of the application software on a WebSocket server requires the same level of care.
        It is worth noting that effective auto-scaling of stateful servers is the job of a good load balancer. Most cloud load balancers handle this job very well.
        So Suggest to use least-connection load balancer so that it can route new request to Web socket servers with least connections

    SSE (Server side events) - used for server -> client connection 
      https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource 
      - single direction from server -> client 
      Ideal use cases of SSE:
        Stock ticker streaming
        twitter feed updating
        Notifications to browser
        Live commenting (but websockets are good for here)
      SSE gotchas:
        No binary support
        Maximum open connections limit
      Advantages of Websockets over SSE:
          Real time, two directional communication.
          Native support in more browsers

    IOT devices
      https://www.youtube.com/watch?v=6mBO2vqLv38
      - all devices are connected to IOT gateway through MQTT or HTTP
      IOT device architecture
        https://www.youtube.com/watch?v=KeaeuUcw02Q&t=630s
        - IoT devices (sensors)
        - IoT Gateway (collect data from sensors)
        - Cloud (Processing engine or event processing layer)
        - Application layer or API management layer 
        entire layers above is secured by device manager and IAM
      IoT Reference architecture
        https://www.youtube.com/watch?v=KeaeuUcw02Q&t=630s
        - Device layer
          like sensors which are interconnected 
          eg., bluetooth (via mobile phone -> Wifi gateway), raspberry pi (via Wifi -> direct connect to ethernet), zigbee (via zigbee gateway)
        - Communication layer or Gateway layer
          Rest protocol - HTTP or MQTT
        - Bus layer or Aggregation layer
          acts as a message broker supports http server or MQTT broker or Gateway
        - Even processing and Analysics layer (cloud)
      How IoT devices connect to the internet
        https://theiotpad.com/different-ways-to-connect-iot-device-over-internet/
        Communication devices include Wi-Fi, Bluetooth, and Ethernet cables.
        Although Wi-Fis use more power than your average Bluetooth system, they are more reliable and scalable. You can receive signals to your mobile devices and relay signals that penetrate barriers like walls and objects.

      Practical 
        https://www.youtube.com/watch?v=QyIeFy2-MvU
        light bulb having an IP address connected via a computer application using node.js



 
  Use Nginx 
    - Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports API gateway : request forwarding based on different paths like api gateway
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth SSL. Also does optimizations with keep alive connection and cache to avoid SSL handshake - https://docs.nginx.com/nginx/admin-guide/security-controls/terminating-ssl-http/ 
         Also a good read https://www.nginx.com/blog/http-keepalives-and-web-performance/
    - Nginx supports L7 (application level load balancer) and also L4 ? 
    - For modern websites, Nginx in form of reverse proxy is set up at multiple layers 
        - first layer : edge server (caching or TLS connection)
        - second layer : API gateway / load balancer
        However, many of them combine both as one ingress service in Nginx+
    Nginx vs HaProxy 
      https://blog.gitnux.com/comparison/haproxy-vs-nginx/#:~:text=Haproxy%20and%20Nginx%20are%20both,web%20server%20depending%20on%20configuration.
      - The main difference between the two is that Haproxy is a proxy server specifically designed to handle high levels of traffic while Nginx can be used as either a proxy or web server depending on configuration. 
      https://cloudinfrastructureservices.co.uk/haproxy-vs-nginx-whats-the-difference/ 
      - At the time of HAProxy’s implementation, the processes do not share any memory that affects the configuration parameters and session persistence is not possible.
      - At the time of NGINX implementation, the processes do share memory.
      - Nginx uses Async IO which makes it memory efficiency and faster.
      - Nginx is used by Netflix, Atlassian, Dropbox, etc.
    Cons with Nginx
      - NGINX does not support exportable metrics (paid edition only)
      - NGINX supports gRPC, WebSocket, and but only support protocols like HTTP, HTTPS, and Email.


  Use gRPC as interservice communication between microservices in your cloud
  https://www.youtube.com/watch?v=gnchfOojMk4
    - its high performant because 
          -it uses protocal buffers as efficiency binary encoding format as compared to json and 
          - http 2.0
    - supports many programming languages
    So when to use REST vs gRPC ? 
      Here are use cases for a REST API:
        Web-based architectures
        Public-facing APIs for ease of understanding by external users
        Simple data communications
      A gRPC API is better for these use cases:
        High-performance systems
        High data loads
        Real-time or streaming applications
        gRPC is NOT recommended for browser -> server communication due to limited support
      

  Multipart upload when upload file or video
    https://bytebytego.com/courses/system-design-interview/s3-like-object-storage 
    It is possible to upload such a large object file directly, but it could take a long time. If the network connection fails in the middle of the upload, we have to start over. A better solution is to slice a large object into smaller parts and upload them independently. After all the parts are uploaded, the object store re-assembles the object from the parts. This process is called multipart upload.

    - S3 client supports multipart upload (https://www.baeldung.com/aws-s3-multipart-upload)
    - S3 also supports faster multipart upload through edge locations called "transfer acceleration" https://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/  
    - HTML support "multipart/form-data" encoding type which is used for forms that include binary data, such as an image or audio files. When a user submits a form with “multipart/form-data” encoding, the data is split into multiple parts and sent to the server in a way that preserves the binary data. The data is then reassembled by the server and processed accordingly.
    - you can parallelize video upload with GOP alignment (https://bytebytego.com/courses/system-design-interview/design-youtube)
    - For email attachment
      An email attachment is sent along with an email message, commonly with Base64 encoding [6]. There is usually a size limit for an email attachment. For example, Outlook and Gmail limit the size of attachments to 20MB and 25MB respectively as of June 2021. This number is highly configurable and varies from individual to corporate accounts. 
      Multipurpose Internet Mail Extension (MIME) [7] is a specification that allows the attachment to be sent over the internet. https://bytebytego.com/courses/system-design-interview/distributed-email-service

  Use GPU for video transcoding 
    Meta uses GPU as ASICs for AI Inference or video transcoding

  Use Snapshot Isolation to handle dirty read(even with read commited ), phantom read (non repeatable) and is high performant as compared to serializable (pessimistic locking)
    How snapshot actually works : https://www.youtube.com/watch?v=Tgpa9TrxsfU 
      it just stores all versions of modified rows in WAL and reads from a committed transaction in WAL. So you do not have to worry about someone modify/add the data when you are read (dirty or phantom read) because at transaction x, all rows were in consistent state.
    https://www.geeksforgeeks.org/what-is-snapshot-isolation/
    Solves Dirty read, phantom read aka non repeatable read 
    https://jennyttt.medium.com/dirty-read-non-repeatable-read-and-phantom-read-bd75dd69d03a
    With nice demo - https://techcommunity.microsoft.com/t5/sql-server-blog/serializable-vs-snapshot-isolation-level/ba-p/383281  
    Cockraoch DB uses serializable snapshot isolation by default - https://github.com/cockroachdb/cockroach/blob/master/docs/design.md 
    
    Issue with Snapshot Isolation : 
     "write skew anamoly" with this approach https://www.youtube.com/watch?v=eym48yrObhY 
      happens when write in one row affects the contraints of other rows 
      - example, unique username, atleast one doctor on call, booking meeting rooms based on time 
      


  Use CQRS pattern to improve performance
    https://medium.com/design-microservices-architecture-with-patterns/cqrs-design-pattern-in-microservices-architectures-5d41e359768c# 
    CQRS stands for Command and Query Responsibility Segregation, a pattern that separates read and update operations for a data store. Implementing CQRS in your application can maximize its performance, scalability, and security.

    CQRS could be combined with event sourcing since it will generate sync events from write DBs to read DBs. But most DBs put WAL only for 30 days by defaut. So its limited time event sourcing.

    Example CQRS application is digital wallet system design where you need to separate our read and write part - https://bytebytego.com/courses/system-design-interview/digital-wallet

    Example, nice examples of event sourcing design - Kafka 
      - https://www.youtube.com/watch?v=lg6aF5PP4Tc
          Event sourcing is an alternative means of storing data as log of events rather than table that is updated,read and deleted. In event sourcing, you create a table and append events to it as they occur building up an event log. So reading data has an additional step because you need to pull up all the events and reduce it to derive into table format. Doing this process in real-time for large number of events is complicated and thus needs pre-processing of table format data from event log. This technique is an example of CQRS where you write in event log and store it immediately in read-only table or materialized views for reading.  
          Confluence using Kafka as event log for writes and generates materialized views for read 

  Read after write consistency 
    https://avikdas.com/2020/04/13/scalability-concepts-read-after-write-consistency.html 
      Read-after-write consistency is the ability to view changes (read data) right after making those changes (write data). For example, if you have a user profile and you change your bio on the profile, you should see the updated bio if you refresh the page. There should be no delay during which the old bio shows up.

      If there is a delay, this is known as a read-after-write inconsistency.

        It’s important to note the consistency only applies to the one performing the write. For example, you may update your profile, but someone else may not see the update for another minute. For most large-scale systems, some amount of delay like this is inevitable, but it’s really important the original writer see their update immediately.
      
      So then what are some solutions ? Pg 163 DDIA
       - always make read from master if its user's own info 
       - but what if there are many editable fields - maintain the mapping of the info most recently modified and based on that read from leader
       - have client store timestamp of most recent write

       Facebook's solution on read-after-write ; https://www.cs.utexas.edu/~vijay/cs380D-s18/feb6-fb.pdf 

  Monotonic read 
      Pg 164 DDIA 
    means user is read the data moving backwards in time. Its possible if one read goes to updated replica and another to outdated replica.
    This is the reason why always have consistent hashing selection of server based on user id 
  
  Consistent prefix reads
    https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#consistent-prefix-reads 
      - If a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order (write 1 : "add milk" and write 2 : "remove milk"). This is a particular problem in partitioned (sharded) databases as there is no global ordering of writes.
      - A solution is to make sure any writes casually related to each other are written to the same partition. If its not possible to write in same partition(large DB), then using version vector is a good approach to capture happens-before (causal) relationship.

  Use Parquet column over row format or csv file 
    https://www.upsolver.com/blog/apache-parquet-why-use
    https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705
      Have you ever used pd.read_csv() in pandas? Well, that command could have run ~50x faster if you had used parquet  instead of CSV.
      Traditionally there are three main layouts that convert our 2 dimensional table down to 1:
        Row-based: sequentially store rows (CSV).
        Column-based: sequentially store columns (ORC).
        Hybrid-base: sequentially store chunks of columns (Parquet).
      hybrid layouts are really effective for OLAP workflows because they support both projection(select) and predicates(where).
      Before moving on, it’s important to note that parquet is often described as a columnar format. However, due to the fact that it stores chunks of columns, as shown in the bottom of figure 2, a hybrid storage layout is a more precise description.
      Parquet leverages metadata to skip parts of our data that can be excluded according to our predicate. 
      Parquet intelligently solves this by storing max and min values for each row group, allowing us to skip entire row groups.
      Additionally it supports compression techniques like RLE, Dictionary and Bit encoding

  

  Use Circuit breaker pattern to support Fault tolerant/resilient services (like services doing retry for failed operations)
    https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker
    The Circuit Breaker pattern, popularized by Michael Nygard in his book, Release It!, can prevent an application from repeatedly trying to execute an operation that's likely to fail. Allowing it to continue without waiting for the fault to be fixed or wasting CPU cycles while it determines that the fault is long lasting.
      The purpose of the Circuit Breaker pattern is different than the Retry pattern. The Retry pattern enables an application to retry an operation in the expectation that it'll succeed. The Circuit Breaker pattern prevents an application from performing an operation that is likely to fail. An application can combine these two patterns by using the Retry pattern to invoke an operation through a circuit breaker. However, the retry logic should be sensitive to any exceptions returned by the circuit breaker and abandon retry attempts if the circuit breaker indicates that a fault is not transient.

     

  Choosing the right Batch size - https://bytebytego.com/courses/system-design-interview/distributed-message-queue 
    The choice of the batch size is a classic tradeoff between throughput and latency (Figure 13). With a large batch size, the throughput increases but latency is higher, due to a longer wait time to accumulate the batch. With a small batch size, requests are sent sooner so the latency is lower, but throughput suffers. Producers can tune the batch size based on use cases.

  Choosing pull over push model for Queue. - https://bytebytego.com/courses/system-design-interview/distributed-message-queue 
    Pros:

      Consumers control the consumption rate. We can have one set of consumers process messages in real-time and another set of consumers process messages in batch mode.

      If the rate of consumption falls below the rate of production, we can scale out the consumers, or simply catch up when it can.

      The pull model is more suitable for batch processing. In the push model, the broker has no knowledge of whether consumers will be able to process messages immediately. If the broker sends one message at a time to the consumer and the consumer is backed up, new messages will end up waiting in the buffer. A pull model pulls all available messages after the consumer’s current position in the log (or up to the configurable max size). It is suitable for aggressive batching of data.

    Cons:
        When there is no message in the broker, a consumer might still keep pulling data, wasting resources. To overcome this issue, many message queues support long polling mode, which allows pulls to wait a specified amount of time for new messages [6].

  Transactional database like SQL
    Pros :
    1. guarantees ACID (ddia pg. 224)
        Atomicity : (overloaded and different from atomic operations in multithreaded programming), here it just means abortability if anything fails
        Consistency : (overloaded due to replica consistency, consistent hashing and in CAP theorem), here it just means invariants in your database must hold true. Example, credits and debits must be balanced. Your application should control this but at times even database can help by integrity constraints (db constraints in hotel reservation system - https://bytebytego.com/courses/system-design-interview/hotel-reservation-system). The letter C does not belong in ACID since its upto the application to handle C
        Isolation : means that concurrently executing transaction are isolated from each other. In textbooks, isolation is formalized as "serializability" which ensures that when all transactions have committed, the result is same as if they all ran serially(one after another) even though they may have run concurrently causing race condition. Oracle 11g offers weak isolation level called Snapshot Isolation
        Durability : committed transactions are never lost despite of failures like disk, network, outage,etc. Solved using WAL. 

        Isolation level pg 234 DDIA
        1. Read committed : can solve the problem of dirty reads and dirty writes. It can be implemented with row level locking.
        locking is fine for writes but slows down read. So to avoid locking on read operations, we need to allow reading the last committed read.It ensures two things - 1. No dirty reads and 2. No dirty writes. This is also a default isolation level for MySQL and PostgreSQL.
        2. Snapshot isolation / Reapeatable Read: 
          With last committed read, there is still an issue of databasse concurrency issue. If user A has 500$ in account 1 and 2 and transfer 100$ from account 1 to account 2, balance is first updated and committed for account 1 and then on account 2. If before account 2 committed, values are read, then from it may appear that the user have only 900$ (account 1 is now 400$ but account 2 is still 500$).
          This isolation solves this problem by taking the snapshot at particular version (MVCC say using timestamp) and allows reading only from that version. This is used in Oracle 11g, Google Cloud spanner and CockRoachDB.


          Issue with snapshot isolation and also read committed isolation : The read committed and snapshot isolation levels we’ve discussed so far have been primarily about the guarantees of what a read-only transaction can see in the pres‐ ence of concurrent writes. We have mostly ignored the issue of two transactions writing concurrently. The best known of these is the lost update problem, illustrated in Figure 7-1 with the example of two concurrent counter increments.

            Concurrent write can cause lost update problem - how to solve it ? 
              - using atomic writes (locks on DB)
              - using application locks
              - using compare-and-set method (update something where something)

            But what if its a write skew or phantom (where a write in one transaction changes the result of a search query in another transaction, is called a phantom)
              - use "materialize conflicts" technique by creating rows. Example, creating meeting room time slots records to ensure no two rooms are in one time slot. But this will not work in cases like unique username or atleast one oncall, you cannot create records of all possible usernames.
              - Use serializable isolation : all read and write transactions are serializable
              - To improve performance by unblocking, you can use Snapshot Serializable solution where you keep reading from consistent snapshot while writes would be serializable by first checking if the snapshot version is correct before processing the write. 

            https://efficientcodeblog.wordpress.com/2017/12/25/leaderless-replication-dynamo-style-quorum-consensus-eventual-consistency-high-availability-and-low-latency/ 
            But what if its distributed -  multileader or leaderless replication ? 
              - if multileader, make sure there is atleast quorum consistency ( strong ) consistency in each datacenter
              - first make sure there is atleast quorum consistency ( strong ) consistency in each datacenter or server(leaderless)
              - some ways to resolve conflicts at DB level
                - now for conflicts, use different strategies
                  - conflict avoidance (just make writes of an item from one leader)
                  - LWW
                  - Conflict on read (calls conflict resolution code on read like Dynamo read-repair /Azure CosmosDB/CouchDB read-repair)
                  - Conflict on write (calls conflict resolution code on write like CRDTs)
                  - Version vectors (like in Riak 2.0) 

                  5. Quorum consistency
                    - But how to ensure you are consistent values from different DB nodes ? 
                    - Quorum consistency (strong) - achieving concensus with R + W > N. Usually N is picked as odd number in DynamoDB
                          w = r = (n + 1) / 2
                      - Limitations of Quorum consistency 
                          Higher Latency – Lower Availability
                          Quorum Consistency can sometimes lead to higher latency and lower availability if proper caution is not taken and there is a network interruption and many replicas become unreachable. This is because, if the number of reachable replicas falls below w or r, the database would become unavailable for writing or reading.
                      - Eventual Consistency
                        From the discussion so far we see that, although quorums appear to guarantee that a read returns the latest written value, in practice it is not so simple. Dynamo-style databases are generally optimized for eventual consistency. The parameters w and r allow us to adjust the probability of stale values being read, but it’s wise to not take them as absolute guarantees.
                        Application - Leaderless replication which can tolerate stale reads and has high availability and low latency
                      - Sloppy quorums and hinted handoff 
                          https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sloppy-quorums-and-hinted-handoff
                            Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n.
              - Multi-datacenter operation
                Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link. 

            Lets take a look in details : 

            Preventing Lost updates
                We have mostly ignored the issue of two transactions writing concurrently
                1. Atomic writes
                    For example, the following instruction is concurrency-safe in most relational databases:
                      UPDATE counters SET value = value + 1 WHERE key = 'foo';
                    Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. This technique is sometimes known as cursor stability.
                2. Explicit locking
                    Another option for preventing lost updates, if the database’s built-in atomic opera‐tions don’t provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated.
                      BEGIN TRANSACTION;
                        SELECT * FROM figures
                        WHERE name = 'robot' AND game_id = 222
                        FOR UPDATE;
                        -- Application logic code starts here
                          -- Check whether move is valid, then update the position
                          -- of the piece that was returned by the previous SELECT.
                        -- Application logic code ends here
                        UPDATE figures SET position = 'c4' WHERE id = 1234;
                      COMMIT;
                      The FOR UPDATE clause indicates that the database should take a lock on all rows returned by this query.
                        This works, but to get it right, you need to carefully think about your application logic. It’s easy to forget to add a necessary lock somewhere in the code, and thus introduce a race condition.
            Automatically detecting lost updates
              1. Compare-and-set
                  If the content has changed and no longer matches 'old content', this update will have no effect, so you need to check whether the update took effect and retry if necessary
                    UPDATE wiki_pages SET content = 'new content'
                    WHERE id = 1234 AND content = 'old content';
              2. Conflict resolution and replication
                  Locks and compare-and-set operations assume that there is a single up-to-date copy of the data. However, databases with multi-leader or leaderless replication usually allow several writes to happen concurrently and replicate them asynchronously, so they cannot guarantee that there is a single up-to-date copy of the data. 
                  Instead, as discussed in “Detecting Concurrent Writes” on page 184, a common approach in such replicated databases is to allow concurrent writes to create several conflicting versions of a value (also known as siblings), and to use application code or special data structures to resolve and merge these versions after the fact. On the other hand, the last write wins (LWW) conflict resolution method is prone to lost updates, as discussed in “Last write wins (discarding concurrent writes)” on page 186. Unfortunately, LWW is the default in many replicated databases. 
            Write Skew and Phantoms
              In the previous sections we saw dirty writes and lost updates, two kinds of race conditions that can occur when different transactions concurrently try to write to the same objects. 
              But what if the objects are different ? The explicit locking query won't work in following cases because the returned rows are different
                1. there are at least two doctors on call,
                2. there are no existing bookings for that room at that time, 
                3. the position on the board doesn’t already have another figure on it, 
                4. the username isn’t already taken, there is still money in the account
              This effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew. 
              Materializing conflicts : please consider this as last resort though due to complexities
                For example, in the meeting room booking case you could imagine creating a table of time slots and rooms. Each row in this table corresponds to a particular room for a particular time period (say, 15 minutes). You create rows for all possible combinations of rooms and time periods ahead of time, e.g. for the next six months.
                Now a transaction that wants to create a booking can lock (SELECT FOR UPDATE) the rows in the table that correspond to the desired room and time period. After it has acquired the locks, it can check for overlapping bookings and insert a new booking as before.
                This approach is called materializing conflicts, because it takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database 
        3. Serializability  
            Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency.
            1. Actual Serial Execution
              The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.
              Two developments caused this rethink: 
                a. RAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory (see “Keeping everything in memory” on page 88). When all data that a transaction needs to access is in memory, transactions can execute much faster than if they have to wait for data to be loaded from disk.
                b. Database designers realized that OLTP transactions are usually short and only make a small number of reads and writes (see “Transaction Processing or Analytics?” on page 90). By contrast, long-running analytic queries are typically readonly, so they can be run on a consistent snapshot (using snapshot isolation) outside of the serial execution loop.
              Encapsulating transactions in stored procedures
                systems with single-threaded serial transaction processing don’t allow interactive multi-statement transactions. Instead, the application must submit the entire transaction code to the database ahead of time, as a stored procedure.
                Stored procedures have existed for some time in relational databases, They have gained a somewhat bad reputation, for various reasons:
                  Each database vendor has its own language for stored procedures. These languages haven’t kept up with developments in general-purpose programming languages, so they look quite ugly.
                  A database is often much more performance-sensitive than an application server, because a single database instance is often shared by many application servers. A badly written stored procedure (e.g., using a lot of memory or CPU time) in a database can cause much more trouble than equivalent badly written code in an application server
                Partitioning
                  Executing all transactions serially makes concurrency control much simpler, but lim‐its the transaction throughput of the database to the speed of a single CPU core on a single machine. Read-only transactions may execute elsewhere, using snapshot isola‐tion, but for applications with high write throughput, the single-threaded transaction processor can become a serious bottleneck. If you can find a way of partitioning your dataset so that each transaction only needs to read and write data within a single partition, then each partition can have its own transaction processing thread running independently from the others. Since cross-partition transactions have additional coordination overhead, they are vastly slower than single-partition transactions. VoltDB reports a throughput of about 1,000 cross-partition writes per second, which is orders of magnitude below its single-partition throughput and cannot be increased by adding more machines 
            2. Two-Phase Locking (2PL)
                    - If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue. (This ensures that B can’t change the object unexpectedly behind A’s back.)
                    - If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue. (Reading an old version of the object, like in Figure 7-1, is not acceptable under 2PL.)
                  After a transaction has acquired the lock, it must continue to hold the lock untilthe end of the transaction (commit or abort). This is where the name “twophase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.
            3. Serializable Snapshot Isolation (SSI)  
                an algorithm called serializable snapshot isolation (SSI) is very promis‐ing. It provides full serializability, but has only a small performance penalty com‐pared to snapshot isolation. Today SSI is used both in single-node databases (the serializable isolation level in PostgreSQL since version 9.1 [41]) and distributed databases (FoundationDB, CockroachDB uses a similar algorithm).
                Basically you read using Snapshot Isolation but while writing, the database detects its a stale snapshot(new values are updated) and instructs abort/retry operations
                How does the database know if a query result might have changed? There are two cases to consider:
                  • Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)
                  • Detecting writes that affect prior reads (the write occurs after the read)
              Performance of serializable snapshot isolation
                Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another trans‐action. Like under snapshot isolation, writers don’t block readers, and vice versa. 
                Compared to serial execution, serializable snapshot isolation is not limited to the throughput of a single CPU core.
                The rate of aborts significantly affects the overall performance of SSI. For example, a transaction that reads and writes data over a long period of time is likely to run into conflicts and abort, so SSI requires that read-write transactions be fairly short

    Consistency and Concensus with distributed databases  Pg 321

    What is Linearizability vs Serializability
        - Linearizability is the recency guarantee while Serializability is the ordering guarantee
         

  How to design ranking system based on multile criteria (used in multiple designs like google ranking, FB ranking)
    Simple algorithm
      https://towardsdatascience.com/ranking-algorithms-know-your-multi-criteria-decision-solving-techniques-20949198f23e
      Consider one attribute at a time and try to maximize or minimize it (as per the requirement) to generate optimized score.
      Introduce weights to each attributes to get optimized weighted scores.
      Combine the weighted scores (of each attribute) to create a final score for an entity (here car).
      for reverse criteria - just use weight * (1 - column %)
    ML algorithm
      https://www.youtube.com/watch?v=lh9CNRDqKBk&t=1168s&ab_channel=MLOps.community 
      Today most systems uses ML for real time ranking. Ranking is always preceded by "Retrieval"
      - Retrieval is performed with embedding model + ANN (nearest neighbour fetch)
      - Ranking is done by adding feature set and using ranking model


  Use Virtual Cluster map for placement service/resource manager (placement service will help to indentify where to place the job or upload the file)
    https://bytebytego.com/courses/system-design-interview/s3-like-object-storage
    The placement service determines which data nodes (primary and replicas) should be chosen to store an object. It maintains a virtual cluster map, which provides the physical topology of the cluster. The virtual cluster map contains location information for each data node which the placement service uses to make sure the replicas are physically separated. This separation is key to high durability. See the “Durability” section below for details.


  Perform rate limiting whenever possible to absorb spikes (eg., NewsFeed service)
    https://bytebytego.com/courses/system-design-interview/design-a-news-feed-system 

  Saving storage space
    https://bytebytego.com/courses/system-design-interview/design-google-drive 
    Moving infrequently used data to cold storage. Cold data is the data that has not been active for months or years. Cold storage like Amazon S3 glacier [11] is much cheaper than S3.

  Try to keep UI and back-end as separate services
    https://michaelwashburnjr.com/blog/4-reasons-web-app-separated-frontend-backend
      Modularity, Re-usability, Content Delivery, Responsiveness(?) and Versioning

  How VPN works ?

  CORS - Cross origin resource sharing 
    https://www.youtube.com/watch?v=4KHiSt0oLJ0&ab_channel=Fireship
    by default, browser does not allow requests to another domain on your website. To allow this, you need to send "Access-Control-Allow-Origin" header in the response.

  

=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================


Distributed systems today and Benchmarking on common systems

    Live streaming
      https://www.youtube.com/watch?v=7AMRfNKwuYo
      - streamer device(encoder) --> PoPs --> transcoding(different resolutions and bitrate) --> packaging (hls, dash format) --> PoPs --> receiving device (decoder)
            1. streamer starts his stream wired up to an encoder (software in browser, webcam, mobile ) which packages the video stream and send it in a transport protocol that the live streaming platform can recieve for further processing.
            2. Transcoding : convert video into different resolutions, frame rate and bit rates which is called "adaptive bit rate streaming". 
              a. Segmentation : transcoded stream is divided into smaller video of 1s called segments
              b. Conversion : Then it converts video into different resolution and bit rates
               Both steps requires massive computation and done in parallel which needs lots of CPU. That's why DAG is used here.
            3. Then Transcoded segments are packaged to support most common live streaming format - HLS, DASH (.mp4). 
                Both of these format consist of the manifest file  and the series of video chunks where each chunk contains video of few seconds. 
                This manifest file is like a directory to tell the client video player what all output formats are, where to load the video chunks over HTTP.
                Both manisfest file and video chunks are usually cached in CDN for faster streaming.

                Just right click on any youtube video and view network call, it requests manifest file periodically ( 5 s)

          Open questions 
            - Why not UDP at streamers device
            - Why PoP
            - If CDN is used at receivers device, how updates on manifest file and video is published
              1. use TTL. when cache reaches TTL 
              2. OR use push updates to the CDN and cache
            - How to avoid thundering herd ? 
              - USE PROMISE CACHE. 
                  - basically if say 1 M clients requests for same item and there is cache miss. So instead of forwarding all requests to DB, only one request is made to DB to populate the cache. Rest all the requests are given promises.

            Questions are answered below
      - Live Streaming system design in detail 
          https://www.youtube.com/watch?v=S8aSoSQJ4G8&t=1079s&ab_channel=System-DesignByVinayakSangar
            - Avoid UDP at source : UDS is fast but lossy which means it can loose resolution at source itself. So use TCP at source and protocol used is RTMP which is based on TCP.  

      * Video transcoding is also called video encoding
        Many types of encoding formats are available; however, most of them contain two parts:
          Container: This is like a basket that contains the video file, audio, and metadata. You can tell the container format by the file extension, such as .avi, .mov, or .mp4.

          Codecs: These are compression and decompression algorithms aim to reduce the video size while preserving the video quality. The most used video codecs are H.264, VP9, and HEVC.

      Why RTMP for live streamer and HLS/DASH for receiver/consumer
        https://linuxhit.com/rtmp-vs-hls-vs-dash-streaming-protocols/#0-rtmp-vs-hls-vs-dash-streaming-protocols 
        RTMP vs HLS vs DASH – Which one should I use?
          If you are producing live video content then you probably want to publish your stream to a streaming server via RTMP. It is possible to allow consumption of the stream via RTMP. But you probably don’t want to do that. HLS has widespread support on client devices that consume your video. Additionally you can leverage CDNs and not need to worry about firewalls.

      - Protocols
        - the most popular transport protocol on streamer side is RTMP which is TCP based but moving towards SRT which is UDP based
        - remember hls, dash formats are different from streaming protocol - hls, dash.
        - RTMP is a TCP-based protocol that is widely used because it offers persistent connections and low-latency streaming. On the other hand, HLS is an HTTP-based protocol for adaptive bitrate streaming of live and on-demand content. It is often better than RTMP because it has a lower latency. https://castr.com/blog/rtmp-vs-hls/ 

        Why not UDP for live streamer? 
          - you will loose packets on source itself

      

    Video conferencing app like Zoom
      https://medium.com/@himanishaik48/zoom-system-design-most-frequently-asked-question-in-interview-f60f6fe8d198 
 
    Multiplayer Gaming distributed app
      https://theredrad.medium.com/designing-a-distributed-system-for-an-online-multiplayer-game-requirements-part-2-de1d1ae9ae9b
      - use of kubernetes
      - use of TCP tunnels
      How unity company multiplayer game architecture - https://www.youtube.com/watch?v=77vYKsXC4IE 
        - Amazon GameLift has dedicated server for hosting games
        - GCP also has game server built on top of kubernetes 
        - use auth provider like playfab
        - use matching server like playfab matching or flexmatch from amazon gamelift
        - leaderboard, 
        - persistent data like sql
        - use GameAnalytics

    Design Live commenting system 
      https://leetcode.com/discuss/interview-question/583184/FBInstagram-'Live-Comments'-System-design/918003 
      - Same as proxity service in byte byte go
        - client <-> server websocket connection. You can use via LB using TCP tunneling
        - redis pub sub 
        - DB for persistence 
      - My try : https://leetcode.com/discuss/interview-question/583184/FBInstagram-'Live-Comments'-System-design/918003 
      - Facebook's  --> Write locally, Read globally technique (similar to sequential consistency, read after write consistency from cache)

    Design search engine
      https://medium.com/double-pointer/system-design-interview-search-engine-edb66b64fd5e 
        Web -> Crawler -> Indexer -> Retiever -> Ranking <-> Web Browser or Mobile

        Crawler : crawls from robots.txt, uses some seed URLs, uses priority Q, adds links in priority Q to crawl next
        Indexer : pre-process (remove stop words), indexes usually inverted index as word -> doc ids
        Retreiver : 
          Since search query have multiple words, two types : 
            Conjunctive Search which is AND based 
            Disjunctive Search which is OR based

    Design top K leaderboard deepdive 
        https://www.youtube.com/watch?v=XbkjEX-jgj0&ab_channel=Jordanhasnolife 

          High level Design

                                                                              (Lambda architecture)

         Client --> LB ---> Leaderboard service ---------> Queue ---------> Count 1 min sketch ( 1 min window precomputation) 


                                                                 ---------> large time window (1 hour window precomputation) -> MySQL 


                                                                 ---------> any time window (exact count) ------------> Time series (burst reads. So no precomputation, so slow for ranking)


                                                                 --------> no time window (over all time) ------------> MySQL


    
    Design recommendation system 
      https://www.youtube.com/watch?v=ZAIBFf8KpgI&ab_channel=Jordanhasnolife 
        Next video to watch about embedding model - https://www.youtube.com/watch?v=lh9CNRDqKBk&t=1630s&ab_channel=MLOps.community 
      These days preference is more on building online recommendation system rather than offline since offline / batch recommendation stops us from building real-time searches and esp. on viral videos 


        Client --> LB ---> Recommendation service ( fan - out because of realtime) ----> Retrieval service (retrieve 1K using embedding model) 
                            |              |                                                (also handles blocked items using bloom filter) 
                            |              |
                            |              |--------------------------------------------> Ranking service (using weights and merge sort )
                            |                                                                 |
                            |                                                                 |
                            |                                                                 | (update the weights)
                            |-------------> User Activity Log parser ----> Batch ML Job ------


      - online recommendation needs retrieval (nearest neighbours) and ranking (rank those neighbours)
        - retrieval using embedding model (embedding is ML-friendly version of original input. learn more in first 8 mins video https://www.youtube.com/watch?v=yfHHvmaMkcA&ab_channel=freeCodeCamp.org )
          - But how fetch 1K nearest embeddings amongst billions of recommendations ? 
            option 1. using geo spatial index ( geaohash for map is 2-dimensional but here is multi-dimensional). Its complex though.
            option 2. using the cache index for each embeddings, here are nearest neighbours. Demands space.
            option 3. using vector database which helps groups embeddings into buckets so your retrieval is faster : https://www.youtube.com/watch?v=72XgD322wZ8&ab_channel=codebasics 

        - ranking using distributed ranking service 
          - But how to rank these huge request with each having 1K neighbours ? 

      https://www.youtube.com/watch?v=lh9CNRDqKBk&t=1630s&ab_channel=MLOps.community 
      - place the embedding model on our retrieval services 
          - Embedding model -> Item data into embedding model -> get 100/1000 nearest neighbouts 
      - place the ranking algo on distributed rabking services
          - Add features to candidates -> rank top k candidates 
      Meta used Multiray platforms for embedding - https://ai.meta.com/blog/multiray-large-scale-AI-models/ 

      Practical of retrieval using embedding : https://www.youtube.com/watch?v=ySus5ZS0b94&ab_channel=AdrianTwarog 


    How to design document retreival
      https://www.youtube.com/watch?v=fcIzAg63WyI&ab_channel=Codetuber
      - using tf * idf like in elastic search 
        https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3
        - tf for term t   = # of term t in doc / total terms 
        - idf for term t  = log ( total docs / # of docs containing term t ) -> basically how common the term is. Example, is, the, a are very common
        - relevance of doc d for term t = tf * idf 

        - Elastic search doc retrieval ranking in 2.0 (https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html)
          - using tf/idf = sum of scores based on each term's tf * idf 
              - score(q,d)  =  
                queryNorm(q)  
              · coord(q,d)    
              · ∑ (           
                    tf(t in d)   
                  · idf(t)²      
                  · t.getBoost() 
                  · norm(t,d)    
                ) (t in q)    

        - Elastic search doc retrieval ranking in 8.10
          - using Practical Scoring Function (BM25) by default 
            https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables 
      Difference between retrieval and classification
        Major difference between classification and retrieval: Classification needs labels for training data, retrieval does not. Retrieval is a purely distance-based approach. 

    Types of database indexes
      https://medium.com/must-know-computer-science/system-design-indexes-f6ad3de9925d 


    Layer 4 Load balancer
      https://www.nginx.com/resources/glossary/layer-4-load-balancing/
      When the Layer 4 load balancer receives a request and makes the load balancing decision, it also performs Network Address Translation (NAT) on the request packet, changing the recorded destination IP address from its own to that of the content server it has chosen on the internal network. Similarly, before forwarding server responses to clients, the load balancer changes the source address recorded in the packet header from the server’s IP address to its own. 
      Layer 4 load balancers make their routing decisions based on address information extracted from the first few packets in the TCP stream, and do not inspect packet content.

    Layer 7 Load balancer
      https://www.nginx.com/resources/glossary/layer-4-load-balancing/ 
      Layer 7 load balancers base their routing decisions on various characteristics of the HTTP header and on the actual contents of the message, such as the URL, the type of data (text, video, graphics), or information in a cookie.
      Taking into consideration so many more aspects of the information being transferred can make Layer 7 load balancing more expensive than Layer 4 in terms of time and required computing power, but it can nevertheless lead to greater overall efficiency. For instance, because a Layer 7 load balancer can determine what type of data (video, text, and so on) a client is requesting, you don’t have to duplicate the same data on all of the load-balanced servers.

    
    Map Reduce
      https://www.youtube.com/watch?v=MAJ0aW5g17c (awesome)
        Example of GFS performing map function on worker servers and reduce using worker servers co-ordinated by master
      The MapReduce framework is a good option to aggregate ad click events. The directed acyclic graph (DAG) is a good model for it [9]. The key to the DAG model is to break down the system into small computing units, like the Map/Aggregate/Reduce nodes (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation). Facebook uses DAG model for its video streaming
      

    Zookeeper 
    https://www.youtube.com/watch?v=gZj16chk0Ss&t=413s&ab_channel=ddddd
    - What it does  ?
      - which node is the master ? (leader election)
      - what tasks are assigned to which workers ? (distributed store)
      - which workers are available currently ? (service discovery)
      - is the lock owned ? (lock ownership)
      - helps in total ordering of transactions ? (total order)

    - How does it work ? 
      - it is push-based so that workers gets notified of any changes

      https://bytebytego.com/courses/system-design-interview/distributed-message-queue
       If you are not familiar with it, Zookeeper is an essential service for distributed systems offering a hierarchical key-value store. It is commonly used to provide a distributed configuration service, synchronization service, and naming registry [2]. 
     - centralized configuration, key value store for state and also co-ordination service like selecting leader/master
     - can perform as a shard manager to find out the shard based on key ( https://kousiknath.medium.com/all-things-sharding-techniques-and-real-life-examples-in-nosql-data-storage-systems-3e8beb98830a#:~:text=You%20can%20use%20some%20configuration,directly%20talk%20to%20the%20shard. )
     - The number of partitions and addresses of all Redis nodes can be stored in a centralized place. We could use Zookeeper [4] as a highly-available configuration storage solution. (https://bytebytego.com/courses/system-design-interview/digital-wallet)
     - Apache Zookeeper [7] is a popular open-source solution for service discovery. It registers all the available chat servers and picks the best chat server for a client based on predefined criteria. (https://bytebytego.com/courses/system-design-interview/design-a-chat-system). You can also use this strategy for game server. 
     - There are many service discovery packages available, with etcd [4] and Zookeeper [5] among the most popular ones. Our need for the service discovery component is very basic. https://bytebytego.com/courses/system-design-interview/nearby-friends 
        - Under the “Key” mentioned in point 1, we store a hash ring of all the active Redis pub/sub servers in the service discovery component (See the consistent hashing chapter in Volume 1 of the System Design Interview book or [6] on details of a hash ring). The hash ring is used by the publishers and subscribers of the Redis pub/sub servers to determine the pub/sub server to talk to for each channel. 
      https://ramcloud.atlassian.net/wiki/spaces/RAM/pages/6848719/ZooKeeper+Performance 
    - Interesting fact about zookeeper 
      https://stackoverflow.com/questions/37293928/zookeeper-vs-in-memory-data-grid-vs-redis 
    - Zookeeper avg req latency ~ 10 ms 
        - 2-core 3 node cluster
        https://ramcloud.atlassian.net/wiki/spaces/RAM/pages/6848719/ZooKeeper+Performance
    

    Kafka
      https://www.youtube.com/watch?v=UNUz1-msbOM
      latency : 5 ms
      Why fast :
      - sequential access from the disk with append-omly log 
      - zero copy principle ( copy path from disk -> os buffer -> application buffer -> socket buffer -> nic buffer TO  disk -> os buffer  -> socket buffer )
        - visual in ( Bytebytego Sessions, Tokens, JWT, SSO, and OAuth in One Diagram)
      How to guarantee "exactly-once" processing ? 
        1. https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/
            From producer to broker
              Through idempotence of message through sequnce number. So even if the producer sends twice, broker can dedup.
            From broker to consumer
              Through Atomic transaction - This feature also allows you to commit your consumer offsets in the same transaction along with the data you have processed, thereby allowing end-to-end exactly-once semantics. It can be enabled by selected isolation level = read committed.

      How Kafka stores offset by consumer group for each partition - https://bytebytego.com/courses/system-design-interview/distributed-message-queue
        In message queus, The data access patterns for consumer states (last consumed offset) are:
            Frequent read and write operations but the volume is not high.
            Data is updated frequently and is rarely deleted.
            Random read and write operations.
            Data consistency is important.

          Lots of storage solutions can be used for storing the consumer state data. Considering the data consistency and fast read/write requirements, a KV store like Zookeeper is a great choice. Kafka has moved the offset storage from Zookeeper to Kafka brokers. 
            

      Excatly once delivery is hard but Yelp implements it. Check it out https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 
        To achieve “exactly-once” processing, we need to put operations between step 4 to step 6 in one distributed transaction. A distributed transaction is a transaction that works across several nodes. If any of the operations fails, the whole transaction is rolled back.
      Benchmarking
        https://developer.confluent.io/learn/kafka-performance/
        5 ms
        600 MB/s on 25 Gbps network on AWS 
        1 million messages per second
        i3en.2xlarge instance type (with 16 vCores, 64 GB RAM, 2 x 2,500 GB NVMe SSDs)
        Good performance but do not use it for large storage

      When to use Kafka ? 
        https://www.engati.com/blog/redis-kafka-rabbitmq
        Use it when you need pull based architecture, lightening fast queuing, streaming and need message persistence
          Pull based : allows many consumer to connect on different offset
          Scale: Apache Kafka can scale up to send a million messages every second.
          Persistency: It supports data persistency.
          Consumer Capability: Kafka only supports one-to-many consumers
        - Kafka does not natively support delayed messages. However, there are a few ways to delay Kafka messages
            Sleep function: Pause processing for a set amount of time. This is a simple approach, but it can lead to long execution times.
            Karafka: Use Karafka's built-in partition pausing mechanism. This is an efficient and reliable way to process delayed messages.
            Kafka Stream: Use a custom MessageChooser to poll the message.

    RabbitMQ
      https://www.upsolver.com/blog/kafka-versus-rabbitmq-architecture-performance-use-case
      - Slow as compared to Kafa and RabbitMQ pub-sub 50K messages per second (https://www.engati.com/blog/redis-kafka-rabbitmq)
      - supports both point to point and pub-sub model.
      - once messages are consumed, the are removed from queue and acknowledgement is provided but they can also be persisted like Kafka depending on configuration
      - It can be both synchronous and asynchronous. supports acknowledge too
      - smart broker -> dumb consumer since it manages the offset consumed
      - uses push model from queue to consumer
      - has control over consistency 
      - does not have message ordering 
      - does not guarantee atomicity
      Use case - 
        Use it when you need push-based model,  do not need to persist message and play smart broker role (complex routing) - https://www.engati.com/blog/redis-kafka-rabbitmq
        - Applications that need to support legacy protocols, such as STOMP, MQTT (a messaging protocol for the Internet of Things (IoT).), AMQP, 0-9-1.
        - Granular control over consistency/set of guarantees on a per-message basis
        - (Smart broker) - Complex routing to consumers
        - Applications that need a variety of publish/subscribe, point-to-point request/reply messaging capabilities.

    Redis pub sub
      https://bytebytego.com/courses/system-design-interview/nearby-friends 
      https://www.engati.com/blog/redis-kafka-rabbitmq
      Use it when you need lightening fast queueing service without persisting message.
        Push based system : smart broker, dumb consumer
        Scale: Redis can send up to 1 million messages per second like Kafka
        Persistency: It does not support data persistency, it is an in-memory data store. However, you can configure to be persistent ~ 1 min snapshotting https://redis.io/docs/management/persistence/
        Consumer Capability: It can handle both, one-to-one and one-to-many consumers.
      Redis pub sub needs scaling support with external party like Zookeeper to discover the right node/shard holding the channel. Refer this awesome video - https://www.youtube.com/watch?v=6G22a5Iooqk&ab_channel=Redis
      


    RocketMQ 
      doesn’t support delayed messages with arbitrary time precision, but delayed messages with specific levels are supported. Message delay levels are 1s, 5s, 10s, 30s, 1m, 2m, 3m, 4m, 6m, 8m, 9m, 10m, 20m, 30m, 1h, and 2h. https://bytebytego.com/courses/system-design-interview/distributed-message-queue

    Hierarchical time wheel [17].
      https://bytebytego.com/courses/system-design-interview/distributed-message-queue
      for scheduled messages
 


    Streaming vs Batching systems 
     https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
     Batch : MapReduce
     Stream : Flink (also mentioned in https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system )


    Block storage vs File Storage vs Object storage
      https://aws.amazon.com/what-is/block-storage/
      Block storage
        Common storage devices like hard disk drives (HDD) and solid-state drives (SSD) that are physically attached to servers are all considered as block storage. Files are split into evenly size blocks of data with each block having its own address. Block storage presents the raw blocks to the server as a volume. This is the most flexible and versatile form of storage. The server can format the raw blocks and use them as a file system, or it can hand control of those blocks to an application.
        - can mount VM to this using iSCSI protocol
        - used in operational systems (its obvious) and databases
        - As per geeksforgeeks, this storage is not accessible via the internet and only for attaching to VMs (https://www.geeksforgeeks.org/difference-between-aws-s3-and-aws-ebs/)
        - Metadata(location of each block in the file, name,etc) is limited in block storage which makes it faster
        Granular control
          Developers gain a high degree of control over storing data on block storage. For example, they can optimize performance by grouping fast-changing data on specific blocks and storing static files on others. This improves system performance as ongoing updates only affect a small number of data blocks instead of an entire file. For example, block storage gives you the flexibility to tier fast-changing data on solid state disk (SSD) for the highest performance, and store warm or cold data on lower cost hard drives (HDD).
          By contrast, file storage has an extra layer consisting of a file system (NFS, SMB) to process before accessing the data.

      File storage
        - built on top of block storage. It provides a higher-level abstraction to make it easier to handle files and directories. Data is stored as files under a hierarchical directory structure. 
        - File storage could be made accessible by a large number of servers using common file-level network protocols like SMB/CIFS [3] and NFS [4].
        - high performant as block storage
        - used in big data applications, which demand significant node throughput, low-latency file access, and read-after-write operations.

      Object storage 
        https://bytebytego.com/courses/system-design-interview/s3-like-object-storage 
        - low cost, vast scalability and supports binary+unstructured data as compared to block and file. It is low performant though.
        - Object storage stores all data as objects in a flat structure. There is no hierarchical directory structure. Data access is normally provided via a RESTful API.
        - Data lake and big data analytics, Backup and restoration, Reliable disaster recovery and Methodical archiving using S# Glacier

      AWS EBS vs EFS vs S3 : https://www.missioncloud.com/blog/resource-amazon-ebs-vs-efs-vs-s3-picking-the-best-aws-storage-option-for-your-business
      EBS vs S3 video - https://www.youtube.com/watch?v=r8bVw0iVvGk 

      Block storage vs object storage
        https://aws.amazon.com/what-is/block-storage/ 
        Both storage solutions are beneficial depending on the use case. Block storage provides low latency and high-performance values in various use cases. Its features are primarily useful for structured database storage, VM file system volumes, and high volumes of read and write loads. Object storage is best used for large amounts of unstructured data, especially when durability, unlimited storage, scalability, and complex metadata management are relevant factors for overall performance.


        https://www.cloudflare.com/learning/cloud/object-storage-vs-block-storage/#:~:text=Block%20storage%20is%20fast%2C%20and,content%2C%20webpages%2C%20and%20emails. 
        Capability	                  Block storage	                                                    Object storage
        Storage capacity	            Limited	                                                            Nearly unlimited
        Storage method	              Data stored in blocks of fixed size, reassembled on demand	        Unstructured data in non-hierarchical data lake
        Metadata	                    Limited	                                                            Unlimited and customizable
        Data retrieval method	        Data lookup table	                                                  Customizable
        Performance	                  Fast, especially for small files	                                  Depends, but works well with large files
        Cost	                        Depends on vendor, usually more expensive	                          Depends on vendor, usually less expensive 
                                                                                                          (aside from egress fees)

   
  Observability systems today - top ones are Datadog and Prometheus 
    https://www.squadcast.com/compare/prometheus-vs-datadog-a-comparative-overview
    ![Alt text](image.png)
    Datadog vs Prometheus
    Pull vs Push
      Prometheus' pull-based model collects metrics from instrumented services at regular intervals and stores the collected information in its own time series database (Prometheus's own TSDB) for efficient querying and analysis, while Datadog offers both pull-based and push-based models, providing flexibility in data collection; its scalable distributed storage system (Datadog's own database) also makes it suitable for large scale deployments.
    Open source vs Paid
      Prometheus is an open-source tool and does not incur licensing costs, although costs associated with running Prometheus at scale like computing, storage or networking costs may incur. There are paid Prometheus as a service (PaaS) offerings from providers like Amazon, Google, Microsoft etc. which would cost you anywhere between $0.03 - $0.06 per Prometheus node per hour. 
      Datadog follows a subscription-based pricing model. The cost depends on the number of hosts and features required; while this comes at a cost, using Datadog provides dedicated support, regular updates, and managed infrastructure management - making your life simpler in many ways!

  Performance engineering 
    Linux commands 
      Cgroup
        https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01 
        Control groups (cgroups) are a feature of the Linux kernel that allows administrators to monitor and limit the resources used by groups of processes. Cgroups can limit the following resources: CPU usage, Memory usage, Block IO, Network IO, Access to device nodes. 
        Cgroups can also allocate resources to groups of processes. These resources include: CPU time, System memory, Network bandwidth. 
        For example, cgroups can limit a Google Chrome process to 30% total CPU usage or 1 GB of RAM. 
        Cgroup was used in Facebook to isolate allocation of resources of WDBs + WDL from main application. This provides resource guarantee of your application
      Top
        https://medium.com/@TheAnshuman/linux-commands-to-check-cpu-utilization-memory-and-disk-c9daf8a53be8 
        Host level metrics and also per process metrics 
        It provides real-time information about system performance, including CPU and memory utilization.
        Top/hTOP DOES NOT SUPPORT CGROUPS 
        FACEBOOK RELEASED BELOW as an interactive resource monitoring tool - https://www.youtube.com/watch?v=4PB-RaUt2OY&ab_channel=MetaOpenSource 
      Iostat
        https://www.atlantic.net/vps-hosting/how-to-check-linux-cpu-usage-or-utilization/ 
        Host level metrics and also per process metrics 
        This command in Linux is used to report and monitor system input/output (I/O) device statistics, including information about disk utilization and performance.
      Types of profiling techniques : 
        Sampling Profiling
          The profiler periodically "samples" the state of the program at set intervals (e.g., every 10 milliseconds).
          It takes a snapshot of the program's current state (e.g., which function is being executed or which memory areas are in use) and aggregates this data over time.
          Advantages: Low overhead, less intrusive to the application.
          Disadvantages: Less precise, as it only captures snapshots rather than continuous data.
        Instrumentation Profiling
          In this approach, the profiler actively logs data during each function call or memory allocation/deallocation.
          Advantages: Provides more detailed and precise data (e.g., exact execution times, specific memory usage).
          Disadvantages: Can introduce higher overhead due to the frequent logging of data, affecting the program's performance
        Event-Based Profiling:
          Configure the profiler to start only when specific conditions are met (e.g., high CPU utilization or memory pressure).
          Example: Use CloudWatch Alarms to trigger profiling when CPU usage exceeds 80%.
       
    Performance monitoring needs to be done at different levels 
      Example of Meta stack - https://atscaleconference.com/systemscale-ai-observability/
      ![Alt text](image-1.png)
      - Host level / bare metal telemetry
      - Application tracing - BPF tracing, Strobelight, Pytorch Profiler, Kineto, Demand level tracing like QPS
      - Unified dataset which combines host level and application tracing, has optimization opportunities from bad patterns, regressions
      - UI Dashboarding
      Host level / bare metal telemetry 
          Meta uses Dynalog for this - distributed telemetry daemon running on all Meta’s hosts. It fulfills several important roles:
            It collects system telemetry at the host/process level.
            It collects workload metadata and conducts resource accounting to the proper workload owner.
            It functions as a server for real-time telemetry queries to support use cases such as load balancing and autoscaling.
            It serves as the moderator of the telemetry tools on the host to interact with remote on-demand user requests.
          ![Alt text](image-3.png)
        One major challenge of AI observability comes from the heterogeneity of the fleet. As the computational demand increases with the complexity of AI workloads, the heterogeneity of the fleet increases with the deployment of various application-specific integrated circuits (ASICs), including the Meta Training and Inference Accelerator (MTIA), third-party vendor ASICs such as GPU, inference accelerators, and video encoder/transcoders. To address the challenge, the Dynolog collector module generalizes the metric collection and configuration through LibAsicMon, a platform agnostic observability system for AI accelerators.

      Application tracing - Kineto, Pytorch, Strobelight
        end-to-end PROFILING
          System performance is the result of interaction across different layers of the stack. At Meta we use various tools to understand the performance and interactions across these layers.

          For example, a workflow might make a call over the network to get some data, do some processing on the host (CPU), and call a Kernel on the device (GPU). To get a holistic view of system performance, we want to understand the time spent in the network call, CPU processing, queuing the call on the device, and the actual execution start and end times on the device. Using this data we can identify bottlenecks, understand inefficiencies, and identify opportunities for improvement.
          ![Alt text](image-2.png)
        KINETO
          Kineto is a CPU+GPU profiling library that provides access to timeline traces and hardware performance counters. It is built on top of Nvidia’s CUPTI. We rely on Kineto for information on kernel executions, memory operations, and utilization metrics for various hardware units. Kineto is open sourced and integrated into PyTorch Profiler. 
        PYTORCH PROFILER
          PyTorch Profiler is another tool we rely on for profiling our workload performance. PyTorch Profiler allows the collection of performance metrics during training and inference.
        STROBELIGHT
          Strobelight is a daemon that runs on all of Meta’s fleet hosts and acts as both a profiler and a profiler orchestration tool. Profiling can be triggered by certain events (such as an OOM event), runs on a pre-configured schedule or on-demand. Strobelight relies on eBPF (or simply BPF)  for many of its sub-profilers.  And we have recently added a BPF-based CPU -> GPU profiler to Strobelight’s profiler suite. 
        BPF 
          In a nutshell, BPF is Linux kernel technology that can run sandboxed programs in a privileged context such as the operating system kernel.
          GPUSNOOP
            Gpusnoop is a BPF-based profiler suite that Strobelight supports. It can hook into a variety of interesting GPU events such as cuda kernel launches, cuda sync events, and cuda memory events. It also supports profiling PyTorch memory events.
        python profiler
          pstats / line profiler - not good for production though
          event based vs statistical profiler 
            event based 
              - based on some event - exception, functional. Example cprofile, line_profiler above 
            statistical 
              - probe(search) call stack periodically. Example vmprof
        java profiler (https://digma.ai/9-best-java-profilers-to-use-in-2024/#types-of-java-profilers)
          - JProfiler, IntelliJ Profiler
            

      EXAMPLE FEATURES
        - AUTO-PROFILING AND ON-DEMAND PROFILING  
        

    BPF(Berkely packet filtering) profiling libraries to check performance bottleneck 
      nice video - https://www.youtube.com/watch?v=bGAVrtb_tFs&ab_channel=eBPF%26CiliumCommunity
      - its a bytecode and execution evironment which provides direct extension to kernel modules(no need to change OS kernel code) that we are using to create observability, networking, security and other software.
      then how do you get quick and easy BPF performance wins 
      - top commands : 
        (1. execsnoop - anything periodic running 2. opensnoop - any misconfigurations or file not found 3. tcplife - any unexpected TCP sessions, 4. ext4slower - any filesystem I/O slower than 10ms 5. biosnoop - Any unusual disk access patterns or outliers ) 
         - example of performance bottleneck as a perl process detected on Cassandra DB using biosnoop rather than iostat command https://www.youtube.com/watch?v=bGAVrtb_tFs&ab_channel=eBPF%26CiliumCommunity 
      - Its future is nice GUI. Example Netflix - ![Alt text](<Screenshot 2023-11-18 at 5.56.00 PM.png>)

    eBPF - https://www.youtube.com/watch?v=KhPrMW5Rbbc&t=57s&ab_channel=CNCF%5BCloudNativeComputingFoundation%5D
    Also Really great video : https://www.youtube.com/watch?v=eVsMkXDE_5I&ab_channel=F5DevCentral
      - language and runtime to extend kernal modules so that we do not need to modify the kernel
      - great substitute to avoid Istio service mesh for tracing which deploys heavy sidecar
      - used for L4 LB (Facebook), AWS / Google uses EKS which uses eBPF for tracing (kubectl trace, networking and secutiy), Smartphones 
      - Grafana will soon be integrated with eBPF for observability 

    How to collect a Function level stack trace 
      - using linux perf tool which is included in linux kernel  
          perf record -g --call-graph dwarf -p $your_process_id 
      - example, Facebook's xenon -> https://github.com/facebook/hhvm/wiki/Profiling


    How to collect CPU instructions - MIPS 
      https://stackoverflow.com/questions/50019857/benchmarking-how-to-count-number-of-instructions-sent-to-cpu-to-find-consumed 
      perf stat --all-user ./my_program 


    Examples of 
      - Demand efficiency 
        - adding cache at function level like "Memoize" by simply tracing amount of function calls on a request 
        - Promise cache by simply checking large number of concurrent requests to DB server (MySQL cassandra) for same data 
        - removing GKs 
        - removing dead ifs 
        - removing old frameworks (simple pattern matching)
        - unsued tables in MySQL, Dataware houses and their pipelines/models 
        - Inefficiency or infrequent DB queries 
        - Logging optimization 
          - synchronous to async logging
          - log rotation granularity based on application to avoid large files 
          - archive old logs to disk using typical linux fsync command (Forces all in-memory data of a file, along with its metadata)
          - avoid string formatting in logs if that log level is not even enabled 
      - Software efficiency
        - For horizontal software written in C, using Folly libraries rather than traditional ones (in memory json manipulation or string-formatting)
            - folly::small_vector over std::vector: Ideal for small collections, reducing heap allocations and improving cache locality.
            - folly::F14 Hash Tables: Highly efficient hash table implementations with superior performance compared to standard std::unordered_map.
            - folly::fbvector : A drop-in replacement for std::vector with improved memory management and performance characteristics.
            - String Manipulation: Folly includes several optimized string handling utilities (e.g., folly::fbstring), which are faster and more memory-efficient than std::string in many scenarios. fbstring uses small-string optimization to minimize allocations for short strings, which is particularly useful in text-heavy applications.
        - Logging and Debugging: Folly includes high-performance logging tools like folly::logging, which is optimized for low-latency, high-frequency logging scenarios. This helps reduce the overhead associated with logging in production.
            - Logging optimization 
              - synchronous to async logging
              - log rotation granularity based on application to avoid large files 
              - archive old logs to disk using typical linux fsync command (Forces all in-memory data of a file, along with its metadata)
              - avoid string formatting in logs if that log level is not even enabled  
        - ML ranking : vector optimizations with sparse matrix. 
        - Implementation of Async IO
        - using BPF (say biosnoop command )
        - Case study : Example of unexplained spikes latency and cost in EKS/ECS cluster 
          - Problem Statement : 
            1. The latency occurred intermittently and at random services.
            2. Metrics pointed to general delays in network communication, but no specific service or node showed clear patterns.
            3. The system handled thousands of requests per second, so capturing detailed traces was costly and impractical.
          - Root causes : 
            1. Application Level: Blocking I/O operations in Java applications limited scalability and throughput, exhausting thread pools under high load.
            2. Network Level: TCP retransmissions, kube-proxy inefficiencies, and misconfigured retry mechanisms compounded latency issues.
            3. Infrastructure Level: Saturated socket buffers and inefficient service routing worsened the bottleneck.
          - Details : https://docs.google.com/document/d/1c_B2-Jcz4tlmtyoOMsnX5hZzlMtP4LHJPYW6GZPJ8bY/edit?tab=t.0
      - Utilization efficiency 
        - Autoscaling using capacity modelling (throughput and stopper metrics)
        - unused host 
        - Kubernetes right sizing using AWS Karpenter - https://www.youtube.com/watch?v=FIBc8GkjFU0&ab_channel=ContainersfromtheCouch 
          - https://karpenter.sh/ 
          - Say if a new pod is needed say just needed instance size i3en.xl, 
            - Cluster Autoscaler reserves a new node (AWS instances) of fixed sixed in node group of i3en.12xl
            - Karpenter reserves a new node (AWS instances) of flexible size which is i3en.xl. Secondly, it dynamically resolves fragementation across instance sizes of same type.
              - Research on AWS Karpenter : move it across AWS instance families rather than just types AND also perform defragmentation across instance families using live migration.

      - HW efficiency
        - replace new hardware based on bottlneck resource
        - avoid double occupancy
        - degrade efficiency to solve unsellables 

    Regression detection algorithms 
      - Linear regression 
        https://www.youtube.com/watch?v=CtsRRUddV2s 
        - y = mx + c  
      - CUSUM
        cumulative sum of differences (say the threshold is +/- 1% CPU) 
        For performance regression of software, Facebook's FBDetect uses CUSUM algorithm for funcation, endpoint level regression (https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/240834238_864881024149380_9006597177607274562_n.pdf?_nc_cat=102&ccb=1-7&_nc_sid=e280be&_nc_ohc=BdBe3qV_7l4AX8fnuxL&_nc_ht=scontent-sjc3-1.xx&oh=00_AfBf-ilwsWINjklZzowIBAvQLW_6NHIc0pH-aW7I11gNLg&oe=655F2806)
        along with there are other challenges 
          - seasonaility (peak hour)
            - Fourier regression can help remove the seasonality 
          - spiky
            - Use butterworth lowpass filter to help with smoothing 
          - code pushes 
          - deduplicates
            - change points that are close in time and connected via call stack are grouped together
            - also nodes (functions) with largest number of exclusive stack traces are removed in greedy fashion (usually common function) and magnitude (samples) of remaining are again evualted till samples fall below the threshold
          - cost shift 
            - check the parent cost till the congifurable level - example depth of 3. However, also use the large stack trace removal in dedup #2
             explained above. 
                example common parent function have large stack trace - example 10^10 / 10^12 ~ 1 % cpu. Then its hardly going to change if the child regresses.
            - check if there shift in demand from one endpoint to another

      Load testing algorithms
        - using throughput and stopper metrics
          Throughput 
            - find the increasing metrics increasing with load. Its in fixed set (instructions, QPS, GPU hours, ALM)
            - detected using linear regression 
          Stopper
            - find the metrics increasing and impacting the throughput above. It has wide ranges (CPU and Mem utilization, Mem bandwidth, disk I/O, network I/O, error rate, latency, revenue)
            - detected using linear regression 

          In both settings, the limit of a machine detoriating is 
            - health check 
            - 5xx error limit 
            - latency (most online systems have upper limit of 500 ms). It does not apply to offline systems

      Example of performance wins 
        - Hulu
          - Convert Java microservices to Go 
        - Meta 
          - Demand efficiency 
            - through memoization
            - promise cache
            - long tail but impactful - unused GKs, unused DBs and their jobs, infrequent query 
          - Software efficiency 
            - using folly libraries 
            - using BPF insights
          - Utilization efficiency
            - load testing + capacity automation due to 
                - Lack of clarity on service bottleneck metrics
                - Uncertainty regarding the appropriate capacity for Disaster Recovery Buffer (DR) and Growth Buffer
                - Double occupancy (DO) issue during host replacement, where old and new hosts serve traffic simultaneously.
                - Regional Fluidity (RF) issue caused due to uneven capacity distribution across regions.
                - Autoscaling Stateful services poses reliability and performance challenges.
        - Amazon
          - Demand efficiency ( all end customers)
            - add eBPF for cloud cost optimizer
            - Right sizing :  add seamless instance family migration to AWS Karpenter feature to save costs 
          - Platform efficiemncy 
            - Software 
              - Redshift and Elasticseach
                - set quotas
                - from Code guru profiler stats
                - async io
                - biosnoop to avoid unwanted disk access
                - logging optimization 
                - memoization 
                - promise caching
                - dead if
                - case study of eBPF observability
                  - https://docs.google.com/document/d/1c_B2-Jcz4tlmtyoOMsnX5hZzlMtP4LHJPYW6GZPJ8bY/edit?tab=t.0
            - Utilization ( no problems on autoscaling )
              - defragmentation using Live migration
            - HW efficiency
              - degrade efficiency by smartly select high risk hosts based on temperature, age, power supply, DCO bandwidth and capacity to evacuate 
          

            Solution for Autoscaling
              - Intially load testing is run for a week. After 3 days, it can halt if throughput and stopper metrics patterns are consistent.
              - Then it runs periodic load testing every 4 (config) hours to help determine throughput and stopper metrics 
                - use throughput metrics to get total instructions at peak hour for the service at lowest granalarity - region level + machine type
                - calculate throughput for DR bufer, organic growth and spike buffer from throughput metric 
                - get the estimate of # of machines needed
                - submit this data to autoscaling job 
                How load testing is done ? 
                Initially, it runs for a week (configurable) or ends early if peek hour or steady traffic is determined. After that it runs every 4 hours and submit its stats to autoscaling.
                  - runs every 4 hours by default but runs more frequently if the stopper metrics vary 
                  - tags the selected hosts on every region at machine type 
                  - tags the service router to gradually route the traffic to those selected hosts gradually based on configurable load
                  - marks the selected hosts to do advanced profiling 
                  - waits for machine to drain 
                    - health check
                    - error rate
                    - response time for online services. 

                What if load testing results (throughput + stopper) does not show consistent patterns throughout the week. This was 45% of the case ? 
                  This usually happens with different demand sources. One thing observed was with different demand sources, throughput metric was consistent but stopper metrics could change. Example, call from FB feed to ranking platform is CPU bound but from Insta feed to ranking platform is Mem bound.
                  SOLUTION : 
                  
                  1.  fewer demand sources 
                      - perform sharding of services by demand sources after load testing 
                      - Set quotas accordingly. ( say max throughput is 15 ins, then FB can max have 5 ins and insta can have max 10 ins )
                      - for hard limit quotas, inform the teams to use promise cache
                      Pros : great utilization 
                      Cons : cubersome to manage for 100+ demand sources especially shared endpoints like WWW

                  2. multiple 100+ demand sources : 
                      - This is hard but performing below steps helped 
                        1. group demand sources based on throughput and stopper metrics
                        2. integrate ML to undestand anamolies on existing groups and perform re-group using clustering algorithm
                      Pros : easier to manage
                      Cons : too much resource overhead on load testing because of time-based and grouping
                

                 QUESTION : 
                       
                     What if Demand sources limit/quotas not set (unbounded)
                      - For viral content, we use promise cache to avoid multiple requests for same resource.
                      - Service level rate limiter with spike buffer

                   
                How autoscaling is done ? 
              
                  - while scaling down, 
                    - it prioritizes intersections which has low traffic by first tagging the service router to not route traffic to selected hosts (machine per 1% of throughput)
                    - checks the impact on throughput and stopper metrics 
                    - decomission the hosts
                  - while scaling up.
                    - similar strategy but to save costs further it tries to consume capacity from released flexible pool of machines rather than service selected machine type 

                What about autoscaling on stateful services ? 
                  - this is hard and manual often and needs to be carefully done if automation 
                    - region + machine type granularity helps 
                    - make sure you deroute the traffic very slowly

          
              

      

  Cache benchmarking

    Azure Redis Cache : 
        53 GB, with 99.9% availability (https://github.com/Huachao/azure-content/blob/master/articles/redis-cache/cache-faq.md)
        230 us (https://redis.io/docs/management/optimization/latency/)
        Redis QPS - 50K https://redis.io/docs/management/optimization/benchmarks/
        1 million TPS - https://bytebytego.com/courses/system-design-interview/digital-wallet 
        Redis also privides sorted set data structure. Our leaderboard use case maps perfectly to sorted sets. Internally, a sorted set is implemented by two data structures: a hash table and a skip list [1]. The hash table maps users to scores and the skip list maps scores to users. In sorted sets, users are sorted by scores. (https://bytebytego.com/courses/system-design-interview/real-time-gaming-leaderboard)
        Sorted set is used in Rate limiter sliding window design which fetches based on timestamp range and avoids race conditions since redis sis single threaded.
        Redis cluster also supports sharding with hash slot ( different from consistent hashing)
        Redis cluster uses master-replica model
        Redis Cluster does not guarantee strong consistency in terms of CAP. But has strict isolation level - seriazable
        Max Size of each redis instance from AWS : 500 GB https://aws.amazon.com/elasticache/pricing/?nc=sn&loc=4 
        Sorted Set - https://redis.io/docs/data-types/sorted-sets/
        How redis works 
          https://www.youtube.com/watch?v=5TRFpFBccQM
        Redis is single threaded and supports serializability isolation level (serial execution), epi pg. 253
        Persistence 
          - you can configure to be persistent ~ 1 min snapshotting https://redis.io/docs/management/persistence/

        Promise Cache to resolve thundering herd - VERY IMPORTANT
          https://redis.com/blog/caches-promises-locks/
          - basically if say 1 M clients requests for same item and there is cache miss. So instead of forwarding all requests to DB, only one request is made to DB to populate the cache. Rest all the requests are given promises. 
          - With redis, you can create the lock:my_value using NX option. This option ensures only one request will be able to set the key. 
          - So if a request is able to get the this lock - lock:my_value, then its your job to fetch the value from SOT(source of truth) and set in DB. Once you set the key, you can notify on the pub/sub channel for lock:my_value
          - All other requests which  are not able to get this lock, they subscribe to notif:foo pub sub channel and gets notified about the requested value.

    CDN
        https://www.youtube.com/watch?v=RI9np1LWzqw
        - brings content closer to the user to increase the performance
        - CDN server locations called point of presence PoPs. Server inside POPs are called edge server
        - DNS based routing and anycast used to send the content to the user
        - reduce bandwidth requirements of origin server
        - can minify js files, transform image from old format to modern format
        - Because of its reverse proxy capabilities, all TLS connection terminate at edge server(CDN) thereby reducing TLS handshake overhead. One of the reason why modern servers cache dynamic content on CDN
        - Security : huge network capacity to avoid DDOS attacks. CDN built on anycast network to diffuse ddos traffic
        - Improves availability by having copies available at many locations 

        Pricing 
          https://cloud.google.com/cdn/pricing 
          - remember CDN charges for egress (0.02$ per GiB), cache fill (same charge - 0.01$ per GiB) and 0.0075 per 10K req


      Memcache Vs Redis
        https://www.baeldung.com/memcached-vs-redis#:~:text=Architecture,can%20perform%20better%20than%20Redis. 

        Memcached is a distributed memory caching system designed for ease of use and simplicity and is well-suited as a cache or a session store.
        Redis is an in-memory data structure store that offers a rich set of features. It is useful as a cache, database, message broker (pub sub), and queue.

        Memcache does not support persistence 
        Redis supports persistence (https://redis.com/redis-enterprise/technology/durable-redis/)

        Memcached stores key-value pairs as a String and has a 1MB size limit per value. 
        However, Redis also supports other data structures like list, set, and hash, and can store values of up to 512MB in size.

        Memcached doesn't support transactions, although its operations are atomic.
        Redis provides out-of-the-box support for transactions to execute commands. We can start the transaction using the MULTI command. Then, we can use the EXEC command for the execution of the following subsequent commands. Finally, Redis provides the WATCH command for the conditional execution of the transaction. 
        One of the reason why Redis is a good choice for applications which need atomic transactions and serializable isolation - Rate 
        You can also have LUA script support in Redis which executes atomically

        Memcached doesn't support publish/subscribe messaging out-of-the-box.
        Redis, on the other hand, provides functionality to publish and subscribe to messages using pub/sub message queues.

        Geospatial support is useful for implementing location-based features for our applications. Unlike Memcached, Redis comes with special commands to manage real-time geospatial data. For instance, the GEODIST command calculates the distance between two geospatial entries. Likewise, the GEORADIUS command returns all the entries within the radius provided. Additionally, we can use Spring Data Redis to enable Redis geospatial support in a Java application.

        Memcached implements a multi-threaded architecture by utilizing multiple cores. Therefore, for storing larger datasets, Memcached can perform better than Redis.
        Redis uses a single core and shows better performance than Memcached in storing small datasets when measured in terms of cores.

        We can certainly conclude that Memcached is a solid choice for solving simple caching problems. However, generally speaking, Redis outperforms Memcached by offering richer functionality and various features that are promising for complex use-cases.



  SQL DB and benchmarking - Relational
    1. MySQL benchmarking
          48 cores, single node
          4 x # of cores ~ 200
            200 reads ~ 200 us
            200 writes ~ 1 ms (writes 4x slower)
          load test
            10K reads ~ 10-20 ms 
            10K writes ~ 100 ms (writes 4x slower)

          - Indexed Read
              a. with primary key reads which is already indexed
                (https://dev.mysql.com/blog-archive/mysql-connection-handling-and-scaling/)
                Recommend 4 * number of cores parallel for ~ 200 us based on simple primary key look ups. For example, 120 (~4 * 32) parellel requests for 32 cores will take 200 us
                1K requests on 32 will take ~ 1ms for simply select query
          
          - Write
             http://minervadb.com/wp-content/uploads/2020/10/MySQL-8-Performance-Benchmarking-on-Amazon-EC2.pdf 
            impact of write performance on indexing
              4 times slower : https://logicalread.com/impact-of-adding-mysql-indexes-mc12/#.Y__7d-zMLuU 

          - General benchmarking
            Today, a relational database running on a typical data center node can support a few thousand transactions per second.


          - Max size set by MySQL
            256 TB (https://dev.mysql.com/doc/mysql-reslimits-excerpt/8.0/en/table-size-limit.html#:~:text=You%20are%20using%20a%20MyISAM,2567%20%E2%88%92%201%20bytes)
            AWS limit : 64 TB storage and 24 TB of RAM

          Some facts
          - MySQL client <-> server is socket based connection. Check why ?
          - Max connection limit for MySQL = 100 K
          - per connection -> one user thread
          - size of user thread depends on THD connection data structure which is per connection ~ 10 MB 
          - max THD ~ 10K 
          - So recommended min size of your MySQL = 10 MB * 10 K = 100 GB ??

          - MySQL sorting (https://www.pankajtanwar.in/blog/what-is-the-sorting-algorithm-behind-order-by-query-in-mysql)
              External merge sort (quick sort + merge sort) if data doesn’t fits into the memory
              Quick sort, if data fits into the memory and we want all of it
              Heap sort, if data fits into the memory but we are using LIMIT to fetch only some results
              Index lookup (not exactly a sorting algorithm, just a pre-calculated binary tree)

          - MySQL query cache (https://docs.oracle.com/cd/E17952_01/mysql-5.1-en/query-cache.html)
              As of MySQL 5.1.63, the query cache is not supported for partitioned tables, and is automatically disabled for queries involving partitioned tables. The query cache cannot be enabled for such queries. 

          - MySQL sharding (https://stackoverflow.com/questions/1610887/how-to-partition-mysql-across-multiple-servers)
              this is different from mysql partitioning (https://vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions/)
          
          - Moreover, MySQL involves no standard implementation for sharding. (https://kinsta.com/blog/mongodb-vs-mysql/)

          - MySQL indexing 
              https://www.youtube.com/watch?v=YuRO9-rOgv4 using B tree
              B tree performance : https://www.youtube.com/watch?v=FgWbADOG44s 
              Its inefficient as compared to B tree for writes : https://www.youtube.com/watch?v=I6jB0nM9SKU 

          - MySQL replica sync
              https://serverfault.com/questions/30605/how-fast-is-mysql-replication
              MySQL replication happens as close to real-time as possible (AWS has <100 ms SLA), as limited by disk and network I/O. The slaves open a socket to the master, which is kept open. When a transaction occurs on the master, it gets recorded in the binlog (MySQLs replication log), and is simply replayed on the slave(s). If the socket between master and slave is interrupted, the binlog is replayed for the slave upon the next successful connection.


          - Multi-region failover time for AWS RDS : 60-100s 
              https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html

          - Replication lag for AWS RDS : < 100 ms 
              https://www.bluematador.com/docs/troubleshooting/rds-replica-lag#:~:text=Replication%20lag%20measures%20how%20far,than%20100ms%20of%20replication%20lag. 

          - we could simply deploy a simple relational database on each data node. SQLite [19] is a good choice here. It is a file-based relational database with a solid reputation. https://bytebytego.com/courses/system-design-interview/s3-like-object-storage 


          - Practical
              how MySQL gurantees ACID : https://www.youtube.com/watch?v=0OYFsJ1-1YA
              Nice theoretical : https://www.youtube.com/watch?v=clPPKgYJC10 
               

    2. CockroachDB
      https://github.com/cockroachdb/cockroach/blob/master/docs/design.md 
        CockroachDB is a distributed SQL database. The primary design goals are scalability, strong consistency and survivability (hence the name). CockroachDB aims to tolerate disk, machine, rack, and even datacenter failures with minimal latency disruption and no manual intervention. CockroachDB nodes are symmetric; a design goal is homogeneous deployment (one binary) with minimal configuration and no required external dependencies. 
        Distributed transaction : https://www.youtube.com/watch?v=OJySfiMKXLs&t=1271s 
       - uses RocksDB for its key value storage
       -  Nice video explaining how concurrency is handle on distributed transactions : https://www.youtube.com/watch?v=iD_Yk5AhNGc 
          1. So atomicity is guaranteed with Raft
            Raft : https://thesecretlivesofdata.com/raft/ 
              - Raft handles atomic writes and consistent reads
              
          How about Isolation 
          2. SSI by default in distributed databases - how?
              - MVCC - multiversion concurrency control -  https://youtu.be/iD_Yk5AhNGc?t=1783
                - uses timestamp as form of versioning but depends on time synchronization which is a challenges in distributed systems. Even with NTP, there is a challenge. 

        Example of shifting to CockroachDB from Cassandra 
          https://www.cockroachlabs.com/blog/cassandra-to-cockroachdb/#more-on-cockroachdb-vs-cassandra
          - needed relational data modelling
          - SQL query flexibility

        Benchmarking : 
        Benchmark
        - Single row reads and writes
          https://www.cockroachlabs.com/docs/v22.2/performance.html
          across availability zones - 3 node cluster, c5d.9xlarge (36v CPUs, 72 G RAM)
            100K-200K QPS
            single-row reads in 50-100 ms and processes 
            single-row writes in 50-100 ms
        - Batch
          https://www.cockroachlabs.com/docs/v22.2/performance-benchmarking-with-tpcc-small   
            - p95 in 250-300 ms
              



    NoSQL value DB and benchmarking
        1. Timeseries : OpenTSDB / InfluxDB

             Why Time-series DB and not other key-values like Cassandra, MongoDB ???? 
                - in built support for time-based queries example using Time-structured merge tree
                - in built support for sampling through retention config

                Proof : Thus performance is 100x faster
                  https://www.influxdata.com/blog/influxdb-vs-cassandra-time-series/ 
                     InfluxDB 1.8.10 and Cassandra v4.0.5, 
                  - InfluxDB outperformed Cassandra in all three tests with 5x greater write throughput, while using 2.4x less disk space, and delivering up to 100x faster response times for tested queries. Thus, can handle spiky reads

            Different time-series DB in the market and its comparison
                https://www.youtube.com/watch?v=W-ouPw944CM
                Influx DB
                  - private and non open source
                  - uses push based model
                  
                Prometheus (also used in metrics monitoring)
                  - uses pull based model and thus samples heavily. May not be ideal for monitoring based on revenue but good for system stats.

                Datadog (also used in metrics monitoring)
                  - free+paid version 
                  - offers both pull-based and push-based models for data collection. 
                  - Datadog pulls data for monitoring by: 
                    Running a customizable Agent check
                    Scrubbing available endpoints
                    Looking for exposed metrics

                  - Datadog also pulls in: 
                    CloudWatch metrics and events
                    Tags from your cloud provider


                Timescale DB based on Postgres
                  - built on top of postgres so that you can run MySQL
                  - not suitable for scale and does not have AWS RDS support 


            Influx DB benchmarking from bytebytego : 
              8 cores and 32GB RAM can handle over 250,000 writes per second, greater than 1 M series of storage, 25 burst read QPS 
              (https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system)

              Performance optimization
                https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system
                  According to a research paper published by Facebook [24], at least 85% of all queries to the operational data store were for data collected in the past 26 hours. If we use a time-series database that harnesses this property, it could have a significant impact on overall system performance. If you are interested in the design of the storage engine, please refer to the design document of the Influx DB storage engine [26].
                  - Storage : Store only delta for timestamps
                  - Storage and querying : Downsampling
                  - use cold storage for old data

            

        2. Object storage / cold storage benchmarking
            S3 Standard is designed for 99.99% data availability and durability of 99.999999999% of objects across multiple Availability Zones in a given year. AWS S3 provides a great performance. It automatically scales to high request rates, with a very low latency of 100–200 milliseconds.Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket (https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)
            Another option is to store the data in Amazon S3 using one of the columnar data formats like ORC [5], Parquet [6], or AVRO [7]. We could put a cap on the size of each file (say, 10GB) and the stream processor responsible for writing the raw data could handle the file rotation when the size cap is reached. (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation)


        3. Cassandra

              100s video : https://www.youtube.com/watch?v=ziq7FUKpCS8
              Slower version of 100s : https://www.youtube.com/watch?v=YjYWsN1vek8 
              
             
              How Cassandra data is  stored in disk :  
                https://www.baeldung.com/cassandra-storage-engine (LSM Tree + SS Table + Sparse Index)
               
                
              Cassandra as Columnar databse :
                https://stackoverflow.com/questions/13010225/why-many-refer-to-cassandra-as-a-column-oriented-database
                http://dbmsmusings.blogspot.com/2010/03/distinguishing-two-major-types-of_29.html
                https://www.baeldung.com/cassandra-column-family-data-model 
                  - Its not a direct column based store but its based on column family which stores data row wise
                
                Cassandra column family : 
                  https://www.scylladb.com/glossary/cassandra-column-family/ 
                  Basically you can create multiple column families with different columns and each will have its own LSM tree + SS Table
                  Practical : 
                  Create multiple column families in Cassandra within a key space
                  https://www.dbrnd.com/2016/05/nosql-create-your-first-cassandra-column-family-table/  

              Great read : Use of parition key + clustering key storages in Cassandra
                http://distributeddatastore.blogspot.com/2020/03/cassandra-new-sstable-storage-format.html
                https://cassandra.apache.org/doc/latest/cassandra/architecture/storage_engine.html
                - parition key to assign the nodes and also the key component of SS Table. So there inside every node, there is an index.db in form of SSTable storing partition key and its position of data in data.db
                - At the position of parition key in data.db, clustering key is used as sorted data within that partition key
                - Remember a partition key could be in more than 1 SSTable due to subsequent insert/update

              How updates are handled in Cassandra 
              https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlWriteUpdate.html 
              Periodically, the rows stored in memory are streamed to disk into structures called SSTables. At certain intervals, Cassandra compacts smaller SSTables into larger SSTables. If Cassandra encounters two or more versions of the same row during this process, Cassandra only writes the most recent version to the new SSTable. After compaction, Cassandra drops the original SSTables, deleting the outdated rows.
              
              Cassandra indexing
                Paritioning : https://www.youtube.com/watch?v=Np5RJpCiCzM
                Clustering used as sort key : https://www.youtube.com/watch?v=OCakxrzwuU4 
                  Remember data in cassandra on disk supports SSTable which is sorted string table.
                  So searching could be done in logN times like relational or mongoDB
                  compaction could be ddone using merge sort

              Practical 
                    Cassandra indexing (partition + clustering which is sort key) : https://www.youtube.com/watch?v=S9rmf4X7E_E

              Garbage collection
                4.0 uses Z GC instead of G1 GC (java8 to java 16)

              Cassandra and map-reduce
                https://subscription.packtpub.com/book/data/9781787127296/1/ch01lvl1sec5/mapreduce-and-spark
                Unlike MongoDB, Cassandra does not offer built-in MapReduce capabilities. But it can be integrated with Hadoop in order to perform MapReduce operations across Cassandra data sets, or Spark for real-time data analysis. 
                Datastax is Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra (https://github.com/datastax/spark-cassandra-connector)

            

              Cassandra performance on writes + reads on indexes 
                - LSM tree + SS tables
                Great video : https://www.youtube.com/watch?v=I6jB0nM9SKU 
                Good read : https://rahulpradeep.medium.com/sstables-and-lsm-trees-5ba6c5529325 
                B Tree vs LSM tree technique - https://www.youtube.com/watch?v=4z7-SrDiBoU

              Consistent hashing
                If we add a new node to the cluster, it automatically rebalances the virtual nodes among all nodes. No manual resharding is required. See Cassandra’s official documentation for more details [22]. - https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 

              Consencus
                Paxos - https://cassandra.apache.org/_/blog/Cassandra-4.1-is-here.html 
                  Some examples of use cases are selling seats to a concert and where an INSERT or UPDATE operation must be unique, such as for a customer ID. Historically, Lightweight Transactions (LWTs) have suffered from poor performance, particularly in a WAN setting and under contention. As LWTs used a read-before-write approach, LWTs had more round trips which hurt performance.
                  Recognizing these shortcomings, Paxos was optimized, improving LWT performance by 50%, improving latency, and halving the number of required round-trip to achieve consensus. Linearizability is now guaranteed across range movements in-line with what you would expect from a database with strong consistency.

              Challenges with Cassandra
                1. Data loss or durability challenge : LWW (last write wins) is common issue with multi-leader replication and leaderless databases like Cassandra - Pg 292 DDIA. With Paxos, you can achieve Linearizability but face issue with LWW if clocks are not synchronized.
                2. Cassandra uses consistent hashing and so not good for range queries
                
              Benchmarking
                - Expensive hardware https://www.scylladb.com/2021/08/19/cassandra-4-0-vs-cassandra-3-11-comparing-performance/
                  i3.4xlarge (16 vCPUs, 122 GB), 3 node cluster
                   Read : 100 K ops/s
                   Write : 100 K ops/s
                   Read Latency : ~10 ms
                   Write Latency : ~10 ms

        4. MongoDB
              - How Mongo DB search works : https://www.youtube.com/watch?v=tSgPhxZdhLk&t=96s 
                  using index in B tree
              - MongoDB supports range-based or hash-based sharding - https://kinsta.com/blog/mongodb-vs-mysql/
              - For aggregations, Instead of map-reduce, you should use an aggregation pipeline. Aggregation pipelines provide better performance and usability than map-reduce. https://www.mongodb.com/docs/manual/core/map-reduce/
              - How Mongo DB sharding works : https://kinsta.com/blog/mongodb-vs-mysql/ 
              - Practical
                  set up mongo based sharding https://www.youtube.com/watch?v=mjSNKjTzeao 
                  mongoDB gridFS https://www.youtube.com/watch?v=mZE3aBdr010 

        6. Dynamo DB
            Nice intro in https://bytebytego.com/courses/system-design-interview/real-time-gaming-leaderboard

            DynamoDB is a fully managed NoSQL database that offers reliable performance and great scalability. To allow efficient access to data with attributes other than the primary key, we can leverage global secondary indexes [16] in DynamoDB. A global secondary index contains a selection of attributes from the parent table, but they are organized using a different primary key. 
            So Every global secondary index must have a partition key, and can have an optional sort key. The index key schema can be different from the base table schema. 

            DynamoDB splits data across multiple nodes using consistent hashing.

            uses quorum consistency (type of strong consistency)
            uses read-repair and anti-entropy
            supports serializable transactions


        7. S3

        8. Neo4j
            https://www.youtube.com/watch?v=GM9bB4ytGao&ab_channel=Neo4j 
            Data is already in the form of relationship 
            benchmark
             -> depth of 2 ~ 10 ms
             -> depth of 3 ~ 150ms
             -> depth of 5 ~ 2s vs 1 hour in MySQL

        9. RocksDB
           https://rocksdb.org/
          embeddable persistent key-value store for fast storage
          - high performance due to LSM implementations
          - RocksDB is optimized for fast, low latency storage such as flash drives and high-speed disk drives. RocksDB exploits the full potential of high read/write rates offered by flash or RAM.
          RocksDB                                 vs                    Redis 
          optimized for SSDs(flash), disks                              in-memory db
          more storage larger than RAM                                  storage size restricted due to RAM (in-memory)
          performance close to its SSD/Disk (~1-10ms)                   best performance (200 us with 50K transaction on 53 G)

        10. Apache Flink - Stream processing system
          https://www.youtube.com/watch?v=ZU1r7uEAO7o 
           - Vs Batch processing, it processes the events immediately locally storing the state
           https://www.youtube.com/watch?v=_G-hQfT02BA 
           - Achive fault tolerant with periodic snapshots to S3/HDFS storage which serve as a checkpoint. 
           - Also guarantees exactly-once processing with checkpoint markers 
           https://www.youtube.com/watch?v=LeJG-M9HnCI
           - how Flink job manager deploys job on task managers

          How Flink handles backpressure (with practical)? 
            https://www.ververica.com/blog/how-flink-handles-backpressure 

          (Very important) Deep dive : How Flink network stack works (buffer) to support back-pressure ? 
            https://flink.apache.org/2019/06/05/a-deep-dive-into-flinks-network-stack/  
            - They used credit based flow control. Basically sender has knowledge about receivers capacity and receiver also knows about senders backlog.
            - Design of Flink
                          Task manager 1      (Netty Server in Java for NIO)   ------(TCP Connection)--------              Task Manager 2


                    |---------------------------------------------------------                                                               
                    |                                                         |
                    |     - running subtask 1 (knows subtask 3,4 capacity).   |
                    |              - buffer for subtask 3                     |
                    |              - buffer for subtask 4                     |
                    |                                                         |
                    |                                                         |
                    |    - running subtask 2 (knows subtask 3,4 capacity).    |
                    |               - buffer for subtask 3                    |
                    |              - buffer for subtask 4                     |
                    |                                                         |
                    |                                                         |
                    |---------------------------------------------------------|
                    
            - When are messages send by Netty server ? (related to coding question)
                opt 1. When buffer is full but can take some time to fill the buffer 
                opt 2. When there is buffer timeout but have overhead of maintaining buffer indexes (leetcode question)
                opt 3. When certain event happens like checkpoint barrier
                    
                  

          Kafka Stream vs Flink ? 
            https://www.openlogic.com/blog/apache-flink-vs-kafka-streams#
            Kafka Streams : Kafka Streams API has an embeddable library that eliminates the need for building clusters and allows it to be seamlessly integrated into your existing toolstack. Developers can focus on their application without needing to worry about deployment. Plus, teams get all the benefits of Kafka including, failover, scalability, and security.

            Flink : It was the first open source framework that could deliver on throughput at scale (up to tens of millions of events per second), sub-second latency as low as 10s of milliseconds, and accurate results. Flink runs self-contained streams in a cluster model that can be deployed using resource managers or standalone. 

            Difference : 
              - Set up and Maintenance (Kafka wins) :
                -  The main difference between Flink vs. Kafka Streams is that Flink is a data processing framework that uses a cluster model, whereas the Kafka Streams API is an embeddable library that eliminates the need for building clusters.
              - Bounded + Unbounded (Flink wins) : 
                - Kafka Streams only supports unbounded streams (streams with a start but no defined end). Flink, on the other hand, supports both bounded streams (defined start and end) and unbounded streams.
              - Data sources support (Flink wins) : 
                - Where Flink can ingest data from multiple sources, like external files or other message queues, Kafka Streams are shackled to Kafka topics as the source. Both support multiple data types for sink/output.

          Flink vs Spark ? 
            https://www.youtube.com/watch?v=VAwtpa9EHf0 
            - Performance (Flink wins) 
              - Spark can be configured to be near real time processing but has slow performance
              - Flink is made for real-time processing with 2x performance as compared to Spark
            - Memory (Flink wins)
              - Spark faces OOM due to its in-memory nodes wherease Flink can take help of disks
              
        11. Google Cloud Spanner
            https://research.google/pubs/pub39966/
            Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions.
            https://en.wikipedia.org/wiki/Spanner_(database)
            Spanner is a distributed SQL database management and storage service developed by Google.[1] It provides features such as global transactions, strongly consistent reads, and automatic multi-site replication and failover. Spanner is used in Google F1, the database for its advertising business Google Ads.[2]

  Bigdata systems today
    Understand Hadoop ecosystem
      https://www.youtube.com/watch?v=YzAAhbmSt_k
      MapReduce - allows parallel processing and merging
      HDFS - Highly distributed file system based on Google file system (GFS)
      YARN - Job scheduling and resource management
      Common utilitieis
    https://ahana.io/learn/comparisons/hive-vs-presto-vs-spark/ 

    HBase
      It is nothing but a hadoop database and its no SQL. Great layered diagram in https://www.educba.com/hadoop-vs-hbase/ 

    Hive
      https://www.youtube.com/watch?v=taTfW2kXSoE 
      - Hive creates SQL query layer which in turn uses map reduce to query big tables stored in HDFS. This helps SQL experts query big data in HDFS directly in SQL and does not need complicated logic of map reduce.
      - Hive can also use Spark as its engine instead of Map Reduce
      - Hive is not a database

      Hive architecture - https://www.youtube.com/watch?v=rr17cbPGWGA&t=270s
        Hive is optimized for query throughput and thus supports large data aggregations 

    Presto
      https://www.youtube.com/watch?v=hEFsHQ_kJR8 
       - similar to Hive, but has amazing capability of query multiple databases - noSQL + SQL ones including Hive. It is optimized for latency and thus used for interactive queries or quick data exploration. Facebook uses Presto.
      Nice clarification about Presto
        https://ahana.io/learn/presto/
        - Presto does not use Map reduce 
        - Presto is not a database


      How twitter uses presto
        https://www.youtube.com/watch?v=WaooVcS75xA

    Dataflow engines - Spark, Flink pg 421 
      - does not have to perform strict map-reduce operations

    Spark
      https://www.ibm.com/cloud/blog/hadoop-vs-spark
        Spark is a Hadoop enhancement to MapReduce. The primary difference between Spark and MapReduce is that Spark processes and retains data in memory for subsequent steps, whereas MapReduce processes data on disk. As a result, for smaller workloads, Spark’s data processing speeds are up to 100x faster than MapReduce.

        Furthermore, as opposed to the two-stage execution process in MapReduce, Spark creates a Directed Acyclic Graph (DAG) to schedule tasks and the orchestration of nodes across the Hadoop cluster. This task-tracking process enables fault tolerance, which reapplies recorded operations to data from a previous state.

        Also optimizes the allocation of task to operators based on data locality - pg 422 DDIA

        Working of DAG Optimizer in Spark
        https://data-flair.training/blogs/dag-in-apache-spark/
          We optimize the DAG in Apache Spark by rearranging and combining operators wherever possible. For, example if we submit a spark job which has a map() operation followed by a filter operation. The DAG Optimizer will rearrange the order of these operators since filtering will reduce the number of records to undergo map operation.

        Then why DAG is better than MapReduce ? 
          BEST - explanation  
          https://www.quora.com/What-are-the-advantages-of-DAG-directed-acyclic-graph-execution-of-big-data-algorithms-over-MapReduce-I-know-that-Apache-Spark-Storm-and-Tez-use-the-DAG-execution-model-over-MapReduce-Why-Are-there-any-disadvantages 

        Spark architecture
          - components include spark SQL, streaming, machine learning, 
          - follows master-slave architecture ( Driver which is master -> cluster manager -> worker node with cache)
          - Driver uses its Sparkcontext and connect with cluster manager to get resource info and generates RDD datastructures. Then it convert core transformations and actions into DAG.
          - Driver schedules the task on workers and resources are allocated to tasks by cluster manager.
            Spark architecture in 3 mins : https://www.youtube.com/watch?v=rJFg2i_auAg&ab_channel=BigDataElearning 

          - Some Cluster manager used by Spark
              https://data-flair.training/blogs/apache-spark-cluster-managers-tutorial/
              - Apache Spark Standalone Cluster Manager 
                  - simple allocates resources based on the core. By default, an application will grab all the cores in the cluster.
              - Apache Mesos (popular)
                  - Mesos handles the workload in distributed environment by dynamic resource sharing and isolation. It is healthful for deployment and management of applications in large-scale cluster environments. Apache Mesos clubs together the existing resource of the machines/nodes in a cluster. From this, a variety of workloads may use. This is node abstraction, thus it decreases an overhead of allocating a specific machine for different workloads. It is resource management platform for Hadoop and Big Data cluster.
                  In some way, Apache Mesos is the reverse of virtualization. This is because in virtualization one physical resource divides into many virtual resources. While in Mesos many physical resources are club into a single virtual resource. 
              - Hadoop YARN

              What to use
              Hence, in this Apache Spark Cluster Managers tutorial, we can say Standalone mode is easy to set up among all. It will provide almost all the same features as the other cluster managers.Moreover, to use richer resource scheduling capabilities (e.g. queues), both YARN and Mesos provide these features



        Other DAG resource manager example - Apache Airflow
            https://www.youtube.com/watch?v=mtJHMdoi_Gg 

        Parquet
          https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
          Parquet is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. 

        

  Attempt to standardized DB selection criteria 
      1. SQL vs noSQL 


          - Draw Data model first

           http://tinyurl.com/26jf4dtq (SQL vs DynamoDB)
          - SQL :                       Read heavy + B tree  + Fixed schema + Relationship + no Sharding ( hard to shared but arguable with AWS Aurora)
          - no SQL like DynamoDB :      Write less + B tree  + Schema flexbility 
          - no SQL like CassandraDB :   Write heavy + LSM tree + Schema flexibility


      2.  MySQL vs MongoDB

      3.  MongoDB vs Cassandra
            https://www.youtube.com/watch?v=3z2EzILA3Rk 

                Cassandra                          vs               MongoDB 
            column data model                                   json document data model
            no config server(ring structure)                    mongo master, slave and mongo config servers for replication
            high availability due to multiple masters           master -> slave takes time
            accept writes in parallel                           write capacity is limited since they just go to master
            similar to SQL                                        based on json formatting

            When mongo DB
              query also based on secondary indexing and flexible querying
              built in data aggregation framework like aggregation pipeline
              read-heavy workload

            When Cassandra 
                query based on primary key indexing 
                100% uptime guarantee due to replication and inconsistency resolution using Paxos(similar to raft)
                high write speed
                language support for SQL
                write-heavy, read-heavy workload
                eventual consistency (as per bytebytego - https://bytebytego.com/courses/system-design-interview/design-a-key-value-store )

     

      Open questions
            1. Why is MySQL better for read heavy low write workload ? answered above
            2. MySQL is better for ACID systems and structured ? How ? answered above
            2. How MySQL syncs with its replicas ? answered above
            2. Why is Cassandra (noSQL) better for read heavy write heavy workload ? answered above
            3. Aggregation on sharded MySQL needs Application level handling vs Cassandra or HBase(Column) can handle this since they are built on top of distributed systems . Fact and derived above
            4. How MySQL indexing works ? answered above
            5. Cassandra benchmarking ? answered above
            7. Time taken from master to slave - MySQL, MongoDB ? MySQL ( 100 ms aws)
            9. Refer NTP ? answered above
            8. MongoDB benchmarking ?
            6. Study about graphdbs ? 

 Operation system
    User level threads vs Kernel level threads
      https://www.youtube.com/watch?v=_5q8ZK6hwzM

 

 
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================


References

Dump : https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
Quick read : https://github.com/Jeevan-kumar-Raj/Grokking-System-Design

    
- Load balancing
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/load-balancing.md
      https://www.cloudflare.com/learning/performance/types-of-load-balancing-algorithms/
      https://igotanoffer.com/blogs/tech/load-balancing-system-design-interview
    Static load balancing - round robin, weight round robin, IP hash
    Dynamic load balanacing - least connection, least resource, least response time, least bandwidth
    When to use Static ?
      If the server pool and the requests are both homogeneous, and there is only one balancer. Stateless servers handling single api can use static load balancer
      
    When to use Dynamic ?
      if the requests are heterogenous, Least Load is helpful to prevent servers from being intermittently overloaded.
      
    What are Hardware load balancers ?
      They have high performant hardware resources like L4 and L7. 
      Read about L4 and L7 here https://levelup.gitconnected.com/l4-vs-l7-load-balancing-d2012e271f56
      
    What are Software load balancers ?
      They run on standard servers and are less hardware optimized, but cheaper to set up and run. Example, Nginx or HAProxy. For Nginx 100s video, 
      refer https://www.youtube.com/watch?v=JKxlsvZXG7c. Nginx can serve routing to different servers, rate limiting, handle spikes,
      reverse proxy, security, cache, etc. 
      
    When to use Software or Hardware ?
      Tip : Since software load balancing can run on ordinary hardware and supports, always prefer software load balancer. Example Nginx
       
      
    What is proxy server, reverse proxy and API Gateway ? How is different from load balancer
      proxy vs reverse proxy vs load balancer : https://www.youtube.com/watch?v=MiqrArNSxSM
      Proxy : just protects the client side
      Reverse Proxy : protects the server by serving as proxy for all backend services - routing, rate limitng, load balancing
      API gateway : rate limiting, routing, handle spikes
      Load balancer : only performs load balancing
      
      So Reverse proxy can perform both API Gateway and Load Balancing. Example nginx.
      A load balancer can never be API gateway or reverse proxy.
      
     Always choose Nginx ? Why ?
      It is powerful due to non-blocking worker threads
        - Nginx does not create one thread/process per request/connection and wait for response from server or next request from client. Instead it has fixed worker threads which can listen to multiple clients or server responses at a time whichever is ready https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/ 

       Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports request forwarding based on different paths like api gateway
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth SSL. Also does optimizations with keep alive connection and cache to avoid SSL handshake - https://docs.nginx.com/nginx/admin-guide/security-controls/terminating-ssl-http/ 
         Also a good read https://www.nginx.com/blog/http-keepalives-and-web-performance/
       Nginx supports L7

       Why Load balancer + API gateway should be consolidated with  Nginx 
         https://www.nginx.com/blog/consolidating-your-api-gateway-and-load-balancer-with-nginx/
        1 million QPS, 65K SSL TPS, 70 Gbps throughput 
        Netflix, Hulu, Airbnb, Pinterest


    
================================================================

      
  - Caching
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/caching.md
      Distributed cache on Application server VS Global cache like Redis, (mostly preferred)
      CDN is another form of cache for static content but its often expensive
      
          
       What are different caching strategies ? 
          https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
          Cache-Aside (application server has cache aside like Redis, Memcache and DB separately)
            - Pros : logic controlled by application server, resilent to cache failures, data model in cache vs db could be different
            - Cons : stale data in cache for db writes
          Read-Through Cache (application server reads from cache only)
            - Pros : logic controlled by cache, read-heavy, good consistency when combined with write through 
            - Cons : needs data model to be consistent, first time data always results in cache miss
            Application : autocomplete suggestion to read from trie cache - https://bytebytego.com/courses/system-design-interview/design-a-search-autocomplete-system 

          Belpw are Cache writes invalidation policy 

          Write through (data is written to cache and synced with storage)
            - Pros : fast retreival and data consistent systems
            - Cons : slow writes though
          Write around (data is written to storage, not cache)
            - Pros : Saves write operation on the cache
            - Cons : recently written data creates a cache miss and higher latency.
          Write back (data is written to cache only, then synced later to storage)
            - Pros : fast retreival, low latency read / writes
            - Cons : Risk of data loss
            
        Real applications of 
          1. read-through, write-through for high consistency
                DynamoDB Accelerator (DAX) for dynamoDB
          2. read-through, write-around for high performance for situations where data is written once and read less frequently or never.
                real-time logs, chatroom messages
          3. cache-aside, write back cache to absorb spikes during peak load
                custom applications using redis
          4. write back cache
                InnoDB which is relational database storage engine. Queries are first written to memory and eventually flushed to the disk.
                
                
================================================================
    - Sharding / Partitioning 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sharding.md
          https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
          Horizontal, Vertical and Directory based paritioning
            Directory based paritioning has work around to solve some challenges with Horizontal and Vertical?
          What's the best Partitioning criteria ?
            Consitent hashing which is combination of hash and list based partitioning
          Commom problems with Sharding / Partitioning
            Joins -> Denormalization
            Referential Integrity (primary key -> foreign key. look at image in https://en.wikipedia.org/wiki/Referential_integrity)
            Hot shard problem needs rebalancing
              How to rebalance ? 
              It would take downtime, check directory based partitioning as per our educative.io resource.
              But consistent hashing can solve this problem better - https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648
              Great guide above - check consistent hashing with gossip protocol so that each partition knows where to fetch data from db
            Heterogeneity: the number of virtual nodes for a server is proportional to the server capacity. For example, servers with higher capacity are assigned with more virtual nodes.
          Applications of consistent hashing ?
          Apache Cassandra, Dynamo DB
 
 ================================================================

    - Indexing
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/indexes.md
          What is a database index ? 
            https://www.codecademy.com/article/sql-indexes
          Pros
            helps in speeding up the search 
          Cons
            lowers write/update/delete performance by 4 times
          When to use indexes ?
            In the case of data sets that are many terabytes in size but with very small payloads (e.g., 1 KB), 
            indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge since
            we can’t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is 
            spread over several physical devices—this means we need some way to find the correct physical location of the desired data. 
            Indexes are the best way to do this.
            
   ================================================================

     - Proxy 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/proxies.md
          
          Usage : 
          filter requests, log requests, cache, encryption and most important batch request. Further, it can get smarter to o collapse requests for data that is spatially close together in the storage (consecutively on disk). This strategy will result in decreasing request latency. 
          For example, let’s say a bunch of servers request parts of file: part1, part2, part3, etc. We can set up our proxy in such a way 
          that it can recognize the spatial locality of the individual requests, thus collapsing them into a single request and reading complete file, 
          which will greatly minimize the reads from the data origin.
          
          Proxy are of two types 
          Client proxy to protect the clients
          Server proxy to protect the servers. Also called reverse proxy
          
          Should we use proxy then?
          Only use proxy to protect the client.
          For server side, Always Nginx since its open source and supports both reverse proxy + load balancing + API gateway.
          
          
   
   ==============================================================================

          Queues
          
            Resource :
            - https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/queues.md
            
            When to use queue :
            Queues are implemented on the asynchronous communication protocol, meaning when a client submits a task to a queue they 
            are no longer required to wait for the results; instead, they need only acknowledgment that the request was properly received.
            
            Queues are also used for fault tolerance as they can provide some protection from service outages and failures. For example, 
            we can create a highly robust queue that can retry service requests that have failed due to transient system failures
            
            When not to use queue :
            When client expects respone in real-time
            
            Example of queues : 
             RabbitMQ and Kafka (which is open source)
             
  ====================================================================================
  
        Redundancy and Replication
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/redundancy.md
          
          When to use Redundant system :
          Always. its a key concept of distributed system.
          Always prefer shared nothing architecture so that each node can operate independently and can scale.
          
          
    ====================================================================================
        SQL vs NoSQL
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sql-vs-nosql.md
          
          SQL 
          Relational databases store data in rows and columns. Example, MySQL, Oracle, MS SQL Server, SQLite, Postgres
          
          NoSQL 
          Key-value :  Example, DynamoDB, Redis, Memcache
          Document DB : Example, MongoDB, 
          Wide-Column Database : Example, HBase, Cassandra - https://hevodata.com/learn/columnar-databases/
          Graph Databases : Example, Neo4j
          
          Differences between both 
          Storage : SQL is tables but NoSQL has different storage models
          Schema : changing schema with SQL is possible but requires whole database modification
          Querying : using SQL for SQL dbs. 
          Scalability : SQL is vertically scalable and possible to scale it horizontally but has limitations. NoSQL is horizontally scalable
          Reliability or ACID : Definitely SQL ensures ACID but NoSQL solutions sacrifice ACID compliance for performance and scalability.
          
          When to use SQL
            ACID compliance for e-commerce and finanicial transactions
            Your data is structured and unchanging.
            Often Read heavy and low-write

            
          When to use NoSQL
            - Storing large volumes of data that often have little to no structure. 
            - Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution 
            but requires data to be easily spread across multiple servers to scale up. 
            - Rapid development. NoSQL is extremely useful for rapid development as it doesn’t need to be prepped ahead of time. 
            
          What about write performance ? 
          noSQL has better write performance due to SSTable + LSM tree implementation. Example, Cassandra
          

            
            
  ====================================================================================
  
    CAP Theorem
    Resource : 
    https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/cap-theorem.md
    
    CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of 
    the following guarantees (CAP): Consistency, Availability and Partition tolerance.
    
    We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates 
    in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client 
    reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.

    When to choose consistency over availability ? 
      Banking, Payment systems
    
    When to choose availability over consistency   ? 
      Most distributed system use cases like Google Maps, Twitter, etc.
      
    Since reliability consists of both consistency and availability, ASK YOUR interviewer what it means for system to be 99.99% reliable
    
    
   ====================================================================================
 
    Consistent Hashing
    
    Resource 
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/consistent-hashing.md
      Recommended ones 
        https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648 - Explains well about rebalancing
        https://bytebytego.com/courses/system-design-interview/design-consistent-hashing
        
      How does it work ? 
        Hashing is done in the range where it maps the key to the integer in that range. Example 0 -> 255 are the integers placed in ring form such that values are wrapped around.
        
        1. the servers are mapped to the integers in that range
        2. to map key to a server, simply hash the key to the integer in that range and move clockwise till you find the server (could be binary search)
        
        Now how this is better :
        1. adding a server : say S1 is added at position 20 and is near S2 at position 25. Then keys from before 20 wuld map to S1.
        2. removing a server : say S1 is removed at position 20 and was near S2 at position 25. Then  all keys even before 20 would map to S2. 
        3. uniform distribution : add “virtual replicas”. Instead of mapping each server to a single point on the ring, 
            we map it to multiple points on the ring, i.e. replicas. 
        4. easy to rebalance : when server is added or removed, only its next clockwise neighbouring server is affected and requires re-mapping of keys.
        
        
    ====================================================================================
    
    Client-Server Communication
    
      Resource : 
        https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/client-server-communication.md
        
      Types : 
      1. Standard HTTP Web Request : client opens the connection, gets response from the server and terminate the connection 
      2. Ajax Polling : client performs HTTP Web Request above periodically (say every 1 s). May create unecessary empty responses if server is not ready.
      3. Long Polling : client performs HTTP Web Request and waits for long time till response is received (say 1 min). Then sends request again.
      4. Web Socket: persistent connection between a client and a server that both parties can use to start sending data at any time.
      5. SSE :  persistent onnection between a client and a server where server send data to the client but not the reverse
      
      When to use Long Polling vs Web Socket ? 
        https://ably.com/blog/websockets-vs-long-polling
        https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o
        
        Clearly Websockets have many advantages over long polling and thus are appropriate for many applications which require consistent low latency 
        full duplex high frequency communication such as chat applications and real time applications.
        
        Scaling up 
        Websockets have problems with load distribution due to persistent connection.
        Long polling will have equal load distribution after its long timeout
        
     When to use Web Socker vs Server Sent Events ?
        Websocket for bi-directional communication while SSE for server to client communication
        https://blog.bitsrc.io/websockets-vs-server-sent-events-968659ab0870
        
        WebSockets are widely used and valued in technological solutions such as real-time polling, chat, media players, multiplayer games, etc.
        Server-Sent Events: There are many applications where sending data from the client isn’t necessary. 
        SSEs are especially useful in status updates, social-media news feeds, push notifications, newsletters, etc.

      Keep alive : 
        https://www.imperva.com/learn/performance/http-keep-alive/
        - improves latency to avoid 3-way handskae and SSL/TLS connections
        - less consumption of network resources to use single connection. This can drop network conjestion




===============================================================

- Key characteristics
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/key-characteristics.md 
    Scalability, Availability, Reliability, Efficiency and Servicability/Manageability
    -> Remember an Avaiability system does not means Reliable but Reliable system is always available. 

  How to handle burst / spiky reads ? 
    Use Nignx to do request collapsing for similar type of content
    Use queue if content does not need to be delivered real-time
    Other strategies : Data Cache, CDN (https://www.onecloudsystems.com/2016/10/25/how-to-ensure-site-can-handle-traffic-spikes/)

  Pull vs push models
    In Pull approach, the metrics collector needs to know the complete list of service endpoints to pull data from. The good news is that we have a reliable, scalable, and maintainable solution available through Service Discovery, provided by etcd [14], Zookeeper [15], etc., wherein services register their availability and the metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes. 
    If a server goes down, then you can re-try in the next pull. But here you need dedupe logic or  store the offset in S3 to know what messages to retry.

    In a push model, a collection agent is commonly installed on every server being monitored. Aggregation is an effective way to reduce the volume of data sent to the metrics collector. If the push traffic is high and the metrics collector rejects the push with an error, the agent could keep a small buffer of data locally (possibly by storing them locally on disk), and resend them later.
    If a server goes down or cannot handle burst traffic, then you loose the message.
 

    
   
================================================================================================================================
- Key learnings from system design

 
  
   
         
    

      
    
   
  

  

  
  
  

  
  

  


  




























              






=================================================================================

    
